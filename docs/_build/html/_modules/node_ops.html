<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>node_ops &mdash; antk 0.3.0 documentation</title>
    
    <link rel="stylesheet" href="../_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.3.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="antk 0.3.0 documentation" href="../index.html" />
    <link rel="up" title="Module code" href="index.html" /> 
  </head>
  <body role="document">

<div style="background-color: white; text-align: center; padding: 10px 10px 15px 15px">
<a href="../index.html"><img src="../_static/antlogo.png" border="0" alt="ant logo"/></a>
</div>

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li><a href="../index.html">home</a>|&nbsp;</li>
        <li><a href="../search.html">search</a>|&nbsp;</li>

          <li class="nav-item nav-item-1"><a href="index.html" accesskey="U">Module code</a> &raquo;</li> 
      </ul>
    </div>

      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for node_ops</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">scipy.sparse</span> <span class="k">as</span> <span class="nn">sps</span>
<span class="kn">from</span> <span class="nn">antk.core</span> <span class="k">import</span> <span class="n">loader</span>
<span class="kn">import</span> <span class="nn">numbers</span>

<span class="kn">from</span> <span class="nn">antk.lib.decorate</span> <span class="k">import</span> <span class="n">pholder</span><span class="p">,</span> <span class="n">variable</span><span class="p">,</span> <span class="n">node_op</span><span class="p">,</span> <span class="n">neural_net</span><span class="p">,</span> <span class="n">act</span><span class="p">,</span> <span class="n">relu</span><span class="p">,</span> <span class="n">loss_function</span>


<span class="n">ACTIVATION_LAYERS</span> <span class="o">=</span> <span class="s1">&#39;activation_layers&#39;</span>
<span class="n">NORMALIZED_ACTIVATIONS</span> <span class="o">=</span> <span class="s1">&#39;normalized_activations&#39;</span>

<div class="viewcode-block" id="MissingShapeError"><a class="viewcode-back" href="../node_ops.html#node_ops.MissingShapeError">[docs]</a><span class="k">class</span> <span class="nc">MissingShapeError</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Raised when :any:`placeholder` can not infer shape.&#39;&#39;&#39;</span>
    <span class="k">pass</span></div>

<div class="viewcode-block" id="fan_scale"><a class="viewcode-back" href="../node_ops.html#node_ops.fan_scale">[docs]</a><span class="k">def</span> <span class="nf">fan_scale</span><span class="p">(</span><span class="n">initrange</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">tensor_in</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">activation</span> <span class="o">==</span> <span class="n">relu</span><span class="p">:</span>
        <span class="n">initrange</span> <span class="o">*=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">tensor_in</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">initrange</span> <span class="o">*=</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="n">numpy</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">tensor_in</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">])))</span></div>




<span class="c1"># ===============================================================================</span>
<span class="c1"># ===================NODES=======================================================</span>
<span class="c1"># ===============================================================================</span>
<div class="viewcode-block" id="ident"><a class="viewcode-back" href="../node_ops.html#node_ops.ident">[docs]</a><span class="k">def</span> <span class="nf">ident</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;ident&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity function for grouping tensors in graph, during config parsing.</span>

<span class="sd">    :param tensor_in: A Tensor_ or list of tensors</span>
<span class="sd">    :return: tensor_in</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_in</span></div>

<span class="nd">@pholder</span>
<div class="viewcode-block" id="placeholder"><a class="viewcode-back" href="../node_ops.html#node_ops.placeholder">[docs]</a><span class="k">def</span> <span class="nf">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;placeholder&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Wrapper to create tensorflow_ Placeholder_ which infers dimensions given data.</span>

<span class="sd">    :param dtype: Tensorflow dtype to initiliaze a Placeholder.</span>
<span class="sd">    :param shape: Dimensions of Placeholder</span>
<span class="sd">    :param data: Data to infer dimensions of Placeholder from.</span>
<span class="sd">    :param name: Unique name for variable scope.</span>
<span class="sd">    :return: A Tensorflow_ Placeholder.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MissingShapeError</span><span class="p">(</span><span class="s1">&#39;Shape or data to infer the shape from must be provided&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="ow">is</span> <span class="n">loader</span><span class="o">.</span><span class="n">HotIndex</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shapespec</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>
            <span class="n">shape</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">shapespec</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">shapespec</span><span class="p">)])</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>

<span class="nd">@variable</span>
<div class="viewcode-block" id="weights"><a class="viewcode-back" href="../node_ops.html#node_ops.weights">[docs]</a><span class="k">def</span> <span class="nf">weights</span><span class="p">(</span><span class="n">distribution</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">initrange</span><span class="o">=</span><span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weights&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Wrapper parameterizing common constructions of tf.Variables.</span>

<span class="sd">    :param distribution: A string identifying distribution &#39;tnorm&#39; for truncated normal, &#39;rnorm&#39; for random normal, &#39;constant&#39; for constant, &#39;uniform&#39; for uniform.</span>
<span class="sd">    :param shape: Shape of weight tensor.</span>
<span class="sd">    :param dtype: dtype for weights</span>
<span class="sd">    :param initrange: Scales standard normal and trunctated normal, value of constant dist., and range of uniform dist. [-initrange, initrange].</span>
<span class="sd">    :param seed: For reproducible results.</span>
<span class="sd">    :param l2: Floating point number determining degree of of l2 regularization for these weights in gradient descent update.</span>
<span class="sd">    :param name: For variable scope.</span>
<span class="sd">    :return: A tf.Variable.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">distribution</span> <span class="o">==</span> <span class="s1">&#39;norm&#39;</span><span class="p">:</span>
        <span class="n">wghts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initrange</span><span class="o">*</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">seed</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">distribution</span> <span class="o">==</span> <span class="s1">&#39;tnorm&#39;</span><span class="p">:</span>
        <span class="n">wghts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initrange</span><span class="o">*</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">seed</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">distribution</span> <span class="o">==</span> <span class="s1">&#39;uniform&#39;</span><span class="p">:</span>
        <span class="n">wghts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="o">-</span><span class="n">initrange</span><span class="p">,</span> <span class="n">initrange</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">seed</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">distribution</span> <span class="o">==</span> <span class="s1">&#39;constant&#39;</span><span class="p">:</span>
        <span class="n">wghts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">initrange</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Argument &#39;distribution takes values &#39;norm&#39;, &#39;tnorm&#39;, &#39;uniform&#39;, &#39;constant&#39;, &quot;</span>
                          <span class="s2">&quot;Received </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">distribution</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">l2</span> <span class="o">!=</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="s1">&#39;losses&#39;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_loss</span><span class="p">(</span><span class="n">wghts</span><span class="p">),</span> <span class="n">l2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;weight_loss&#39;</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">wghts</span></div>

<span class="nd">@node_op</span>
<div class="viewcode-block" id="cosine"><a class="viewcode-back" href="../node_ops.html#node_ops.cosine">[docs]</a><span class="k">def</span> <span class="nf">cosine</span><span class="p">(</span><span class="n">operands</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;cosine&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes the cosine of vectors in corresponding rows of the two matrix tensors_ in operands.</span>

<span class="sd">    :param operands:  A list of two tensors to take cosine of.</span>
<span class="sd">    :param name: An optional name for unique variable scope.</span>
<span class="sd">    :return: A tensor with dimensions (operands[0].shape[0], 1)</span>
<span class="sd">    :raises: ValueError when operands do not have matching shapes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shape1</span> <span class="o">=</span> <span class="n">operands</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="n">shape2</span> <span class="o">=</span> <span class="n">operands</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">shape1</span> <span class="o">==</span> <span class="n">shape2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cosine expects matching shapes for operands. Found operands[0] shape = </span><span class="si">%s</span><span class="s2">, &quot;</span>
                         <span class="s2">&quot;operands[1] shape = </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">shape1</span><span class="p">,</span> <span class="n">shape2</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">xlen</span> <span class="o">=</span> <span class="n">node_op</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">)(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">operands</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">operands</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;cosine&#39;</span><span class="p">)</span>
        <span class="n">ylen</span> <span class="o">=</span> <span class="n">node_op</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">)(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">operands</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">operands</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">norm</span> <span class="o">=</span> <span class="n">node_op</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">mul</span><span class="p">)(</span><span class="n">xlen</span><span class="p">,</span> <span class="n">ylen</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">x_dot_y</span><span class="p">(</span><span class="n">operands</span><span class="p">),</span> <span class="n">norm</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>

<span class="nd">@node_op</span>
<div class="viewcode-block" id="x_dot_y"><a class="viewcode-back" href="../node_ops.html#node_ops.x_dot_y">[docs]</a><span class="k">def</span> <span class="nf">x_dot_y</span><span class="p">(</span><span class="n">operands</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;x_dot_y&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes the inner product for rows of operands[1], and operands[2],</span>
<span class="sd">    and adds optional bias, operands[3], operands[4].</span>
<span class="sd">    If either operands[1] or operands[2] or both is a list of tensors</span>
<span class="sd">    then a list of the pairwise dot products (with bias when len(operands) &gt; 2)</span>
<span class="sd">    of the lists is returned.</span>

<span class="sd">    :param operands: A list of 2, 3, or 4 tensors_ (the first two tensors may be replaced by lists of tensors</span>
<span class="sd">                                                    in which case the return value will a list of the dot products</span>
<span class="sd">                                                    for all members of the cross product of the two lists.).</span>
<span class="sd">    :param name: An optional identifier for unique variable_scope_.</span>
<span class="sd">    :return: A tensor or list of tensors with dimension (operands[1].shape[0], 1).</span>
<span class="sd">    :raises: Value error when operands is not a list of at least two tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">operands</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">operands</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;x_dot_y needs a list of 2-4 tensors.&quot;</span><span class="p">)</span>
    <span class="n">outproducts</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">operands</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">:</span>
        <span class="n">operands</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">operands</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">operands</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">:</span>
        <span class="n">operands</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">operands</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">operands</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">operands</span><span class="p">[</span><span class="mi">1</span><span class="p">])):</span>
            <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;right</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span> <span class="o">+</span> <span class="s1">&#39;left</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">j</span><span class="p">):</span>
                <span class="n">dot</span> <span class="o">=</span> <span class="n">node_op</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">)(</span><span class="n">tf</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">operands</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">operands</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="p">]),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">operands</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
                    <span class="n">dot</span> <span class="o">=</span> <span class="n">dot</span> <span class="o">+</span> <span class="n">operands</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">operands</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
                    <span class="n">dot</span> <span class="o">=</span> <span class="n">dot</span> <span class="o">+</span> <span class="n">operands</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
                <span class="n">outproducts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dot</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">outproducts</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">outproducts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">outproducts</span></div>

<span class="nd">@node_op</span>
<div class="viewcode-block" id="lookup"><a class="viewcode-back" href="../node_ops.html#node_ops.lookup">[docs]</a><span class="k">def</span> <span class="nf">lookup</span><span class="p">(</span><span class="n">dataname</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="n">data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="n">indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span>
           <span class="n">initrange</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">makeplace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;lookup&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A wrapper for `tensorflow&#39;s`_ `embedding_lookup`_ which infers the shape of the</span>
<span class="sd">    weight matrix and placeholder value from the parameter *data*.</span>

<span class="sd">    :param dataname: Used exclusively by config.py</span>
<span class="sd">    :param data: A :any:`HotIndex` object</span>
<span class="sd">    :param indices: A `Placeholder`_. If indices is none the dimensions will be inferred from *data*</span>
<span class="sd">    :param distribution: Distribution for lookup weight initialization</span>
<span class="sd">    :param initrange: Initrange for weight distribution.</span>
<span class="sd">    :param l2: Floating point number determining degree of of l2 regularization for these weights in gradient descent update.</span>
<span class="sd">    :param shape: The dimensions of the output tensor_, typically [None, output-size]</span>
<span class="sd">    :param makeplace: A boolean to tell whether or not a placeholder has been created for this data (Used by config.py)</span>
<span class="sd">    :param name: A name for unique variable scope.</span>
<span class="sd">    :return: tf.nn.embedding_lookup(wghts, indices), wghts, indices</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="ow">is</span> <span class="n">loader</span><span class="o">.</span><span class="n">HotIndex</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">makeplace</span><span class="p">:</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="n">dataname</span><span class="p">)</span>
        <span class="n">wghts</span> <span class="o">=</span> <span class="n">weights</span><span class="p">(</span><span class="n">distribution</span><span class="p">,</span> <span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">initrange</span><span class="o">=</span><span class="n">initrange</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="n">l2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="o">+</span><span class="s1">&#39;_weights&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">wghts</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">),</span> <span class="n">wghts</span><span class="p">,</span> <span class="n">indices</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type of data for lookup indices must be antk.core.loader.HotIndex&quot;</span><span class="p">)</span></div>


<span class="nd">@node_op</span>
<div class="viewcode-block" id="embedding"><a class="viewcode-back" href="../node_ops.html#node_ops.embedding">[docs]</a><span class="k">def</span> <span class="nf">embedding</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;embedding&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A wrapper for `tensorflow&#39;s`_ `embedding_lookup`_</span>

<span class="sd">    :param tensors: A list of two tensors_ , matrix, indices</span>
<span class="sd">    :param name: Unique name for variable scope</span>
<span class="sd">    :return: A matrix tensor_ where the i-th row = matrix[indices[i]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">matrix</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>

<span class="nd">@node_op</span>
<div class="viewcode-block" id="mult_log_reg"><a class="viewcode-back" href="../node_ops.html#node_ops.mult_log_reg">[docs]</a><span class="k">def</span> <span class="nf">mult_log_reg</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">numclasses</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">initrange</span><span class="o">=</span><span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;log_reg&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs mulitnomial logistic regression forward pass. Weights and bias initialized to zeros.</span>

<span class="sd">    :param tensor_in: A tensor_ or placeholder_</span>
<span class="sd">    :param numclasses: For classificatio</span>
<span class="sd">    :param data: For shape inference.</span>
<span class="sd">    :param dtype: For :any:`weights` initialization.</span>
<span class="sd">    :param initrange: For :any:`weights` initialization.</span>
<span class="sd">    :param seed: For :any:`weights` initialization.</span>
<span class="sd">    :param l2: For :any:`weights` initialization.</span>
<span class="sd">    :param name: For `variable_scope`_</span>
<span class="sd">    :return:  A tensor shape=(tensor_in.shape[0], numclasses)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="ow">is</span> <span class="n">loader</span><span class="o">.</span><span class="n">HotIndex</span><span class="p">:</span>
            <span class="n">numclasses</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">dim</span>
        <span class="k">elif</span> <span class="n">loader</span><span class="o">.</span><span class="n">is_one_hot</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
            <span class="n">numclasses</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">MissingShapeError</span><span class="p">(</span><span class="s1">&#39;Can not infer shape from data: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">data</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">numclasses</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">MissingShapeError</span><span class="p">(</span><span class="s1">&#39;Can not infer shape. Need numclasses or data argument.&#39;</span><span class="p">)</span>
    <span class="n">inshape</span> <span class="o">=</span> <span class="n">tensor_in</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">weights</span><span class="p">(</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">inshape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">numclasses</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">initrange</span><span class="o">=</span><span class="n">initrange</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="n">l2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_weights&#39;</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">weights</span><span class="p">(</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">numclasses</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">initrange</span><span class="o">=</span><span class="n">initrange</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="n">l2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_bias&#39;</span><span class="p">)</span>
    <span class="n">tensor_out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor_out</span></div>

<span class="nd">@node_op</span>
<div class="viewcode-block" id="concat"><a class="viewcode-back" href="../node_ops.html#node_ops.concat">[docs]</a><span class="k">def</span> <span class="nf">concat</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;concat&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Matrix multiplies each tensor_ in *tensors* by its own weight matrix and adds together the results.</span>

<span class="sd">    :param tensors: A list of tensors.</span>
<span class="sd">    :param output_dim: Dimension of output</span>
<span class="sd">    :param name: An optional identifier for unique variable_scope_.</span>
<span class="sd">    :return: A tensor with shape [None, output_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensors</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;inTensor</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">):</span>
            <span class="n">tensor_in</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_linear&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">combo</span> <span class="o">=</span> <span class="n">tensor_in</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">combo</span> <span class="o">=</span> <span class="n">combo</span> <span class="o">+</span> <span class="n">tensor_in</span>
    <span class="k">return</span> <span class="n">combo</span></div>

<span class="nd">@neural_net</span>
<span class="k">def</span> <span class="nf">dnn</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="s1">&#39;tnorm&#39;</span><span class="p">,</span>
        <span class="n">initrange</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fan_scaling</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dnn&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates fully connected deep neural network subgraph. Adapted From skflow_ `dnn_ops.py`_</span>
<span class="sd">        `Neural Networks and Deep Learning`_</span>

<span class="sd">        `Using Neural Nets to Recognize Handwritten Digits`_</span>

<span class="sd">    :param tensor_in: tensor_ or placeholder_ for input features.</span>
<span class="sd">    :param hidden_units: list of counts of hidden units in each layer.</span>
<span class="sd">    :param activation: activation function between layers. Can be None.</span>
<span class="sd">    :param distribution: Distribution for lookup weight initialization</span>
<span class="sd">    :param initrange: Initrange for weight distribution.</span>
<span class="sd">    :param l2: Floating point number determining degree of of l2 regularization for these weights in gradient descent update.</span>
<span class="sd">    :param bn: Whether or not to use batch normalization</span>
<span class="sd">    :param keep_prob: if not None, will add a dropout layer with given</span>
<span class="sd">                    probability.</span>
<span class="sd">    :param name: A name for unique variable_scope_.</span>
<span class="sd">    :return: A tensor_ which would be a deep neural network.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n_units</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;layer</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">fan_scaling</span><span class="p">:</span>
                <span class="n">initrange</span> <span class="o">=</span> <span class="n">fan_scale</span><span class="p">(</span><span class="n">initrange</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">tensor_in</span><span class="p">)</span>
            <span class="n">tensor_in</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                               <span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span> <span class="n">initrange</span><span class="o">=</span><span class="n">initrange</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="n">l2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
            <span class="n">tensor_in</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_activation&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">bn</span><span class="p">:</span>
                <span class="n">tensor_in</span> <span class="o">=</span> <span class="n">batch_normalize</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_bn&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">keep_prob</span><span class="p">:</span>
                <span class="n">tensor_in</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_dropouts&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor_in</span>


<span class="nd">@neural_net</span>
<span class="k">def</span> <span class="nf">convolutional_net</span><span class="p">(</span><span class="n">in_progress</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    See: `Tensorflow Deep MNIST for Experts`_ ,</span>
<span class="sd">    `Tensorflow Convolutional Neural Networks`_ ,</span>
<span class="sd">    `ImageNet Classification with Deep Convolutional Neural Networks`_ ,</span>
<span class="sd">    `skflow/examples/text_classification_character_cnn.py`_ ,</span>
<span class="sd">    `skflow/examples/text_classification_cnn.py`_ ,</span>
<span class="sd">    `Character-level Convolutional Networks for Text Classification`_</span>

<span class="sd">    :param in_progress:</span>
<span class="sd">    :return:</span>
<span class="sd">    &quot;&quot;&quot;</span>

<span class="nd">@neural_net</span>
<span class="k">def</span> <span class="nf">residual_dnn</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="s1">&#39;tnorm&#39;</span><span class="p">,</span>
        <span class="n">initrange</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fan_scaling</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">skiplayers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dnn&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates residual neural network with shortcut connections.</span>
<span class="sd">        `Deep Residual Learning for Image Recognition`_</span>

<span class="sd">    :param tensor_in: tensor_ or placeholder_ for input features.</span>
<span class="sd">    :param hidden_units: list of counts of hidden units in each layer.</span>
<span class="sd">    :param activation: activation function between layers. Can be None.</span>
<span class="sd">    :param distribution: Distribution for lookup weight initialization</span>
<span class="sd">    :param initrange: Initrange for weight distribution.</span>
<span class="sd">    :param l2: Floating point number determining degree of of l2 regularization for these weights in gradient descent update.</span>
<span class="sd">    :param bn: Whether or not to use batch normalization</span>
<span class="sd">    :param keep_prob: if not None, will add a dropout layer with given</span>
<span class="sd">                    probability.</span>
<span class="sd">    :param skiplayers: The number of layers to skip for the shortcut connection.</span>
<span class="sd">    :param name: A name for unique variable scope</span>
<span class="sd">    :return: A tensor_ which would be a residual deep neural network.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">)</span> <span class="o">%</span> <span class="n">skiplayers</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The number of layers must be a multiple of skiplayers&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fan_scaling</span><span class="p">:</span>
        <span class="n">initrange</span> <span class="o">=</span> <span class="n">fan_scale</span><span class="p">(</span><span class="n">initrange</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">tensor_in</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">)</span><span class="o">//</span><span class="n">skiplayers</span><span class="p">):</span>
        <span class="n">shortcut</span> <span class="o">=</span> <span class="n">tensor_in</span>
        <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">k</span><span class="o">*</span><span class="n">skiplayers</span><span class="p">,</span> <span class="n">k</span><span class="o">*</span><span class="n">skiplayers</span> <span class="o">+</span> <span class="n">skiplayers</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n_units</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]):</span>
            <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;layer</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="o">*</span><span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)):</span>
                <span class="n">tensor_in</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                                   <span class="n">initrange</span><span class="o">=</span><span class="n">initrange</span><span class="p">,</span>
                                   <span class="n">l2</span><span class="o">=</span><span class="n">l2</span><span class="p">,</span>
                                   <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
                <span class="n">tensor_in</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_activation&#39;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">bn</span><span class="p">:</span>
                    <span class="n">tensor_in</span> <span class="o">=</span> <span class="n">batch_normalize</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_bn&#39;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">keep_prob</span><span class="p">:</span>
                    <span class="n">tensor_in</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_dropouts&#39;</span><span class="p">)</span>
        <span class="n">shp1</span><span class="p">,</span> <span class="n">shp2</span> <span class="o">=</span> <span class="n">shortcut</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">(),</span> <span class="n">tensor_in</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">shp1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">shp2</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;skip_connect</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">k</span><span class="p">):</span>
                <span class="n">shortcut</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">shortcut</span><span class="p">,</span> <span class="n">shp2</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                  <span class="n">initrange</span><span class="o">=</span><span class="n">initrange</span><span class="p">,</span>
                                  <span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="n">l2</span><span class="p">,</span>
                                  <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_skiptransform&#39;</span><span class="p">)</span>
        <span class="n">tensor_in</span> <span class="o">=</span> <span class="n">tensor_in</span> <span class="o">+</span> <span class="n">shortcut</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_skipconnection&#39;</span><span class="p">,</span> <span class="n">tensor_in</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor_in</span>

<span class="nd">@neural_net</span>
<span class="k">def</span> <span class="nf">highway_dnn</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="s1">&#39;tnorm&#39;</span><span class="p">,</span>
                <span class="n">initrange</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fan_scaling</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">bias_start</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;highway_dnn&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A highway deep neural network.</span>
<span class="sd">        `Training Very Deep Networks`_</span>

<span class="sd">    :param tensor_in: A 2d matrix tensor_.</span>
<span class="sd">    :param hidden_units:  list of counts of hidden units in each layer.</span>
<span class="sd">    :param activation: Non-linearity to perform. Can be ident for no non-linearity.</span>
<span class="sd">    :param distribution: Distribution for lookup weight initialization</span>
<span class="sd">    :param initrange: Initrange for weight distribution.</span>
<span class="sd">    :param l2: Floating point number determining degree of of l2 regularization for these weights in gradient descent update.</span>
<span class="sd">    :param bn: Whether or not to use batch normalization</span>
<span class="sd">    :param keep_prob: Dropout rate.</span>
<span class="sd">    :param bias_start: initialization of transform bias weights</span>
<span class="sd">    :param name: A name for unique variable_scope.</span>
<span class="sd">    :return: A tensor_ which would be a highway deep neural network.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">fan_scaling</span><span class="p">:</span>
        <span class="n">initrange</span> <span class="o">=</span> <span class="n">fan_scale</span><span class="p">(</span><span class="n">initrange</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">tensor_in</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n_units</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;layer</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;hidden&#39;</span><span class="p">):</span>
                <span class="n">hidden</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                           <span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span> <span class="n">initrange</span><span class="o">=</span><span class="n">initrange</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="n">l2</span><span class="p">,</span>
                                           <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
                <span class="n">hidden</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="o">+</span><span class="s1">&#39;_activation&#39;</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;transform&#39;</span><span class="p">):</span>
                <span class="n">transform</span> <span class="o">=</span> <span class="n">act</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">)(</span><span class="n">linear</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span>
                                              <span class="n">bias_start</span><span class="o">=</span><span class="n">bias_start</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                              <span class="n">initrange</span><span class="o">=</span><span class="n">initrange</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="n">l2</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                                              <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_transform&#39;</span><span class="p">))</span>
            <span class="n">tensor_in</span> <span class="o">=</span> <span class="n">hidden</span> <span class="o">*</span> <span class="n">transform</span> <span class="o">+</span> <span class="n">tensor_in</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">transform</span><span class="p">)</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">tensor_in</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">bn</span><span class="p">:</span>
                <span class="n">tensor_in</span> <span class="o">=</span> <span class="n">batch_normalize</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_bn&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">keep_prob</span><span class="p">:</span>
                <span class="n">tensor_in</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_dropouts&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor_in</span>

<span class="nd">@node_op</span>
<div class="viewcode-block" id="dropout"><a class="viewcode-back" href="../node_ops.html#node_ops.dropout">[docs]</a><span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds dropout node. Adapted from skflow `dropout_ops.py`_ .</span>
<span class="sd">        `Dropout A Simple Way to Prevent Neural Networks from Overfitting`_</span>

<span class="sd">    :param tensor_in: Input tensor_.</span>
<span class="sd">    :param prob: The percent of weights to keep.</span>
<span class="sd">    :param name: A name for the tensor.</span>
<span class="sd">    :return: Tensor_ of the same shape of *tensor_in*.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
        <span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="s1">&#39;dropout_prob&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="n">prob</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span></div>

<span class="nd">@node_op</span>
<div class="viewcode-block" id="linear"><a class="viewcode-back" href="../node_ops.html#node_ops.linear">[docs]</a><span class="k">def</span> <span class="nf">linear</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">bias_start</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
           <span class="n">distribution</span><span class="o">=</span><span class="s1">&#39;tnorm&#39;</span><span class="p">,</span> <span class="n">initrange</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
           <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Linear&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Linear map: :math:`\sum_i(args[i] * W_i)`, where :math:`W_i` is a variable.</span>

<span class="sd">    :param args: a 2D Tensor_</span>
<span class="sd">    :param output_size: int, second dimension of W[i].</span>
<span class="sd">    :param bias: boolean, whether to add a bias term or not.</span>
<span class="sd">    :param bias_start: starting value to initialize the bias; 0 by default.</span>
<span class="sd">    :param distribution: Distribution for lookup weight initialization</span>
<span class="sd">    :param initrange: Initrange for weight distribution.</span>
<span class="sd">    :param l2: Floating point number determining degree of of l2 regularization for these weights in gradient descent update.</span>
<span class="sd">    :param name: VariableScope for the created subgraph; defaults to &quot;Linear&quot;.</span>
<span class="sd">    :return: A 2D Tensor with shape [batch x output_size] equal to</span>
<span class="sd">        :math:`\sum_i(args[i] * W_i)`, where :math:`W_i` are newly created matrices.</span>
<span class="sd">    :raises: ValueError: if some of the arguments has unspecified or wrong shape.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">tensor_in</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Linear is expecting 2D arguments: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Linear expects shape[1] of arguments: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">weights</span><span class="p">(</span><span class="n">distribution</span><span class="p">,</span> <span class="p">[</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">output_size</span><span class="p">],</span> <span class="n">initrange</span><span class="o">=</span><span class="n">initrange</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="n">l2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="o">+</span><span class="s1">&#39;_weights&#39;</span><span class="p">)</span>
    <span class="n">tensor_out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">W</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">bias</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tensor_out</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">weights</span><span class="p">(</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">output_size</span><span class="p">],</span> <span class="n">initrange</span><span class="o">=</span><span class="n">bias_start</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="o">+</span><span class="s1">&#39;_bias&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor_out</span> <span class="o">+</span> <span class="n">b</span></div>

<span class="nd">@node_op</span>
<div class="viewcode-block" id="batch_normalize"><a class="viewcode-back" href="../node_ops.html#node_ops.batch_normalize">[docs]</a><span class="k">def</span> <span class="nf">batch_normalize</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;batch_norm&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch Normalization: Adapted from tensorflow `nn.py`_ and skflow `batch_norm_ops.py`_ .</span>
<span class="sd">        `Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift`_</span>

<span class="sd">    :param tensor_in: input Tensor_</span>
<span class="sd">    :param epsilon: A float number to avoid being divided by 0.</span>
<span class="sd">    :param name: For variable_scope_</span>
<span class="sd">    :return: Tensor with variance bounded by a unit and mean of zero according to the batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">tensor_in</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="n">weights</span><span class="p">(</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">initrange</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_gamma&#39;</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">weights</span><span class="p">(</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">initrange</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_beta&#39;</span><span class="p">)</span>
    <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span> <span class="o">=</span> <span class="n">node_op</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">moments</span><span class="p">)(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="n">inv</span> <span class="o">=</span> <span class="n">node_op</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">)(</span><span class="n">epsilon</span> <span class="o">+</span> <span class="n">variance</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="n">tensor_in</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">tensor_in</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">*</span> <span class="n">inv</span> <span class="o">+</span> <span class="n">gamma</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">NORMALIZED_ACTIVATIONS</span><span class="p">,</span> <span class="n">tensor_in</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor_in</span></div>

<span class="nd">@node_op</span>
<div class="viewcode-block" id="nmode_tensor_tomatrix"><a class="viewcode-back" href="../node_ops.html#node_ops.nmode_tensor_tomatrix">[docs]</a><span class="k">def</span> <span class="nf">nmode_tensor_tomatrix</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;nmode_matricize&#39;</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Nmode tensor unfolding (for order three tensor) from Kolda and Bader `Tensor Decompositions and Applications`_</span>

<span class="sd">    :param tensor: Order 3 tensor to unfold</span>
<span class="sd">    :param mode: Mode to unfold (0,1,2, columns, rows, or fibers)</span>
<span class="sd">    :param name: For variable scoping</span>
<span class="sd">    :return: A matrix (order 2 tensor) with shape dim(mode) X :math:`\Pi_{othermodes}` dim(othermodes)    &#39;&#39;&#39;</span>

    <span class="n">tensorshape</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">matricized_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensorshape</span><span class="p">[</span><span class="n">mode</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">matricized_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensorshape</span><span class="p">[</span><span class="n">mode</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">matricized_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensorshape</span><span class="p">[</span><span class="n">mode</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">matricized_shape</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">tensor</span></div>

<span class="nd">@node_op</span>
<div class="viewcode-block" id="nmode_tensor_multiply"><a class="viewcode-back" href="../node_ops.html#node_ops.nmode_tensor_multiply">[docs]</a><span class="k">def</span> <span class="nf">nmode_tensor_multiply</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">leave_flattened</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;nmode_multiply&#39;</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Nth mode tensor multiplication (for order three tensor) from Kolda and Bader `Tensor Decompositions and Applications`_</span>
<span class="sd">    Works for vectors (matrix with a 1 dimension or matrices)</span>

<span class="sd">    :param tensors: A list of tensors the first is an order three tensor the second and order 2</span>
<span class="sd">    :param mode: The mode to perform multiplication against.</span>
<span class="sd">    :param leave_flattened: Whether or not to reshape tensor back to order 3</span>
<span class="sd">    :param keep_dims: Whether or not to remove 1 dimensions</span>
<span class="sd">    :param name: For variable scope</span>
<span class="sd">    :return: Either an order 3 or order 2 tensor</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">matrix</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">tensorshape</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="n">matrixshape</span> <span class="o">=</span> <span class="n">matrix</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">tensorshape</span><span class="p">[</span><span class="n">mode</span><span class="p">]</span> <span class="o">!=</span> <span class="n">matrixshape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Number of columns of matrix must equal dimension of tensor mode&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">flattened_product</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">nmode_tensor_tomatrix</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">mode</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">leave_flattened</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">product</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">flattened_product</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">tensorshape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="n">tensorshape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]),</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">product</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">flattened_product</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">tensorshape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="n">tensorshape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]),</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">product</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">flattened_product</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">tensorshape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">tensorshape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]),</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">keep_dims</span><span class="p">:</span>
            <span class="n">product</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">product</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">product</span></div>

<span class="nd">@node_op</span>
<div class="viewcode-block" id="binary_tensor_combine"><a class="viewcode-back" href="../node_ops.html#node_ops.binary_tensor_combine">[docs]</a><span class="k">def</span> <span class="nf">binary_tensor_combine</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">initrange</span><span class="o">=</span><span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;binary_tensor_combine&#39;</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    For performing tensor multiplications with batches of data points against an order 3</span>
<span class="sd">    weight tensor.</span>

<span class="sd">    :param tensors: A list of two matrices each with first dim batch-size</span>
<span class="sd">    :param output_dim: The dimension of the third mode of the weight tensor</span>
<span class="sd">    :param initrange: For initializing weight tensor</span>
<span class="sd">    :param name: For variable scope</span>
<span class="sd">    :return: A matrix with shape batch_size X output_dim</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">mat1</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">mat2</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">mat1shape</span> <span class="o">=</span> <span class="n">mat1</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="n">mat2shape</span> <span class="o">=</span> <span class="n">mat2</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">mat1shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">mat2shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Number of rows must match for matrices being combined.&quot;</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">weights</span><span class="p">(</span><span class="s1">&#39;tnorm&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">mat1</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span>
                                    <span class="n">mat2</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span>
                                    <span class="n">output_dim</span><span class="p">],</span>  <span class="n">dtype</span><span class="o">=</span><span class="n">mat1</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="n">l2</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">name</span><span class="o">+</span><span class="s1">&#39;_weights&#39;</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">prod</span> <span class="o">=</span> <span class="n">nmode_tensor_multiply</span><span class="p">([</span><span class="n">t</span><span class="p">,</span> <span class="n">mat1</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">mat2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">mat2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">batch_matmul</span><span class="p">(</span><span class="n">mat2</span><span class="p">,</span> <span class="n">prod</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span></div>

<span class="nd">@node_op</span>
<div class="viewcode-block" id="ternary_tensor_combine"><a class="viewcode-back" href="../node_ops.html#node_ops.ternary_tensor_combine">[docs]</a><span class="k">def</span> <span class="nf">ternary_tensor_combine</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">initrange</span><span class="o">=</span><span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;ternary_tensor_combine&#39;</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    For performing tensor multiplications with batches of data points against an order 3</span>
<span class="sd">    weight tensor.</span>

<span class="sd">    :param tensors:</span>
<span class="sd">    :param output_dim:</span>
<span class="sd">    :param initrange:</span>
<span class="sd">    :param name:</span>
<span class="sd">    :return:</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">combine_pair</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
    <span class="n">combined</span> <span class="o">=</span> <span class="n">binary_tensor_combine</span><span class="p">(</span><span class="n">combine_pair</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">tensors</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">l2</span><span class="o">=</span><span class="n">l2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_dot_y</span><span class="p">([</span><span class="n">combined</span><span class="p">,</span><span class="n">tensors</span><span class="p">[</span><span class="mi">2</span><span class="p">]])</span></div>

<span class="nd">@node_op</span>
<div class="viewcode-block" id="khatri_rao"><a class="viewcode-back" href="../node_ops.html#node_ops.khatri_rao">[docs]</a><span class="k">def</span> <span class="nf">khatri_rao</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;khatrirao&#39;</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    From `David Palzer`_</span>

<span class="sd">    :param tensors:</span>
<span class="sd">    :param name:</span>
<span class="sd">    :return:</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">h1</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">h2</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">L1</span> <span class="o">=</span> <span class="n">h1</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">L2</span> <span class="o">=</span> <span class="n">h2</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">L1</span><span class="o">*</span><span class="n">L2</span>
    <span class="n">h2Tiled</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">h2</span><span class="p">,[</span><span class="mi">1</span><span class="p">,</span><span class="n">L1</span><span class="p">])</span> <span class="c1"># how to tile h2 # L1 and L2 are the number of cols in H1 and H2 respectively</span>
    <span class="n">h1Tiled</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="p">[</span><span class="n">L2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">])</span> <span class="c1"># how to tile h1</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">h1Tiled</span><span class="p">,</span><span class="n">h2Tiled</span><span class="p">)</span></div>

<span class="nd">@node_op</span>
<div class="viewcode-block" id="binary_tensor_combine2"><a class="viewcode-back" href="../node_ops.html#node_ops.binary_tensor_combine2">[docs]</a><span class="k">def</span> <span class="nf">binary_tensor_combine2</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">initrange</span><span class="o">=</span><span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;binary_tensor_combine2&#39;</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">khatri_rao</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">weights</span><span class="p">(</span><span class="s1">&#39;tnorm&#39;</span><span class="p">,</span>
                    <span class="p">[</span><span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span>
                     <span class="n">output_dim</span><span class="p">],</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span></div>

<span class="c1"># ==================================================================================</span>
<span class="c1"># =============EVALUATION METRICS / LOSS FUNCTIONS==================================</span>
<span class="c1"># ==================================================================================</span>

<span class="nd">@loss_function</span>
<div class="viewcode-block" id="se"><a class="viewcode-back" href="../node_ops.html#node_ops.se">[docs]</a><span class="k">def</span> <span class="nf">se</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Squared Error.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">targets</span><span class="p">))</span></div>

<span class="nd">@loss_function</span>
<div class="viewcode-block" id="mse"><a class="viewcode-back" href="../node_ops.html#node_ops.mse">[docs]</a><span class="k">def</span> <span class="nf">mse</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Mean Squared Error.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">targets</span><span class="p">))</span></div>

<span class="nd">@loss_function</span>
<div class="viewcode-block" id="rmse"><a class="viewcode-back" href="../node_ops.html#node_ops.rmse">[docs]</a><span class="k">def</span> <span class="nf">rmse</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Root Mean Squared Error</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)))</span></div>

<span class="nd">@loss_function</span>
<div class="viewcode-block" id="mae"><a class="viewcode-back" href="../node_ops.html#node_ops.mae">[docs]</a><span class="k">def</span> <span class="nf">mae</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Mean Absolute Error&#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">targets</span><span class="p">))</span></div>


<span class="nd">@loss_function</span>
<div class="viewcode-block" id="other_cross_entropy"><a class="viewcode-back" href="../node_ops.html#node_ops.other_cross_entropy">[docs]</a><span class="k">def</span> <span class="nf">other_cross_entropy</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Logistic Loss&#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">targets</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">predictions</span><span class="p">))</span></div>

<span class="nd">@loss_function</span>
<div class="viewcode-block" id="cross_entropy"><a class="viewcode-back" href="../node_ops.html#node_ops.cross_entropy">[docs]</a><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">targets</span><span class="o">*</span><span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">predictions</span> <span class="o">+</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">8</span><span class="p">))</span></div>

<span class="nd">@loss_function</span>
<div class="viewcode-block" id="perplexity"><a class="viewcode-back" href="../node_ops.html#node_ops.perplexity">[docs]</a><span class="k">def</span> <span class="nf">perplexity</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">))</span></div>

<span class="nd">@loss_function</span>
<div class="viewcode-block" id="detection"><a class="viewcode-back" href="../node_ops.html#node_ops.detection">[docs]</a><span class="k">def</span> <span class="nf">detection</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">greater_equal</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">threshold</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span></div>

<span class="nd">@loss_function</span>
<div class="viewcode-block" id="recall"><a class="viewcode-back" href="../node_ops.html#node_ops.recall">[docs]</a><span class="k">def</span> <span class="nf">recall</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">detects</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Percentage of actual classes predicted</span>

<span class="sd">    :param targets: A one hot encoding of class labels (num_points X numclasses)</span>
<span class="sd">    :param predictions: A real valued matrix with indices ranging between zero and 1 (num_points X numclasses)</span>
<span class="sd">    :param threshold: The detection threshold (between zero and 1)</span>
<span class="sd">    :param detects: In case detection is precomputed for efficiency when evaluating both precision and recall</span>
<span class="sd">    :return: A scalar value</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">detects</span><span class="p">:</span>
        <span class="n">detects</span> <span class="o">=</span> <span class="n">detection</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">detects</span><span class="p">,</span> <span class="n">targets</span><span class="p">)),</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">targets</span><span class="p">))</span></div>

<span class="nd">@loss_function</span>
<div class="viewcode-block" id="precision"><a class="viewcode-back" href="../node_ops.html#node_ops.precision">[docs]</a><span class="k">def</span> <span class="nf">precision</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">detects</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Percentage of classes detected which are correct.</span>

<span class="sd">    :param targets: A one hot encoding of class labels (num_points X numclasses)</span>
<span class="sd">    :param predictions: A real valued matrix with indices ranging between zero and 1 (num_points X numclasses)</span>
<span class="sd">    :param threshold: The detection threshold (between zero and 1)</span>
<span class="sd">    :param detects: In case detection is precomputed for efficiency when evaluating both precision and recall</span>
<span class="sd">    :return: A scalar value</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">detects</span><span class="p">:</span>
        <span class="n">detects</span> <span class="o">=</span> <span class="n">detection</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">detects</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">detects</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">8</span><span class="p">)</span></div>

<span class="nd">@loss_function</span>
<div class="viewcode-block" id="fscore"><a class="viewcode-back" href="../node_ops.html#node_ops.fscore">[docs]</a><span class="k">def</span> <span class="nf">fscore</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">precisions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">recalls</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">precisions</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">recalls</span><span class="p">:</span>
        <span class="n">detects</span> <span class="o">=</span> <span class="n">detection</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
        <span class="n">recalls</span> <span class="o">=</span> <span class="n">recall</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span> <span class="n">detects</span><span class="o">=</span><span class="n">detects</span><span class="p">)</span>
        <span class="n">precisions</span> <span class="o">=</span> <span class="n">precision</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span> <span class="n">detects</span><span class="o">=</span><span class="n">detects</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">precisions</span><span class="p">,</span> <span class="n">recalls</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">precisions</span> <span class="o">+</span> <span class="n">recalls</span> <span class="o">+</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">8</span><span class="p">))</span></div>

<span class="nd">@loss_function</span>
<div class="viewcode-block" id="accuracy"><a class="viewcode-back" href="../node_ops.html#node_ops.accuracy">[docs]</a><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">correct_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct_prediction</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li><a href="../index.html">home</a>|&nbsp;</li>
        <li><a href="../search.html">search</a>|&nbsp;</li>

          <li class="nav-item nav-item-1"><a href="index.html" >Module code</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &copy; Copyright 2016, Aaron Tuor.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.4.5.
    </div>
  </body>
</html>