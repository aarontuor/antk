% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{00A0}{\nobreakspace}
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}
\usepackage{eqparbox}


\addto\captionsenglish{\renewcommand{\figurename}{Fig. }}
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\SetupFloatingEnvironment{literal-block}{name=Listing }



\title{antk Documentation}
\date{July 23, 2016}
\release{0.3.0}
\author{Aaron Tuor}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\setcounter{tocdepth}{1}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.67,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.67,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.67,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.55,0.13}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.41,0.55}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.41,0.55}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.71,0.32,0.80}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.55,0.00,0.55}{##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.41,0.55}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.71,0.32,0.80}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.12,0.53,0.61}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.67,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.80,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@ul=\underline\def\PYG@tc##1{\textcolor[rgb]{0.00,0.55,0.27}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.41,0.55}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.55,0.00}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.55,0.00}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.55,0.27}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.44,0.48,0.49}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.55,0.27}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.55,0.27}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.80,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.11,0.49,0.44}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.55,0.00,0.55}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.41,0.55}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.80,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@ch\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.55,0.13}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.71,0.32,0.80}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.55,0.00,0.55}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.80,0.42,0.13}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.55,0.00}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.55,0.13}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.55,0.00,0.55}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.55,0.13}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.71,0.32,0.80}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.65,0.09,0.09}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{0.89,0.82,0.82}{\strut ##1}}}
\expandafter\def\csname PYG@tok@mb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.71,0.32,0.80}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.80,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.11,0.49,0.44}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.71,0.32,0.80}{##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.55,0.00,0.55}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.71,0.32,0.80}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.55,0.00,0.55}{##1}}}
\expandafter\def\csname PYG@tok@cpf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.55,0.13}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.55,0.00,0.55}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.80,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.55,0.00,0.55}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.65,0.65,0.65}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.80,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.80,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.55,0.00,0.55}{##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.80,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.80,0.33,0.33}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZhy{\char`\-}
\def\PYGZsq{\char`\'}
\def\PYGZdq{\char`\"}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\renewcommand\PYGZsq{\textquotesingle}

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}
\emph{Principal Authors} \href{http://sw.cs.wwu.edu/~tuora/aarontuor/}{Aaron Tuor} , \href{http://fw.cs.wwu.edu/~hutchib2/}{Brian Hutchinson}



The Automated Neural-graph toolkit is a high level machine learning toolkit built on top of Google's \href{https://www.tensorflow.org/}{Tensorflow} to
facilitate rapid prototyping of Neural Network models which may consist of multiple models chained together. This includes
models which have multiple input and/or multiple output streams.

ANTk will be most useful to people who have gone through some of the basic tensorflow tutorials, have some machine learning
background, and wish to take advantage
of some of tensorflow's more advanced features. The code itself is consistent, well-formatted, well-documented, and abstracted
only to a point necessary for code reuse, and complex model development. The toolkit code contains tensorflow usage developed and discovered over six
months of machine learning research conducted in tensorflow, by Hutch Research based out of Western Washington University's Computer Science Department.

The kernel of the toolkit is comprised of 4 independent, but
complementary modules:
\begin{quote}
\begin{description}
\item[{\textbf{loader:}}] \leavevmode
Implements a general purpose data loader for python non-sequential machine learning tasks.
Contains functions for common data pre-processing tasks.

\item[{\textbf{config:}}] \leavevmode
Facilitates the generation of complex tensorflow models, built from
compositions of tensorflow functions.

\item[{\textbf{node\_ops:}}] \leavevmode
Contains functions taking a tensor or structured list of tensors and returning a tensor or structured list of tensors.
The functions are commonly used compositions of tensorflow functions which operate on tensors.

\item[{\textbf{generic\_model:}}] \leavevmode
A general purpose model builder equipped with generic train, and predict functions which takes parameters for
optimization strategy, mini-batch, etc...

\end{description}
\end{quote}

\textbf{Motivation:}
\begin{quote}

Working at a high level of abstraction is important for the rapid
development of machine learning models. Many successful state of the art models chain together or create an ensemble of several
complex models. To facilitate the need for building models whose components are models we have
developed a highly modularized set of utilities.

While this high level of abstraction is often attractive for development, when working with a highly abstracted machine learning toolkit it
is often difficult to assess details of implementation and the underlying math behind a packaged model. To address this concern
we have made the toolkit implementation and underlying math as transparent as possible. There are links to source code, and relevant scientific papers
in the API and we have taken pains to illuminate the workings of complex code with well formatted mathematical equations. Also,
we have taken care to allow easy access to tensor objects created by high level operations such as deep neural networks.
\end{quote}

\textbf{Design methodology:}
\begin{quote}

ANTK was designed to be highly modular, and allow for a high level of abstraction with a great degree of transparency to
the underlying implementation. We hope that this design can eliminate the reproduction of coding efforts without sacrificing important
knowledge about implementation that may effect the overall performance of a model.
\end{quote}


\chapter{Dependencies}
\label{index:about-antk}\label{index:dependencies}
Tensorflow, scipy, numpy, sklearn, graphviz.

\href{https://www.tensorflow.org/versions/r0.7/get\_started/os\_setup.html}{Install tensorflow}

\href{http://www.graphviz.org/}{Install graphviz}


\chapter{Installation}
\label{index:installation}
A virtual environment is recommended for installation. Make sure that tensorflow is installed in your virtual environment
and graphviz is installed on your system.

In a terminal:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{o}{(}venv\PYG{o}{)}\PYGZdl{} mkdir antk
\PYG{o}{(}venv\PYG{o}{)}\PYGZdl{} \PYG{n+nb}{cd} antk
\PYG{o}{(}venv\PYG{o}{)}\PYGZdl{} git init
Initialized empty Git repository in /home/tuora/garbage/.git/
\PYG{o}{(}venv\PYG{o}{)}\PYGZdl{} git remote add origin https://github.com/aarontuor/antk.git
\PYG{o}{(}venv\PYG{o}{)}\PYGZdl{} git pull origin master
...
\PYG{o}{(}venv\PYG{o}{)}\PYGZdl{} python setup.py develop
\end{Verbatim}


\chapter{Documentation}
\label{index:documentation}

\section{API: ANT modules}
\label{api::doc}\label{api:api-ant-modules}

\subsection{loader}
\label{loader::doc}\label{loader:tensor-decompositions-and-applications}\label{loader:loader}
Implements a general purpose data loader for python non-sequential machine learning tasks. Several common data transformations are provided in this module, e.g., tfidf, whitening, etc.


\subsubsection{Loader Tutorial}
\label{loader_tutorial:loader-tutorial}\label{loader_tutorial::doc}
The {\hyperref[loader::doc]{\emph{\emph{loader}}}} module implements a general purpose data loader for python non-sequential machine learning tasks.


\paragraph{Supported Data Types}
\label{loader_tutorial:supported-data-types}
{\hyperref[loader::doc]{\emph{\emph{loader}}}} is designed to operate on numpy arrays, scipy sparse csr\_matrices, and {\hyperref[loader:loader.HotIndex]{\emph{\code{HotIndex}}}} objects.


\paragraph{HotIndex objects}
\label{loader_tutorial:hotindex-objects}
In the discussion below we distinguish ``one hot'' meaning a matrix with exactly a single 1 per row and zeros elsewhere
from ``many hot'', matrices with only ones and zeros.
In order to address the pervasive need for one hot representations the loader module has some functions for creating
one hot matrices ({\hyperref[loader:loader.toOnehot]{\emph{\code{toOnehot}}}}), transforming one hots to indices ({\hyperref[loader:loader.toIndex]{\emph{\code{toIndex}}}}) and determining if a matrix is
a one hot representation ({\hyperref[loader:loader.is_one_hot]{\emph{\code{is\_one\_hot}}}}).

Also there is a compact index representation of a one hot matrix, the {\hyperref[loader:loader.HotIndex]{\emph{\code{HotIndex}}}} object which has a field to
retain the row size of the one hot matrix, while representing the \emph{on} columns by their indices alone.


\paragraph{Supported File Formats}
\label{loader_tutorial:supported-file-formats}\begin{quote}
\begin{description}
\item[{\textbf{.mat}:}] \leavevmode
Matlab files of matrices made with the matlab save command. Saved matrices to be read must be named \textbf{data}. As of
now some Matlab implementations may load the files with the \emph{load} function but the loaded matrices will have different values.

\item[{\textbf{.sparsetxt}}] \leavevmode
Plain text files where lines correspond to an entry in a matrix where a line consists of values \textbf{i j k}, so a matrix \emph{A} is constructed where  \(A_{ij} = k\). Tokens must be whitespace delimited.

\item[{\textbf{.densetxt}:}] \leavevmode
Plain text files with a matrix represented in standard form. Tokens must be whitespace delimited.

\item[{\textbf{.sparse}:}] \leavevmode
Like \code{.sparsetxt} files but written in binary (no delimiters) to save disk space and speed file i/o. Matrix dimensions are contained in the first bytes of the file.

\item[{\textbf{.binary} / \textbf{.dense}:}] \leavevmode
Like \code{.densetxt} files but written in binary (no delimiters) to save disk space and speed file i/o. Matrix dimensions are contained in the first bytes of the file.

\item[{\textbf{.index}:}] \leavevmode
A saved {\hyperref[loader:loader.HotIndex]{\emph{\code{HotIndex}}}} object written in binary.

\end{description}
\end{quote}


\paragraph{Import and export data}
\label{loader_tutorial:import-and-export-data}\begin{quote}

{\hyperref[loader:loader.export_data]{\emph{\code{export\_data}}}} : Scipy sparse matrices and numpy arrays may be saved to a supported file format with this
function.

{\hyperref[loader:loader.import_data]{\emph{\code{import\_data}}}}: Scipy sparse matrices and numpy arrays may be loaded from a supported file format with this
function.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{antk.core} \PYG{k+kn}{import} \PYG{n}{loader}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{numpy}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{test} \PYG{o}{=} \PYG{n}{numpy}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{test}
\PYG{g+go}{array([[ 0.65769658,  0.22230913,  0.41058879],}
\PYG{g+go}{      [ 0.71498391,  0.47537034,  0.88214378],}
\PYG{g+go}{      [ 0.37795028,  0.02388658,  0.41103339]])}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{loader}\PYG{o}{.}\PYG{n}{export\PYGZus{}data}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test.mat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{test}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{loader}\PYG{o}{.}\PYG{n}{import\PYGZus{}data}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test.mat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{g+go}{array([[ 0.65769658,  0.22230913,  0.41058879],}
\PYG{g+go}{      [ 0.71498391,  0.47537034,  0.88214378],}
\PYG{g+go}{      [ 0.37795028,  0.02388658,  0.41103339]])}
\end{Verbatim}
\end{quote}


\paragraph{The \texttt{DataSet} object}
\label{loader_tutorial:the-dataset-object}
{\hyperref[loader:loader.DataSet]{\emph{\code{DataSet}}}} objects are designed to make data manipulation easier for mini-batch gradient descent training. It is necessary
to package your data in a {\hyperref[loader:loader.DataSet]{\emph{\code{DataSet}}}} object in order to create a {\hyperref[generic_model:generic_model.Model]{\emph{\code{Model}}}} object from antk's {\hyperref[generic_model::doc]{\emph{\emph{generic\_model}}}}
module. You can create a DataSet with a dictionary of numpy arrays, scipy sparse csr\_matrices, and {\hyperref[loader:loader.HotIndex]{\emph{\code{HotIndex}}}} objects.
\begin{quote}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{test2} \PYG{o}{=} \PYG{n}{numpy}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{test3} \PYG{o}{=} \PYG{n}{numpy}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{datadict} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{feature1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{test}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{feature2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{test2}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{feature3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{test3}\PYG{p}{\PYGZcb{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{data} \PYG{o}{=} \PYG{n}{loader}\PYG{o}{.}\PYG{n}{DataSet}\PYG{p}{(}\PYG{n}{datadict}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{data}
\PYG{g+go}{antk.core.DataSet object with fields:}
\PYG{g+go}{    \PYGZsq{}\PYGZus{}labels\PYGZsq{}: \PYGZob{}\PYGZcb{}}
\PYG{g+go}{    \PYGZsq{}\PYGZus{}num\PYGZus{}examples\PYGZsq{}: 3}
\PYG{g+go}{    \PYGZsq{}\PYGZus{}epochs\PYGZus{}completed\PYGZsq{}: 0}
\PYG{g+go}{    \PYGZsq{}\PYGZus{}index\PYGZus{}in\PYGZus{}epoch\PYGZsq{}: 0}
\PYG{g+go}{    \PYGZsq{}\PYGZus{}mix\PYGZus{}after\PYGZus{}epoch\PYGZsq{}: False}
\PYG{g+go}{    \PYGZsq{}\PYGZus{}features\PYGZsq{}: \PYGZob{}\PYGZsq{}feature2\PYGZsq{}: array([[ 0.3053935 ,  0.19926099,  0.43178954,  0.21737312],}
\PYG{g+go}{   [ 0.47352974,  0.33052605,  0.22874512,  0.59903599],}
\PYG{g+go}{   [ 0.62532971,  0.70029533,  0.13582899,  0.39699691]]), \PYGZsq{}feature3\PYGZsq{}: array([[ 0.98901453,  0.48172019,  0.55349593,  0.88056326,  0.87455635],}
\PYG{g+go}{   [ 0.46123761,  0.94292179,  0.13315178,  0.55212266,  0.09410787],}
\PYG{g+go}{   [ 0.90358241,  0.88080438,  0.51443528,  0.69531831,  0.32700497]]), \PYGZsq{}feature1\PYGZsq{}: array([[ 0.55351649,  0.94648234,  0.83976935],}
\PYG{g+go}{   [ 0.95176126,  0.37265882,  0.72076518],}
\PYG{g+go}{   [ 0.97364273,  0.79038134,  0.83085418]])\PYGZcb{}}
\end{Verbatim}
\end{quote}

There is a {\hyperref[loader:loader.DataSet.show]{\emph{\code{DataSet.show}}}} method that will display information about the DataSet.
\begin{quote}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{data}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\PYG{g+go}{features:}
\PYG{g+go}{         feature2: (3, 4) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{         feature3: (3, 5) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{         feature1: (3, 3) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{labels:}
\end{Verbatim}
\end{quote}

There is an optional argument for labels in case you wish to have features and labels in separate maps.
\begin{quote}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{label} \PYG{o}{=} \PYG{n}{numpy}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{data} \PYG{o}{=} \PYG{n}{loader}\PYG{o}{.}\PYG{n}{DataSet}\PYG{p}{(}\PYG{n}{datadict}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{label1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{label}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{data}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\PYG{g+go}{features:}
\PYG{g+go}{                 feature2: (3, 4) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{                 feature3: (3, 5) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{                 feature1: (3, 3) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{labels:}
\PYG{g+go}{                 label1: (3, 10) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\end{Verbatim}
\end{quote}

Matrices in the DataSet can be accessed by their keys.
\begin{quote}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{data}\PYG{o}{.}\PYG{n}{features}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{feature1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{g+go}{array([[ 0.65769658,  0.22230913,  0.41058879],}
\PYG{g+go}{      [ 0.71498391,  0.47537034,  0.88214378],}
\PYG{g+go}{      [ 0.37795028,  0.02388658,  0.41103339]])}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{data}\PYG{o}{.}\PYG{n}{labels}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{label1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{g+go}{        array([[ 0.95719927,  0.5568232 ,  0.18691618,  0.74473549,  0.13150579,}
\PYG{g+go}{                         0.18189613,  0.00841565,  0.36285286,  0.52124701,  0.90096317],}
\PYG{g+go}{                   [ 0.73361071,  0.0939201 ,  0.22622336,  0.47731619,  0.91260044,}
\PYG{g+go}{                         0.98467187,  0.01978079,  0.93664054,  0.92857152,  0.25710894],}
\PYG{g+go}{                   [ 0.024292  ,  0.92705842,  0.0086137 ,  0.33100848,  0.93829355,}
\PYG{g+go}{                         0.04615762,  0.91809485,  0.79796301,  0.88414445,  0.72963613]])}
\end{Verbatim}
\end{quote}

If your data is structured so that your features and labels have rows corresponding to data points then you
can use the {\hyperref[loader:loader.DataSet.next_batch]{\emph{\code{next\_batch}}}} function to grab data for a mini-batch iteration in stochastic gradient descent.
\begin{quote}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{minibatch} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{next\PYGZus{}batch}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{minibatch}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\PYG{g+go}{features:}
\PYG{g+go}{         feature2: (2, 3) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{         feature3: (2, 3) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{         feature1: (2, 3) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{labels:}
\PYG{g+go}{         label1: (2, 10) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\end{Verbatim}
\end{quote}

You can ensure that the order of the data points is shuffled every epoch with the {\hyperref[loader:loader.DataSet.mix_after_epoch]{\emph{\code{mix\_after\_epoch}}}} function,
and see how many epochs the data has been trained with from the \code{epochs\_completed} property.
\begin{quote}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{data}\PYG{o}{.}\PYG{n}{mix\PYGZus{}after\PYGZus{}epoch}\PYG{p}{(}\PYG{n+nb+bp}{True}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{data}\PYG{o}{.}\PYG{n}{next\PYGZus{}batch}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{g+go}{\PYGZlt{}antk.core.loader.DataSet object at 0x7f5c48dc6b10\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{data}\PYG{o}{.}\PYG{n}{epochs\PYGZus{}completed}
\PYG{g+go}{1}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{data}\PYG{o}{.}\PYG{n}{features}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{features1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{g+go}{array([[ 0.71498391,  0.47537034,  0.88214378],}
\PYG{g+go}{       [ 0.65769658,  0.22230913,  0.41058879],}
\PYG{g+go}{       [ 0.37795028,  0.02388658,  0.41103339]])}
\end{Verbatim}
\end{quote}


\paragraph{read\_data\_sets: The loading function}
\label{loader_tutorial:read-data-sets-the-loading-function}
{\hyperref[loader:loader.read_data_sets]{\emph{\code{read\_data\_sets}}}} will automatically load folders of data of the supported file formats into a {\hyperref[loader:loader.DataSets]{\emph{\code{DataSets}}}} object,
which is just a record of DataSet objects with a \emph{show()} method to display all the datasets at once.
Below are some things to know before using the {\hyperref[loader:loader.read_data_sets]{\emph{\code{read\_data\_sets}}}} function.


\subparagraph{Directory Structure}
\label{loader_tutorial:directory-structure}
\includegraphics{{directory}.png}

\emph{directory} at the top level can be named whatever. There are by default assumed to be three directories below \emph{directory} named \textbf{train}, \textbf{dev}, and \textbf{test}. However one may choose to read data from any collection of directories using the \emph{folders} argument.
If the directories specified are not present {\hyperref[loader:loader.Bad_directory_structure_error]{\emph{\code{Bad\_directory\_structure\_error}}}} will be raised during loading. The top level directory may contain other files besides the listed directories. According to the diagram:
\begin{quote}

\emph{N} is the number of feature sets. Not to be confused with the number of elements in a feature vector for a particular feature set.
\emph{Q} is the number of label sets. Not to be confused with the number of elements in a label vector for a particular label set.
The hash for a matrix in a {\hyperref[loader:loader.DataSet.features]{\emph{\code{DataSet.features}}}} attribute is whatever is between \textbf{features\_} and the file extension (\emph{.ext}) in the file name.
The hash for a matrix in a {\hyperref[loader:loader.DataSet.labels]{\emph{\code{DataSet.labels}}}} attribute is whatever is between \textbf{labels\_} and the file extension (\emph{.ext}) in the file name.
\end{quote}

\begin{notice}{note}{Note:}
Rows of feature and data matrices should correspond to individual data points as opposed to the transpose.
There should be the same number of data points in each file of the \textbf{train} directory, and the same is true for
the \textbf{dev} and \textbf{test} directories. The number of data points can of course vary between \textbf{dev}, \textbf{train}, and \textbf{test} directories.
If you have data you want to load that doesn't correspond to the paradigm of matrices which have a number of data points columns there you may use the {\hyperref[loader:loader.read_data_sets]{\emph{\code{read\_data\_sets}}}} \textbf{folders} argument (a list of folder names) to include other directories besides \textbf{dev}, \textbf{train}, and \textbf{test}. In this case all and only the folders specified by the \textbf{folders} argument will be loaded into a {\hyperref[loader:loader.DataSets]{\emph{\code{DataSets}}}} object.
\end{notice}


\subparagraph{Examples}
\label{loader_tutorial:examples}
Below we download, untar, and load a processed and supplemented Movielens 100k dataset, where data points are user/item pairs
for observed movie ratings.

\textbf{Basic usage:}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{loader}\PYG{o}{.}\PYG{n}{maybe\PYGZus{}download}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ml100k.tar.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{http://sw.cs.wwu.edu/\PYGZti{}tuora/aarontuor/ml100k.tar.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{loader}\PYG{o}{.}\PYG{n}{untar}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ml100k.tar.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{loader}\PYG{o}{.}\PYG{n}{read\PYGZus{}data\PYGZus{}sets}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ml100k).show()}
\PYG{g+go}{reading train...}
\PYG{g+go}{reading dev...}
\PYG{g+go}{reading test...}
\PYG{g+go}{dev:}
\PYG{g+go}{features:}
\PYG{g+go}{        item: vec.shape: (10000,) dim: 1682 \PYGZlt{}class \PYGZsq{}antk.core.loader.HotIndex\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        user: vec.shape: (10000,) dim: 943 \PYGZlt{}class \PYGZsq{}antk.core.loader.HotIndex\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        words: (10000, 12734) \PYGZlt{}class \PYGZsq{}scipy.sparse.csc.csc\PYGZus{}matrix\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        time: (10000, 1) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{labels:}
\PYG{g+go}{        genre: (10000, 19) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        ratings: (10000, 1) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        genre\PYGZus{}dist: (10000, 19) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{test:}
\PYG{g+go}{features:}
\PYG{g+go}{        item: vec.shape: (10000,) dim: 1682 \PYGZlt{}class \PYGZsq{}antk.core.loader.HotIndex\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        user: vec.shape: (10000,) dim: 943 \PYGZlt{}class \PYGZsq{}antk.core.loader.HotIndex\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        words: (10000, 12734) \PYGZlt{}class \PYGZsq{}scipy.sparse.csc.csc\PYGZus{}matrix\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        time: (10000, 1) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{labels:}
\PYG{g+go}{        genre: (10000, 19) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        ratings: (10000, 1) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        genre\PYGZus{}dist: (10000, 19) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{train:}
\PYG{g+go}{features:}
\PYG{g+go}{item: vec.shape: (80000,) dim: 1682 \PYGZlt{}class \PYGZsq{}antk.core.loader.HotIndex\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        user: vec.shape: (80000,) dim: 943 \PYGZlt{}class \PYGZsq{}antk.core.loader.HotIndex\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        words: (80000, 12734) \PYGZlt{}class \PYGZsq{}scipy.sparse.csc.csc\PYGZus{}matrix\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        time: (80000, 1) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{labels:}
\PYG{g+go}{        genre: (80000, 19) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        ratings: (80000, 1) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        genre\PYGZus{}dist: (80000, 19) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\end{Verbatim}

\textbf{Other Folders:}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{loader}\PYG{o}{.}\PYG{n}{read\PYGZus{}data\PYGZus{}sets}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ml100k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{folders}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{item}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\PYG{g+go}{reading user...}
\PYG{g+go}{reading item...}
\PYG{g+go}{item:}
\PYG{g+go}{features:}
\PYG{g+go}{        genres: (1682, 19) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        bin\PYGZus{}doc\PYGZus{}term: (1682, 12734) \PYGZlt{}class \PYGZsq{}scipy.sparse.csc.csc\PYGZus{}matrix\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        month: vec.shape: (1682,) dim: 12 \PYGZlt{}class \PYGZsq{}antk.core.loader.HotIndex\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        doc\PYGZus{}term: (1682, 12734) \PYGZlt{}class \PYGZsq{}scipy.sparse.csc.csc\PYGZus{}matrix\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        tfidf\PYGZus{}doc\PYGZus{}term: (1682, 12734) \PYGZlt{}class \PYGZsq{}scipy.sparse.csc.csc\PYGZus{}matrix\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        year: (1682, 1) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{labels:}
\PYG{g+go}{user:}
\PYG{g+go}{features:}
\PYG{g+go}{        occ: vec.shape: (943,) dim: 21 \PYGZlt{}class \PYGZsq{}antk.core.loader.HotIndex\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        age: (943, 1) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        zip: vec.shape: (943,) dim: 1000 \PYGZlt{}class \PYGZsq{}antk.core.loader.HotIndex\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        sex: vec.shape: (943,) dim: 2 \PYGZlt{}class \PYGZsq{}antk.core.loader.HotIndex\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{labels:}
\end{Verbatim}

\textbf{Selecting Files:}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{loader}\PYG{o}{.}\PYG{n}{read\PYGZus{}data\PYGZus{}sets}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ml100k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{folders}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{item}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{hashlist}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{zip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\PYG{g+go}{reading user...}
\PYG{g+go}{reading item...}
\PYG{g+go}{item:}
\PYG{g+go}{features:}
\PYG{g+go}{        year: (1682, 1) \PYGZlt{}type \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{labels:}
\PYG{g+go}{user:}
\PYG{g+go}{features:}
\PYG{g+go}{        zip: vec.shape: (943,) dim: 1000 \PYGZlt{}class \PYGZsq{}antk.core.loader.HotIndex\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{        sex: vec.shape: (943,) dim: 2 \PYGZlt{}class \PYGZsq{}antk.core.loader.HotIndex\PYGZsq{}\PYGZgt{}}
\PYG{g+go}{labels:}
\end{Verbatim}


\paragraph{Loading, Saving, and Testing}
\label{loader_tutorial:loading-saving-and-testing}
{\hyperref[loader:loader.export_data]{\emph{\code{export\_data}}}}

{\hyperref[loader:loader.import_data]{\emph{\code{import\_data}}}}

{\hyperref[loader:loader.is_one_hot]{\emph{\code{is\_one\_hot}}}}

{\hyperref[loader:loader.read_data_sets]{\emph{\code{read\_data\_sets}}}}


\paragraph{Classes}
\label{loader_tutorial:classes}
{\hyperref[loader:loader.DataSet]{\emph{\code{DataSet}}}}

{\hyperref[loader:loader.DataSets]{\emph{\code{DataSets}}}}

{\hyperref[loader:loader.HotIndex]{\emph{\code{HotIndex}}}}


\paragraph{Data Transforms}
\label{loader_tutorial:data-transforms}
{\hyperref[loader:loader.center]{\emph{\code{center}}}}

{\hyperref[loader:loader.l1normalize]{\emph{\code{l1normalize}}}}

{\hyperref[loader:loader.l2normalize]{\emph{\code{l2normalize}}}}

{\hyperref[loader:loader.pca_whiten]{\emph{\code{pca\_whiten}}}}

{\hyperref[loader:loader.tfidf]{\emph{\code{tfidf}}}}

{\hyperref[loader:loader.toOnehot]{\emph{\code{toOnehot}}}}

{\hyperref[loader:loader.toIndex]{\emph{\code{toIndex}}}}

{\hyperref[loader:loader.unit_variance]{\emph{\code{unit\_variance}}}}


\paragraph{Exceptions}
\label{loader_tutorial:exceptions}
{\hyperref[loader:loader.Bad_directory_structure_error]{\emph{\code{Bad\_directory\_structure\_error}}}}

{\hyperref[loader:loader.Mat_format_error]{\emph{\code{Mat\_format\_error}}}}

{\hyperref[loader:loader.Sparse_format_error]{\emph{\code{Sparse\_format\_error}}}}

{\hyperref[loader:loader.Unsupported_format_error]{\emph{\code{Unsupported\_format\_error}}}}


\subsubsection{Loading, Saving, and Testing}
\label{loader:loading-saving-and-testing}
{\hyperref[loader:loader.save]{\emph{\code{save}}}}

{\hyperref[loader:loader.export_data]{\emph{\code{export\_data}}}}

{\hyperref[loader:loader.load]{\emph{\code{load}}}}

{\hyperref[loader:loader.import_data]{\emph{\code{import\_data}}}}

{\hyperref[loader:loader.is_one_hot]{\emph{\code{is\_one\_hot}}}}

{\hyperref[loader:loader.read_data_sets]{\emph{\code{read\_data\_sets}}}}


\subsubsection{Classes}
\label{loader:classes}
{\hyperref[loader:loader.DataSet]{\emph{\code{DataSet}}}}

{\hyperref[loader:loader.DataSets]{\emph{\code{DataSets}}}}

{\hyperref[loader:loader.HotIndex]{\emph{\code{HotIndex}}}}


\subsubsection{Data Transforms}
\label{loader:data-transforms}
{\hyperref[loader:loader.center]{\emph{\code{center}}}}

{\hyperref[loader:loader.l1normalize]{\emph{\code{l1normalize}}}}

{\hyperref[loader:loader.l2normalize]{\emph{\code{l2normalize}}}}

{\hyperref[loader:loader.pca_whiten]{\emph{\code{pca\_whiten}}}}

{\hyperref[loader:loader.tfidf]{\emph{\code{tfidf}}}}

{\hyperref[loader:loader.toOnehot]{\emph{\code{toOnehot}}}}

{\hyperref[loader:loader.toIndex]{\emph{\code{toIndex}}}}

{\hyperref[loader:loader.unit_variance]{\emph{\code{unit\_variance}}}}


\subsubsection{Exceptions}
\label{loader:exceptions}
{\hyperref[loader:loader.Bad_directory_structure_error]{\emph{\code{Bad\_directory\_structure\_error}}}}

{\hyperref[loader:loader.Mat_format_error]{\emph{\code{Mat\_format\_error}}}}

{\hyperref[loader:loader.Sparse_format_error]{\emph{\code{Sparse\_format\_error}}}}

{\hyperref[loader:loader.Unsupported_format_error]{\emph{\code{Unsupported\_format\_error}}}}


\subsubsection{API}
\label{loader:module-loader}\label{loader:api}\index{loader (module)}\index{Bad\_directory\_structure\_error}

\begin{fulllineitems}
\phantomsection\label{loader:loader.Bad_directory_structure_error}\pysigline{\strong{exception }\code{loader.}\bfcode{Bad\_directory\_structure\_error}}
Raised when a data directory specified, does not contain a subfolder specified in the \emph{folders} argument to {\hyperref[loader:loader.read_data_sets]{\emph{\code{read\_data\_sets}}}}.

\end{fulllineitems}

\index{DataSet (class in loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.DataSet}\pysiglinewithargsret{\strong{class }\code{loader.}\bfcode{DataSet}}{\emph{features}, \emph{labels=None}, \emph{num\_examples=None}, \emph{mix=False}}{}
General data structure for mini-batch gradient descent training involving non-sequential data.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{features}} -- A dictionary of string label names to data matrices. Matrices may be {\hyperref[loader:loader.HotIndex]{\emph{\code{HotIndex}}}}, scipy sparse csr\_matrix, or numpy arrays.

\item {} 
\textbf{\texttt{labels}} -- A dictionary of string label names to data matrices. Matrices may be {\hyperref[loader:loader.HotIndex]{\emph{\code{HotIndex}}}}, scipy sparse csr\_matrix, or numpy arrays.

\item {} 
\textbf{\texttt{num\_examples}} -- How many data points.

\item {} 
\textbf{\texttt{mix}} -- Whether or not to shuffle per epoch.

\end{itemize}

\item[{Returns}] \leavevmode


\end{description}\end{quote}
\paragraph{Attributes}
\paragraph{Methods}
\index{features (loader.DataSet attribute)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.DataSet.features}\pysigline{\bfcode{features}}
A dictionary of feature matrices.

\end{fulllineitems}

\index{index\_in\_epoch (loader.DataSet attribute)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.DataSet.index_in_epoch}\pysigline{\bfcode{index\_in\_epoch}}
The number of datapoints that have been trained on in a particular epoch.

\end{fulllineitems}

\index{labels (loader.DataSet attribute)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.DataSet.labels}\pysigline{\bfcode{labels}}
A dictionary of label matrices

\end{fulllineitems}

\index{mix\_after\_epoch() (loader.DataSet method)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.DataSet.mix_after_epoch}\pysiglinewithargsret{\bfcode{mix\_after\_epoch}}{\emph{mix}}{}
Whether or not to shuffle after training for an epoch.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{mix}} -- True or False

\end{description}\end{quote}

\end{fulllineitems}

\index{next\_batch() (loader.DataSet method)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.DataSet.next_batch}\pysiglinewithargsret{\bfcode{next\_batch}}{\emph{batch\_size}}{}~\begin{description}
\item[{Return a sub DataSet of next batch-size examples.}] \leavevmode\begin{description}
\item[{If shuffling enabled:}] \leavevmode
If \emph{batch\_size}
is greater than the number of examples left in the epoch then a batch size DataSet wrapping back to
beginning will be returned.

\item[{If shuffling turned off:}] \leavevmode
If \emph{batch\_size} is greater than the number of examples left in the epoch, points will be shuffled and
batch\_size DataSet is returned starting from index 0.

\end{description}

\end{description}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{batch\_size}} -- int

\item[{Returns}] \leavevmode
A {\hyperref[loader:loader.DataSet]{\emph{\code{DataSet}}}} object with the next \emph{batch\_size} examples.

\end{description}\end{quote}

\end{fulllineitems}

\index{num\_examples (loader.DataSet attribute)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.DataSet.num_examples}\pysigline{\bfcode{num\_examples}}
Number of rows (data points) of the matrices in this {\hyperref[loader:loader.DataSet]{\emph{\code{DataSet}}}}.

\end{fulllineitems}

\index{show() (loader.DataSet method)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.DataSet.show}\pysiglinewithargsret{\bfcode{show}}{}{}
Pretty printing of all the data (dimensions, keys, type) in the {\hyperref[loader:loader.DataSet]{\emph{\code{DataSet}}}} object

\end{fulllineitems}

\index{showmore() (loader.DataSet method)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.DataSet.showmore}\pysiglinewithargsret{\bfcode{showmore}}{}{}
Print a sample of the first up to twenty rows of matrices in DataSet

\end{fulllineitems}


\end{fulllineitems}

\index{DataSets (class in loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.DataSets}\pysiglinewithargsret{\strong{class }\code{loader.}\bfcode{DataSets}}{\emph{datasets\_map}, \emph{mix=False}}{}
A record of DataSet objects with a display function.
\paragraph{Methods}
\index{show() (loader.DataSets method)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.DataSets.show}\pysiglinewithargsret{\bfcode{show}}{}{}
Pretty print data attributes.

\end{fulllineitems}

\index{showmore() (loader.DataSets method)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.DataSets.showmore}\pysiglinewithargsret{\bfcode{showmore}}{}{}
Pretty print data attributes, and data.

\end{fulllineitems}


\end{fulllineitems}

\index{HotIndex (class in loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.HotIndex}\pysiglinewithargsret{\strong{class }\code{loader.}\bfcode{HotIndex}}{\emph{matrix}, \emph{dimension=None}}{}
Index vector representation of one hot matrix. Can hand constructor either a one hot matrix, or vector of indices
and dimension.
\paragraph{Attributes}
\paragraph{Methods}
\index{dim (loader.HotIndex attribute)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.HotIndex.dim}\pysigline{\bfcode{dim}}
The feature dimension of the one hot vector represented as indices.

\end{fulllineitems}

\index{hot() (loader.HotIndex method)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.HotIndex.hot}\pysiglinewithargsret{\bfcode{hot}}{}{}~\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
A one hot scipy sparse csr\_matrix

\end{description}\end{quote}

\end{fulllineitems}

\index{shape (loader.HotIndex attribute)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.HotIndex.shape}\pysigline{\bfcode{shape}}
The shape of the one hot matrix encoded.

\end{fulllineitems}

\index{vec (loader.HotIndex attribute)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.HotIndex.vec}\pysigline{\bfcode{vec}}
The vector of hot indices.

\end{fulllineitems}


\end{fulllineitems}

\index{IndexVector (class in loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.IndexVector}\pysiglinewithargsret{\strong{class }\code{loader.}\bfcode{IndexVector}}{\emph{matrix}, \emph{dimension=None}}{}~\paragraph{Attributes}
\paragraph{Methods}

\end{fulllineitems}

\index{Mat\_format\_error}

\begin{fulllineitems}
\phantomsection\label{loader:loader.Mat_format_error}\pysigline{\strong{exception }\code{loader.}\bfcode{Mat\_format\_error}}
Raised if the .mat file being read does not contain a
variable named \emph{data}.

\end{fulllineitems}

\index{Sparse\_format\_error}

\begin{fulllineitems}
\phantomsection\label{loader:loader.Sparse_format_error}\pysigline{\strong{exception }\code{loader.}\bfcode{Sparse\_format\_error}}
Raised when reading a plain text file with .sparsetxt
extension and there are not three entries per line.

\end{fulllineitems}

\index{Unsupported\_format\_error}

\begin{fulllineitems}
\phantomsection\label{loader:loader.Unsupported_format_error}\pysigline{\strong{exception }\code{loader.}\bfcode{Unsupported\_format\_error}}
Raised when a file is requested to be loaded or saved without one of the supported file extensions.

\end{fulllineitems}

\index{center() (in module loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.center}\pysiglinewithargsret{\code{loader.}\bfcode{center}}{\emph{X}, \emph{axis=None}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{X}} -- A matrix to center about the mean(over columns axis=0, over rows axis=1, over all entries axis=None)

\item[{Returns}] \leavevmode
A matrix with entries centered along the specified axis.

\end{description}\end{quote}

\end{fulllineitems}

\index{export\_data() (in module loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.export_data}\pysiglinewithargsret{\code{loader.}\bfcode{export\_data}}{\emph{filename}, \emph{data}}{}
Decides how to save data by file extension.
Raises {\hyperref[loader:loader.Unsupported_format_error]{\emph{\code{Unsupported\_format\_error}}}} if extension is not one of the supported
extensions (mat, sparse, binary, dense, index).
Data contained in .mat files should be saved in a matrix named \emph{data}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{filename}} -- A file of an accepted format representing a matrix.

\item {} 
\textbf{\texttt{data}} -- A numpy array, scipy sparse matrix, or {\hyperref[loader:loader.HotIndex]{\emph{\code{HotIndex}}}} object.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{import\_data() (in module loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.import_data}\pysiglinewithargsret{\code{loader.}\bfcode{import\_data}}{\emph{filename}}{}
Decides how to load data into python matrices by file extension.
Raises {\hyperref[loader:loader.Unsupported_format_error]{\emph{\code{Unsupported\_format\_error}}}} if extension is not one of the supported
extensions (mat, sparse, binary, dense, sparsetxt, densetxt, index).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{filename}} -- A file of an accepted format representing a matrix.

\item[{Returns}] \leavevmode
A numpy matrix, scipy sparse csr\_matrix, or any:\emph{HotIndex}.

\end{description}\end{quote}

\end{fulllineitems}

\index{is\_one\_hot() (in module loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.is_one_hot}\pysiglinewithargsret{\code{loader.}\bfcode{is\_one\_hot}}{\emph{A}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{A}} -- A numpy array or scipy sparse matrix

\item[{Returns}] \leavevmode
True if matrix is a sparse matrix of one hot vectors, False otherwise

\end{description}\end{quote}
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{numpy}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{antk.core} \PYG{k+kn}{import} \PYG{n}{loader}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{x} \PYG{o}{=} \PYG{n}{numpy}\PYG{o}{.}\PYG{n}{eye}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{loader}\PYG{o}{.}\PYG{n}{is\PYGZus{}one\PYGZus{}hot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{g+go}{True}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{x} \PYG{o}{*}\PYG{o}{=} \PYG{l+m+mi}{5}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{loader}\PYG{o}{.}\PYG{n}{is\PYGZus{}one\PYGZus{}hot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{g+go}{False}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{x} \PYG{o}{=} \PYG{n}{numpy}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{loader}\PYG{o}{.}\PYG{n}{is\PYGZus{}one\PYGZus{}hot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{g+go}{True}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{2}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{loader}\PYG{o}{.}\PYG{n}{is\PYGZus{}one\PYGZus{}hot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{g+go}{False}
\end{Verbatim}

\end{fulllineitems}

\index{l1normalize() (in module loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.l1normalize}\pysiglinewithargsret{\code{loader.}\bfcode{l1normalize}}{\emph{X}, \emph{axis=1}}{}
axis=1 normalizes each row of X by norm of said row. \(l1normalize(X)_{ij} = \frac{X_{ij}}{\sum_k |X_{ik}|}\)

axis=0 normalizes each column of X by norm of said column. \(l1normalize(X)_{ij} = \frac{X_{ij}}{\sum_k
|X_{kj}|}\)

axis=None normalizes entries of X  by norm of X. \(l1normalize(X)_{ij} = \frac{X_{ij}}{\sum_k \sum_p
|X_{kp}|}\)
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} -- A scipy sparse csr\_matrix or numpy array.

\item {} 
\textbf{\texttt{axis}} -- The dimension to normalize over.

\end{itemize}

\item[{Returns}] \leavevmode
A normalized matrix.

\end{description}\end{quote}

\end{fulllineitems}

\index{l2normalize() (in module loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.l2normalize}\pysiglinewithargsret{\code{loader.}\bfcode{l2normalize}}{\emph{X}, \emph{axis=1}}{}
axis=1 normalizes each row of X by norm of said row. \(l2normalize(X)_{ij} = \frac{X_{ij}}{\sqrt{\sum_k X_{
ik}^2}}\)

axis=0 normalizes each column of X by norm of said column. \(l2normalize(X)_{ij} = \frac{X_{ij}}{\sqrt{\sum_k
X_{kj}^2}}\)

axis=None normalizes entries of X  by norm of X. \(l2normalize(X)_{ij} = \frac{X_{ij}}{\sqrt{\sum_k \sum_p
X_{kp}^2}}\)
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} -- A scipy sparse csr\_matrix or numpy array.

\item {} 
\textbf{\texttt{axis}} -- The dimension to normalize over.

\end{itemize}

\item[{Returns}] \leavevmode
A normalized matrix.

\end{description}\end{quote}

\end{fulllineitems}

\index{load() (in module loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.load}\pysiglinewithargsret{\code{loader.}\bfcode{load}}{\emph{filename}}{}
Calls {\hyperref[loader:loader.import_data]{\emph{\code{import\_data}}}}.
Decides how to load data into python matrices by file extension.
Raises {\hyperref[loader:loader.Unsupported_format_error]{\emph{\code{Unsupported\_format\_error}}}} if extension is not one of the supported
extensions (mat, sparse, binary, dense, sparsetxt, densetxt, index).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{filename}} -- A file of an accepted format representing a matrix.

\item[{Returns}] \leavevmode
A numpy matrix, scipy sparse csr\_matrix, or any:\emph{HotIndex}.

\end{description}\end{quote}

\end{fulllineitems}

\index{makedirs() (in module loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.makedirs}\pysiglinewithargsret{\code{loader.}\bfcode{makedirs}}{\emph{datadirectory}, \emph{sub\_directory\_list=(`train'}, \emph{`dev'}, \emph{`test')}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{datadirectory}} -- Name of the directory you want to create containing the subdirectory folders.
If the directory already exists it will be populated with the subdirectory folders.

\item {} 
\textbf{\texttt{sub\_directory\_list}} -- The list of subdirectories you want to create

\end{itemize}

\item[{Returns}] \leavevmode
void

\end{description}\end{quote}

\end{fulllineitems}

\index{maxnormalize() (in module loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.maxnormalize}\pysiglinewithargsret{\code{loader.}\bfcode{maxnormalize}}{\emph{X}, \emph{axis=1}}{}
axis=1 normalizes each row of X by norm of said row. \(maxnormalize(X)_{ij} = \frac{X_{ij}}{max(X_{i:})}\)

axis=0 normalizes each column of X by norm of said column. \(maxnormalize(X)_{ij} = \frac{X_{ij}}{max(X_{
:j})}\)

axis=None normalizes entries of X  norm of X. \(maxnormalize(X)_{ij} = \frac{X_{ij}}{max(X)}\)
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} -- A scipy sparse csr\_matrix or numpy array.

\item {} 
\textbf{\texttt{axis}} -- The dimension to normalize over.

\end{itemize}

\item[{Returns}] \leavevmode
A normalized matrix.

\end{description}\end{quote}

\end{fulllineitems}

\index{maybe\_download() (in module loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.maybe_download}\pysiglinewithargsret{\code{loader.}\bfcode{maybe\_download}}{\emph{filename}, \emph{work\_directory}, \emph{source\_url}}{}
Download the data from source url, unless it's already here. From \href{https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/base.py}{https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/base.py}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{filename}} -- string, name of the file in the directory.

\item {} 
\textbf{\texttt{work\_directory}} -- string, path to working directory.

\item {} 
\textbf{\texttt{source\_url}} -- url to download from if file doesn't exist.

\end{itemize}

\item[{Returns}] \leavevmode
Path to resulting file.

\end{description}\end{quote}

\end{fulllineitems}

\index{pca\_whiten() (in module loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.pca_whiten}\pysiglinewithargsret{\code{loader.}\bfcode{pca\_whiten}}{\emph{X}}{}
Returns matrix with PCA whitening transform applied.
This transform assumes that data points are rows of matrix.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} -- Numpy array, scipy sparse matrix

\item {} 
\textbf{\texttt{axis}} -- Axis to whiten over.

\end{itemize}

\item[{Returns}] \leavevmode


\end{description}\end{quote}

\end{fulllineitems}

\index{read\_data\_sets() (in module loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.read_data_sets}\pysiglinewithargsret{\code{loader.}\bfcode{read\_data\_sets}}{\emph{directory}, \emph{folders=(`train'}, \emph{`dev'}, \emph{`test')}, \emph{hashlist=()}, \emph{mix=False}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{directory}} -- Root directory containing data to load.

\item {} 
\textbf{\texttt{folders}} -- The subfolders of \emph{directory} to read data from by default there are train, dev, and test folders. If you want others you have to make an explicit list.

\item {} 
\textbf{\texttt{hashlist}} -- If you provide a hashlist these files and only these files will be added to your {\hyperref[loader:loader.DataSet]{\emph{\code{DataSet}}}} objects.
It you do not provide a hashlist then anything with
the privileged prefixes {\color{red}\bfseries{}labels\_} or {\color{red}\bfseries{}features\_} will be loaded.

\end{itemize}

\item[{Returns}] \leavevmode
A {\hyperref[loader:loader.DataSets]{\emph{\code{DataSets}}}} object.

\end{description}\end{quote}

\end{fulllineitems}

\index{save() (in module loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.save}\pysiglinewithargsret{\code{loader.}\bfcode{save}}{\emph{filename}, \emph{data}}{}
Calls :any{}`export\_data{}`.
Decides how to save data by file extension.
Raises {\hyperref[loader:loader.Unsupported_format_error]{\emph{\code{Unsupported\_format\_error}}}} if extension is not one of the supported
extensions (mat, sparse, binary, dense, index).
Data contained in .mat files should be saved in a matrix named \emph{data}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{filename}} -- A file of an accepted format representing a matrix.

\item {} 
\textbf{\texttt{data}} -- A numpy array, scipy sparse matrix, or {\hyperref[loader:loader.HotIndex]{\emph{\code{HotIndex}}}} object.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{tfidf() (in module loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.tfidf}\pysiglinewithargsret{\code{loader.}\bfcode{tfidf}}{\emph{X}, \emph{norm='l2row'}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} -- A document-term matrix.

\item {} 
\textbf{\texttt{norm}} -- Normalization strategy: `l2row': normalizes the scores of rows by length of rows after basic tfidf (each document vector is a unit vector), `count': normalizes the scores of rows by the the total word count of a document. `max' normalizes the scores of rows by the maximum count for a single word in a document.

\end{itemize}

\item[{Returns}] \leavevmode
Returns tfidf of document-term matrix X with optional normalization.

\end{description}\end{quote}

\end{fulllineitems}

\index{toIndex() (in module loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.toIndex}\pysiglinewithargsret{\code{loader.}\bfcode{toIndex}}{\emph{A}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{A}} -- A matrix of one hot row vectors.

\item[{Returns}] \leavevmode
The hot indices.

\end{description}\end{quote}
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{numpy}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{antk.core} \PYG{k+kn}{import} \PYG{n}{loader}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{x} \PYG{o}{=} \PYG{n}{numpy}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{loader}\PYG{o}{.}\PYG{n}{toIndex}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{g+go}{array([0, 2, 0])}
\end{Verbatim}

\end{fulllineitems}

\index{toOnehot() (in module loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.toOnehot}\pysiglinewithargsret{\code{loader.}\bfcode{toOnehot}}{\emph{X}, \emph{dim=None}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{X}} -- Vector of indices or {\hyperref[loader:loader.HotIndex]{\emph{\code{HotIndex}}}} object

\item {} 
\textbf{\texttt{dim}} -- Dimension of indexing

\end{itemize}

\item[{Returns}] \leavevmode
A sparse csr\_matrix of one hots.

\end{description}\end{quote}
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{numpy}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{antk.core} \PYG{k+kn}{import} \PYG{n}{loader}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{x} \PYG{o}{=} \PYG{n}{numpy}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{loader}\PYG{o}{.}\PYG{n}{toOnehot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)} 
\PYG{g+go}{\PYGZlt{}4x4 sparse matrix of type \PYGZsq{}\PYGZlt{}type \PYGZsq{}numpy.float64\PYGZsq{}\PYGZgt{}\PYGZsq{}...}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{loader}\PYG{o}{.}\PYG{n}{toOnehot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{.}\PYG{n}{toarray}\PYG{p}{(}\PYG{p}{)}
\PYG{g+go}{array([[ 1.,  0.,  0.,  0.],}
\PYG{g+go}{       [ 0.,  1.,  0.,  0.],}
\PYG{g+go}{       [ 0.,  0.,  1.,  0.],}
\PYG{g+go}{       [ 0.,  0.,  0.,  1.]])}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{x} \PYG{o}{=} \PYG{n}{loader}\PYG{o}{.}\PYG{n}{HotIndex}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{dimension}\PYG{o}{=}\PYG{l+m+mi}{8}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{loader}\PYG{o}{.}\PYG{n}{toOnehot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{.}\PYG{n}{toarray}\PYG{p}{(}\PYG{p}{)}
\PYG{g+go}{array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],}
\PYG{g+go}{       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],}
\PYG{g+go}{       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],}
\PYG{g+go}{       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.]])}
\end{Verbatim}

\end{fulllineitems}

\index{unit\_variance() (in module loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.unit_variance}\pysiglinewithargsret{\code{loader.}\bfcode{unit\_variance}}{\emph{X}, \emph{axis=None}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{X}} -- A matrix to transfrom to have unit variance (over columns axis=0, over rows axis=1, over all entries axis=None)

\item[{Returns}] \leavevmode
A matrix with unit variance along the specified axis.

\end{description}\end{quote}

\end{fulllineitems}

\index{untar() (in module loader)}

\begin{fulllineitems}
\phantomsection\label{loader:loader.untar}\pysiglinewithargsret{\code{loader.}\bfcode{untar}}{\emph{fname}}{}
\end{fulllineitems}



\subsection{config}
\label{config:config}\label{config::doc}\label{config:tensor-decompositions-and-applications}
Facilitates the generation of complex tensorflow models, built from
compositions of tensorflow functions.


\bigskip\hrule{}\bigskip



\subsubsection{Config Tutorial}
\label{config_tutorial:id7}\label{config_tutorial::doc}\label{config_tutorial:config-tutorial}
The config module defines the {\hyperref[config:config.AntGraph]{\emph{\code{AntGraph}}}} class.
The basic idea is to represent any directed acyclic graph (DAG) of higher level tensorflow operations
in a condensed and visually readable format. Here is a picture of a DAG of operations
derived from it's representation in .config format:

{\hspace*{\fill}\includegraphics{{treedot}.png}\hspace*{\fill}}

Here are contents of the corresponding .config file:

\begin{Verbatim}[commandchars=\\\{\}]
dotproduct x\PYGZus{}dot\PYGZus{}y()
\PYGZhy{}all\PYGZus{}user dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors], activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}tanh\PYGZus{}user tf.nn.tanh()
\PYGZhy{}\PYGZhy{}\PYGZhy{}merge\PYGZus{}user concat(\PYGZdl{}kfactors)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}huser lookup(dataname=\PYGZsq{}user\PYGZsq{}, initrange=\PYGZdl{}initrange, shape=[None, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hage dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}agelookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}age placeholder(tf.float32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}user placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hsex dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}sexlookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}sex\PYGZus{}weights weights(\PYGZsq{}tnorm\PYGZsq{}, tf.float32, [2, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}sexes embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}sex placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}user placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hocc dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}occlookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}occ\PYGZus{}weights weights(\PYGZsq{}tnorm\PYGZsq{}, tf.float32, [21, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}occs embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}occ placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}user placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hzip dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}ziplookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}zip\PYGZus{}weights weights(\PYGZsq{}tnorm\PYGZsq{}, tf.float32, [1000, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}zips embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}zip placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}user placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}husertime dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}time placeholder(tf.float32)
\PYGZhy{}all\PYGZus{}item dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors], activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}tanh\PYGZus{}item tf.nn.tanh()
\PYGZhy{}\PYGZhy{}\PYGZhy{}merge\PYGZus{}item concat(\PYGZdl{}kfactors)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hitem lookup(dataname=\PYGZsq{}item\PYGZsq{}, initrange=\PYGZdl{}initrange, shape=[None, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hgenre dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}genrelookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}genre placeholder(tf.float32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}item placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hmonth dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}monthlookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}month\PYGZus{}weights weights(\PYGZsq{}tnorm\PYGZsq{}, tf.float32, [12, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}months embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}month placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}item placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hyear dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}yearlookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}year placeholder(tf.float32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}item placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}htfidf dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}tfidflookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}tfidf\PYGZus{}doc\PYGZus{}term placeholder(tf.float32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}item placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hitemtime dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}time placeholder(tf.float32)
\PYGZhy{}ibias lookup(dataname=\PYGZsq{}item\PYGZsq{}, shape=[None, 1], initrange=\PYGZdl{}initrange)
\PYGZhy{}ubias lookup(dataname=\PYGZsq{}user\PYGZsq{}, shape=[None, 1], initrange=\PYGZdl{}initrange)
\end{Verbatim}

The lines in the .config file consist of a possibly empty graph marker, followed by a node name, followed by a node function call.
We will discuss each of these in turn.


\paragraph{Terms}
\label{config_tutorial:terms}\begin{quote}

\textbf{Node description}: A line in a .config file
\begin{description}
\item[{\textbf{Graph marker}: A character or sequence of characters that delimits graph dependencies. Specified by the graph marker parameter}] \leavevmode
for the constructor to AntGraph. By default `-`.

\end{description}

\textbf{Node name}: The first thing on a line in a .config file after a possibly empty sequence of graph markers and possible whitespace.
\begin{description}
\item[{\textbf{Node function}: A function which takes as its first argument a tensor or structured list of tensors, returns}] \leavevmode
a tensor, or structured list of tensors, and has an optional name argument.

\end{description}

\textbf{Node function call}: The last item in a node description.
\end{quote}


\paragraph{Graph Markers}
\label{config_tutorial:graph-markers}
In the .config file depicted above the graph marker is `-`. The graph markers in a .config file define the edges of
the DAG. Lines in a .config file with no graph markers represent nodes with outorder = 0.
These are the `roots' of the DAG. The graph representation in .config format is similar to a textual tree or forest
representation, however, multiple lines may refer to the same node. For each node description of a node, there is an edge from
this node to the node described by the first line above of this node description that has one less graph marker.


\paragraph{Node Names}
\label{config_tutorial:node-names}
The next thing on a line following a possibly empty sequence of graph markers is the node name. Node names are used
for unique \href{https://www.tensorflow.org/versions/r0.7/how\_tos/variable\_scope/index.html}{variable scope} of the tensors created by the node function call. The number of
nodes in a graph
\begin{quote}

is the number of unique
\end{quote}

node names in the .config file.


\paragraph{Examples}
\label{config_tutorial:examples}
The best way to get a feel for how to construct a DAG in this format is to try some things out. Since node function calls
have no bearing on the high level structure of the computational graph let's simplify things
and omit the node function calls for now. This won't be acceptable .config
syntax but it will help us focus on the exploration of this form of graph representation.

Here is a .config file minus the function calls (notice the optional whitespace before graph markers):

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{dotproduct}
    \PYG{o}{\PYGZhy{}}\PYG{n}{huser}
    \PYG{o}{\PYGZhy{}}\PYG{n}{hitem}
    \PYG{o}{\PYGZhy{}}\PYG{n}{ibias}
    \PYG{o}{\PYGZhy{}}\PYG{n}{ubias}
\end{Verbatim}

Save this content in a file called test.config.
Now in an interpreter:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}from antk.core import config}
\PYG{g+go}{\PYGZgt{}\PYGZgt{}\PYGZgt{}config.testGraph(\PYGZsq{}test.config\PYGZsq{})}
\end{Verbatim}

This image should display:

{\hspace*{\fill}\includegraphics{{no_name}.png}\hspace*{\fill}}

Now experiment with test.config to make some more graphs.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n}{dotproduct}
    \PYG{o}{\PYGZhy{}}\PYG{n}{huser}
        \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{hitem}
    \PYG{o}{\PYGZhy{}}\PYG{n}{ibias}
        \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{hitem}
    \PYG{o}{\PYGZhy{}}\PYG{n}{ubias}
        \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{hitem}
    \PYG{o}{\PYGZhy{}}\PYG{n}{hitem}
\end{Verbatim}

\begin{notice}{note}{Note:}
\textbf{Repeated Node Names} Graph traversal proceeds in the fashion of a postorder tree traversal.
When node names are repeated in a .config file, the output of this node is the output of the node description with this
name which is first encountered in graph traversal.
So, for the above example .config file and its corresponding picture below, the output of the hitem node would be the
output of the node function call (omitted) on line
3. The order in which the nodes are evaluated for the config above is: \textbf{hitem}, \textbf{huser}, \textbf{ibias}, \textbf{ubias},
\textbf{dotproduct}.
\end{notice}

{\hspace*{\fill}\includegraphics{{ex1}.png}\hspace*{\fill}}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{dotproduct}
    \PYG{o}{\PYGZhy{}}\PYG{n}{huser}
        \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{hitem}
    \PYG{o}{\PYGZhy{}}\PYG{n}{ibias}
        \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{hitem}
    \PYG{o}{\PYGZhy{}}\PYG{n}{ubias}
        \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{hitem}
    \PYG{o}{\PYGZhy{}}\PYG{n}{hitem}
\PYG{n}{a}
\PYG{n}{b}
\PYG{n}{c}
\PYG{n}{d}
\end{Verbatim}

{\hspace*{\fill}\includegraphics{{ex2}.png}\hspace*{\fill}}

\begin{notice}{warning}{Warning:}
\textbf{Cycles}: ANTk is designed to create directed acyclic graphs of operations from a config file,
so cycles are not allowed.
Below is an example of a config setup that describes a cycle. This config would cause an error, even if the node function
calls were made with proper inputs.
\end{notice}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{hitem}
    \PYG{o}{\PYGZhy{}}\PYG{n}{huser}
        \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{hitem}
    \PYG{o}{\PYGZhy{}}\PYG{n}{ibias}
        \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{hitem}
    \PYG{o}{\PYGZhy{}}\PYG{n}{ubias}
        \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{hitem}
    \PYG{o}{\PYGZhy{}}\PYG{n}{hitem}
\end{Verbatim}

{\hspace*{\fill}\includegraphics{{ex3}.png}\hspace*{\fill}}


\paragraph{Node Functions}
\label{config_tutorial:node-functions}
The first and only thing that comes after the name in a node description is a node function call.
Node functions always take tensors or structured lists of tensors as input, return tensors or structured lists of tensors
as output, and have an optional name argument.
The syntax for a node function call in a .config is the same as calling the function in a python script,
but omitting the first tensor input argument and the name argument. The tensor input is derived from the graph. A node's
tensor input is a list of the output of it's `child' nodes' (nodes with edges directed to this node) function calls. If
a node has inorder = 1 then its input is a single tensor as opposed to a list of tensors of length 1.

Any node functions defined in {\hyperref[node_ops::doc]{\emph{\emph{node\_ops}}}} may be used in a graph, as well as any tensorflow functions which satisfy the
definition of a node function. For tensorflow node function calls `tensorflow' is abbreviated to `tf'.
User defined node functions may be used in the graph when specified by the optional
arguments \emph{function\_map}, and \emph{imports}, to the {\hyperref[config:config.AntGraph]{\emph{\code{AntGraph}}}} constructor.

The node name is used for the optional name argument of the node function.


\paragraph{The AntGraph object}
\label{config_tutorial:the-antgraph-object}
To use a .config file to build a tensorflow computational graph you call the {\hyperref[config:config.AntGraph]{\emph{\code{AntGraph}}}} constructor with the
path to the .config file as the first argument, and some other optional arguments. We'll make the multinomial logistic
regression model from tensorflow's basic \href{https://www.tensorflow.org/versions/r0.7/tutorials/mnist/beginners/index.html}{MNIST tutorial}, and then extend this model to a deep neural network
in order to demonstrate how to use a .config file in your tensorflow code.

Create a file called antk\_mnist.py and start off by importing the modules and data we need.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k+kn}{import} \PYG{n+nn}{tensorflow} \PYG{k+kn}{as} \PYG{n+nn}{tf}
\PYG{k+kn}{from} \PYG{n+nn}{antk.core} \PYG{k+kn}{import} \PYG{n}{config}
\PYG{k+kn}{from} \PYG{n+nn}{tensorflow.examples.tutorials.mnist} \PYG{k+kn}{import} \PYG{n}{input\PYGZus{}data}

\PYG{n}{mnist} \PYG{o}{=} \PYG{n}{input\PYGZus{}data}\PYG{o}{.}\PYG{n}{read\PYGZus{}data\PYGZus{}sets}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MNIST\PYGZus{}data/}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{one\PYGZus{}hot}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}
\end{Verbatim}

We'll need a config file called logreg.config with the content below:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{pred} \PYG{n}{mult\PYGZus{}log\PYGZus{}reg}\PYG{p}{(}\PYG{n}{numclasses}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{o}{\PYGZhy{}}\PYG{n}{pixels} \PYG{n}{placeholder}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
\end{Verbatim}

Notice that we didn't specify any dimensions for the placeholder \emph{pixels}. We need to hand a dictionary with keys corresponding
to placeholders with unspecified dimensions, and values of the data that will later get fed to this placeholder during
graph execution. This way the constructor will infer the shape of the placeholder. This practice can
help eliminate a common source of errors in constructing a tensorflow graph. To instantiate the graph from this config
file we add to antk\_mnist.py:

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=6,stepnumber=1]
\PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{name\PYGZus{}scope}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{antgraph}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{antgraph} \PYG{o}{=} \PYG{n}{config}\PYG{o}{.}\PYG{n}{AntGraph}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{logreg.config}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pixels}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{test}\PYG{o}{.}\PYG{n}{images}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{antgraph}\PYG{o}{.}\PYG{n}{placeholderdict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pixels}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{antgraph}\PYG{o}{.}\PYG{n}{tensor\PYGZus{}out}
\end{Verbatim}

There are three accessible fields of a {\hyperref[config:config.AntGraph]{\emph{\code{AntGraph}}}} object which contain tensors created during graph construction from
a .config file:
\begin{itemize}
\item {} 
{\hyperref[config:config.AntGraph.tensordict]{\emph{\code{tensordict}}}}: a python dictionary of non-placeholder tensors.

\item {} 
{\hyperref[config:config.AntGraph.placeholderdict]{\emph{\code{placeholderdict}}}}: a python dictionary of placeholder tensors.

\item {} 
{\hyperref[config:config.AntGraph.tensor_out]{\emph{\code{tensor\_out}}}}: The output of the nodes of the graph with outorder 0 (no graph markers).

\end{itemize}

Note that we could replace line 9 above with the following:

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=9,stepnumber=1]
\PYG{n}{y} \PYG{o}{=} \PYG{n}{antgraph}\PYG{o}{.}\PYG{n}{tensordict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pred}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{Verbatim}

We can now complete the simple MNIST model verbatim from the tensorflow tutorial:

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=10,stepnumber=1]
\PYG{n}{y\PYGZus{}} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{placeholder}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{,} \PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{cross\PYGZus{}entropy} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}sum}\PYG{p}{(}\PYG{n}{y\PYGZus{}}\PYG{o}{*}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{train\PYGZus{}step} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{train}\PYG{o}{.}\PYG{n}{GradientDescentOptimizer}\PYG{p}{(}\PYG{l+m+mf}{0.01}\PYG{p}{)}\PYG{o}{.}\PYG{n}{minimize}\PYG{p}{(}\PYG{n}{cross\PYGZus{}entropy}\PYG{p}{)}

\PYG{n}{correct\PYGZus{}prediction} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{equal}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{y\PYGZus{}}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{accuracy} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{cast}\PYG{p}{(}\PYG{n}{correct\PYGZus{}prediction}\PYG{p}{,} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} tensorboard stuff}
\PYG{n}{accuracy\PYGZus{}summary} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{scalar\PYGZus{}summary}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Accuracy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{accuracy}\PYG{p}{)}
\PYG{n}{session} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{Session}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{summary\PYGZus{}writer} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{train}\PYG{o}{.}\PYG{n}{SummaryWriter}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log/logistic\PYGZus{}regression}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{session}\PYG{o}{.}\PYG{n}{graph}\PYG{o}{.}\PYG{n}{as\PYGZus{}graph\PYGZus{}def}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{session}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{initialize\PYGZus{}all\PYGZus{}variables}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{batch\PYGZus{}xs}\PYG{p}{,} \PYG{n}{batch\PYGZus{}ys} \PYG{o}{=} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{train}\PYG{o}{.}\PYG{n}{next\PYGZus{}batch}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{)}
    \PYG{n}{session}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{n}{train\PYGZus{}step}\PYG{p}{,} \PYG{n}{feed\PYGZus{}dict}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{p}{:} \PYG{n}{batch\PYGZus{}xs}\PYG{p}{,} \PYG{n}{y\PYGZus{}}\PYG{p}{:} \PYG{n}{batch\PYGZus{}ys}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

    \PYG{n}{acc}\PYG{p}{,} \PYG{n}{summary\PYGZus{}str} \PYG{o}{=} \PYG{n}{session}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{p}{[}\PYG{n}{accuracy}\PYG{p}{,} \PYG{n}{accuracy\PYGZus{}summary}\PYG{p}{]}\PYG{p}{,} \PYG{n}{feed\PYGZus{}dict}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{p}{:} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{test}\PYG{o}{.}\PYG{n}{images}\PYG{p}{,}
                                           \PYG{n}{y\PYGZus{}}\PYG{p}{:} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{test}\PYG{o}{.}\PYG{n}{labels}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    \PYG{n}{summary\PYGZus{}writer}\PYG{o}{.}\PYG{n}{add\PYGZus{}summary}\PYG{p}{(}\PYG{n}{summary\PYGZus{}str}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}
    \PYG{k}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{epoch: }\PYG{l+s+si}{\PYGZpc{}f}\PYG{l+s+s1}{ acc: }\PYG{l+s+si}{\PYGZpc{}f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{p}{(}\PYG{n+nb}{float}\PYG{p}{(}\PYG{n}{i}\PYG{o}{*}\PYG{l+m+mf}{100.0}\PYG{p}{)}\PYG{o}{/}\PYG{n+nb}{float}\PYG{p}{(}\PYG{n}{mnist}\PYG{o}{.}\PYG{n}{train}\PYG{o}{.}\PYG{n}{images}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{acc}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}

If we let antk\_mnist.py take a command line argument for a .config file
we can use antk\_mnist.py with any number of .config files expressing arbitrarily complex architectures. This will
allow us to quickly search for a better model. Let's use the argparse module to get this command line argument by
adding the following lines to antk\_mnist.py.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{argparse}

\PYG{n}{parser} \PYG{o}{=} \PYG{n}{argparse}\PYG{o}{.}\PYG{n}{ArgumentParser}\PYG{p}{(}\PYG{n}{description}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Model for training arbitrary MNIST digit recognition architectures.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{parser}\PYG{o}{.}\PYG{n}{add\PYGZus{}argument}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{config}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n+nb}{type}\PYG{o}{=}\PYG{n+nb}{str}\PYG{p}{,}
                    \PYG{n}{help}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{The config file for building the ant architecture.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{args} \PYG{o}{=} \PYG{n}{parser}\PYG{o}{.}\PYG{n}{parse\PYGZus{}args}\PYG{p}{(}\PYG{p}{)}
\end{Verbatim}

Now we change the former line 7 to:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{antgraph} \PYG{o}{=} \PYG{n}{AntGraph}\PYG{p}{(}\PYG{n}{args}\PYG{o}{.}\PYG{n}{config}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pixels}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{dev}\PYG{o}{.}\PYG{n}{images}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{Verbatim}

We could try a neural network with nnet\_mnist.config:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{pred} \PYG{n}{mult\PYGZus{}log\PYGZus{}reg}\PYG{p}{(}\PYG{n}{numclasses}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{o}{\PYGZhy{}}\PYG{n}{network} \PYG{n}{dnn}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tanh}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{pixels} \PYG{n}{placeholder}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
\end{Verbatim}

This should get us to about .94 accuracy.
We might want to parameterize the number of hidden nodes per hidden layer or the activation function. For this
we can use some more command line arguments, and the config file variable marker `\$'.

First we change nnet\_mnist.config as follows:

\begin{Verbatim}[commandchars=\\\{\}]
pred mult\PYGZus{}log\PYGZus{}reg(numclasses=10)
\PYGZhy{}network dnn([\PYGZdl{}h1, \PYGZdl{}h2, \PYGZdl{}h3], activation=\PYGZdl{}act)
\PYGZhy{}\PYGZhy{}pixels placeholder(tf.float32)
\end{Verbatim}

Next we need some more command line arguments for antk\_mnist.py. So we need to add these lines:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{parser}\PYG{o}{.}\PYG{n}{add\PYGZus{}argument}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}h1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n+nb}{type}\PYG{o}{=}\PYG{n+nb}{int}\PYG{p}{,}
                    \PYG{n}{help}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Number of hidden nodes in layer 1.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{parser}\PYG{o}{.}\PYG{n}{add\PYGZus{}argument}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}h2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n+nb}{type}\PYG{o}{=}\PYG{n+nb}{int}\PYG{p}{,}
                    \PYG{n}{help}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Number of hidden nodes in layer 2.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{parser}\PYG{o}{.}\PYG{n}{add\PYGZus{}argument}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}h3}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n+nb}{type}\PYG{o}{=}\PYG{n+nb}{int}\PYG{p}{,}
                    \PYG{n}{help}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Number of hidden nodes in layer 3.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{parser}\PYG{o}{.}\PYG{n}{add\PYGZus{}argument}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}act}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n+nb}{type}\PYG{o}{=}\PYG{n+nb}{int}\PYG{p}{,}
                    \PYG{n}{help}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Type of activation function.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{Verbatim}

Finally we need to bind the variables in the .config file in our call to the {\hyperref[config:config.AntGraph]{\emph{\code{AntGraph}}}} constructor
using the optional \emph{variable\_bindings} argument.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{name\PYGZus{}scope}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{antgraph}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{antgraph} \PYG{o}{=} \PYG{n}{AntGraph}\PYG{p}{(}\PYG{n}{args}\PYG{o}{.}\PYG{n}{config}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pixels}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{dev}\PYG{o}{.}\PYG{n}{images}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
                        \PYG{n}{variable\PYGZus{}bindings}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{h1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{args}\PYG{o}{.}\PYG{n}{h1}\PYG{p}{,}
                                           \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{h2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{args}\PYG{o}{.}\PYG{n}{h2}\PYG{p}{,}
                                           \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{h3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{args}\PYG{o}{.}\PYG{n}{h3}\PYG{p}{,}
                                           \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{act}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{args}\PYG{o}{.}\PYG{n}{act}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{Verbatim}

For something really deep we might try a highway network with high\_mnist.config:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{pred} \PYG{n}{mult\PYGZus{}log\PYGZus{}reg}\PYG{p}{(}\PYG{n}{numclasses}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{o}{\PYGZhy{}}\PYG{n}{network3} \PYG{n}{dnn}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{]}\PYG{p}{)}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{network2} \PYG{n}{highway\PYGZus{}dnn}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{50}\PYG{p}{]}\PYG{o}{*}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tanh}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bn}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{network} \PYG{n}{dnn}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{50}\PYG{p}{]}\PYG{p}{)}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{pixels} \PYG{n}{placeholder}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
\end{Verbatim}

This may take 5 or 10 minutes to train but should get around .96 accuracy.

These higher level abstractions are nice for automating the creation of weight and bias \href{https://www.tensorflow.org/versions/r0.7/how\_tos/variables/index.html}{Variables}, and the
\href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/framework.html\#Tensor}{Tensors} involved a deep neural network architecture. However, one may need direct access to tensors created within
a complex operation such as \emph{highway\_dnn}, to for instance analyze the training of a model. There is access to these
tensors via a standard tensorflow function and some collections associated with each node defined in the .config
file. To demonstrate accessing the tensors created by the \emph{highway\_dnn} operation in high\_mnist.config, at the end of
antk\_mnist.py we can add:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{weights} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{get\PYGZus{}collection}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{network}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{bias} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{get\PYGZus{}collection}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{network\PYGZus{}bias}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{other} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{get\PYGZus{}collection}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{network}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{wght} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{weights}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{weight }\PYG{l+s+si}{\PYGZpc{}d}\PYG{l+s+s1}{: name=}\PYG{l+s+si}{\PYGZpc{}s}\PYG{l+s+s1}{ tensor=}\PYG{l+s+si}{\PYGZpc{}s}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{wght}\PYG{o}{.}\PYG{n}{name}\PYG{p}{,} \PYG{n}{wght}\PYG{p}{)}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{b} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{bias}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bias }\PYG{l+s+si}{\PYGZpc{}d}\PYG{l+s+s1}{: name=}\PYG{l+s+si}{\PYGZpc{}s}\PYG{l+s+s1}{ tensor=}\PYG{l+s+si}{\PYGZpc{}s}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{b}\PYG{o}{.}\PYG{n}{name}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{tensor} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{other}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{other }\PYG{l+s+si}{\PYGZpc{}d}\PYG{l+s+s1}{: name=}\PYG{l+s+si}{\PYGZpc{}s}\PYG{l+s+s1}{ tensor=}\PYG{l+s+si}{\PYGZpc{}s}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{tensor}\PYG{o}{.}\PYG{n}{name}\PYG{p}{,} \PYG{n}{tensor}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}

And post training we get the following output modulo two memory addresses:

\begin{Verbatim}[commandchars=\\\{\}]
weight 0: name=antgraph/network/layer0/add:0 tensor=Tensor(\PYGZdq{}antgraph/network/layer0/add:0\PYGZdq{}, shape=(?, 100), dtype=float32)
weight 1: name=antgraph/network/layer1/add:0 tensor=Tensor(\PYGZdq{}antgraph/network/layer1/add:0\PYGZdq{}, shape=(?, 50), dtype=float32)
bias 0: name=network/layer0/network/Bias:0 tensor=\PYGZlt{}tensorflow.python.ops.variables.Variable object at 0x7f1b90764350\PYGZgt{}
bias 1: name=network/layer1/network/Bias:0 tensor=\PYGZlt{}tensorflow.python.ops.variables.Variable object at 0x7f1b90723d50\PYGZgt{}
other 0: name=antgraph/network/layer0/add:0 tensor=Tensor(\PYGZdq{}antgraph/network/layer0/add:0\PYGZdq{}, shape=(?, 100), dtype=float32)
other 1: name=antgraph/network/layer1/add:0 tensor=Tensor(\PYGZdq{}antgraph/network/layer1/add:0\PYGZdq{}, shape=(?, 50), dtype=float32)
\end{Verbatim}
\phantomsection\label{config:module-config}\index{config (module)}\index{AntGraph (class in config)}

\begin{fulllineitems}
\phantomsection\label{config:config.AntGraph}\pysiglinewithargsret{\strong{class }\code{config.}\bfcode{AntGraph}}{\emph{config}, \emph{tensordict=\{\}}, \emph{placeholderdict=\{\}}, \emph{data=None}, \emph{function\_map=\{\}}, \emph{imports=\{\}}, \emph{marker='-`}, \emph{variable\_bindings=None}, \emph{graph\_name='no\_name'}, \emph{graph\_dest='antpics/'}, \emph{develop=False}}{}
Object to store graph information from graph built with config file.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{config}} -- A plain text config file

\item {} 
\textbf{\texttt{tensordict}} -- A dictionary of premade tensors represented in the config by key

\item {} 
\textbf{\texttt{placeholderdict}} -- A dictionary of premade placeholder tensors represented in the config by key

\item {} 
\textbf{\texttt{data}} -- A dictionary of data matrices with keys corresponding to placeholder names in graph.

\item {} 
\textbf{\texttt{function\_map}} -- A dictionary of function\_handle:node\_op pairs to use in building the graph

\item {} 
\textbf{\texttt{imports}} -- A dictionary of module\_name:path\_to\_module key value pairs for custom node\_ops modules.

\item {} 
\textbf{\texttt{marker}} -- The marker for representing graph structure

\item {} 
\textbf{\texttt{variable\_bindings}} -- A dictionary with entries of the form  \emph{variable\_name:value} for variable replacement in config file.

\item {} 
\textbf{\texttt{graph\_name}} -- The name of the graph. Will be used to name the graph pdf file.

\item {} 
\textbf{\texttt{graph\_dest}} -- The folder to write the graph pdf and graph dot string to.

\item {} 
\textbf{\texttt{develop}} -- True\textbar{}False. Whether to print tensor info, while constructing the tensorflow graph.

\end{itemize}

\end{description}\end{quote}
\paragraph{Attributes}
\paragraph{Methods}
\index{display\_graph() (config.AntGraph method)}

\begin{fulllineitems}
\phantomsection\label{config:config.AntGraph.display_graph}\pysiglinewithargsret{\bfcode{display\_graph}}{\emph{pdfviewer='okular'}}{}
Display the pdf image of graph from config file to screen.

\end{fulllineitems}

\index{get\_array() (config.AntGraph method)}

\begin{fulllineitems}
\phantomsection\label{config:config.AntGraph.get_array}\pysiglinewithargsret{\bfcode{get\_array}}{\emph{collection\_name}, \emph{index}, \emph{session}, \emph{graph}}{}
\end{fulllineitems}

\index{placeholderdict (config.AntGraph attribute)}

\begin{fulllineitems}
\phantomsection\label{config:config.AntGraph.placeholderdict}\pysigline{\bfcode{placeholderdict}}
A dictionary of tensors which are placeholders in the graph. The key should correspond to the key of
the corresponding data in a data dictionary.

\end{fulllineitems}

\index{tensor\_out (config.AntGraph attribute)}

\begin{fulllineitems}
\phantomsection\label{config:config.AntGraph.tensor_out}\pysigline{\bfcode{tensor\_out}}
Tensor or list of tensors returned from last node of graph.

\end{fulllineitems}

\index{tensordict (config.AntGraph attribute)}

\begin{fulllineitems}
\phantomsection\label{config:config.AntGraph.tensordict}\pysigline{\bfcode{tensordict}}
A dictionary of tensors which are nodes in the graph.

\end{fulllineitems}


\end{fulllineitems}

\index{GraphMarkerError}

\begin{fulllineitems}
\phantomsection\label{config:config.GraphMarkerError}\pysigline{\strong{exception }\code{config.}\bfcode{GraphMarkerError}}
Raised when leading character of a line (other than first)
in a graph config file is not the specified level marker.

\end{fulllineitems}

\index{MissingDataError}

\begin{fulllineitems}
\phantomsection\label{config:config.MissingDataError}\pysigline{\strong{exception }\code{config.}\bfcode{MissingDataError}}
Raised when data needed to determine shapes is not found in the {\hyperref[loader:loader.DataSet]{\emph{\code{DataSet}}}}.

\end{fulllineitems}

\index{MissingTensorError}

\begin{fulllineitems}
\phantomsection\label{config:config.MissingTensorError}\pysigline{\strong{exception }\code{config.}\bfcode{MissingTensorError}}
Raised when a tensor is described by name only in the graph and it is not in a dictionary.

\end{fulllineitems}

\index{ProcessLookupError}

\begin{fulllineitems}
\phantomsection\label{config:config.ProcessLookupError}\pysigline{\strong{exception }\code{config.}\bfcode{ProcessLookupError}}
Raised when lookup receives a dataname argument without a corresponding value in it's {\hyperref[loader:loader.DataSet]{\emph{\code{DataSet}}}}
and there is not already a Placeholder with that name.

\end{fulllineitems}

\index{RandomNodeFunctionError}

\begin{fulllineitems}
\phantomsection\label{config:config.RandomNodeFunctionError}\pysigline{\strong{exception }\code{config.}\bfcode{RandomNodeFunctionError}}
Raised when something strange happened with a node function call.

\end{fulllineitems}

\index{UndefinedVariableError}

\begin{fulllineitems}
\phantomsection\label{config:config.UndefinedVariableError}\pysigline{\strong{exception }\code{config.}\bfcode{UndefinedVariableError}}
Raised when a a variable in config is not a key in variable\_bindings map handed to graph\_setup.

\end{fulllineitems}

\index{UnsupportedNodeError}

\begin{fulllineitems}
\phantomsection\label{config:config.UnsupportedNodeError}\pysigline{\strong{exception }\code{config.}\bfcode{UnsupportedNodeError}}
Raised when a config file calls a function that is not defined, i.e., has not been imported, or is not in the
node\_ops base file.

\end{fulllineitems}

\index{ph\_rep() (in module config)}

\begin{fulllineitems}
\phantomsection\label{config:config.ph_rep}\pysiglinewithargsret{\code{config.}\bfcode{ph\_rep}}{\emph{ph}}{}
Convenience function for representing a tensorflow placeholder.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{ph}} -- A \href{https://www.tensorflow.org/}{tensorflow} \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/io\_ops.html\#placeholders}{placeholder}.

\item[{Returns}] \leavevmode
A string representing the placeholder.

\end{description}\end{quote}

\end{fulllineitems}

\index{testGraph() (in module config)}

\begin{fulllineitems}
\phantomsection\label{config:config.testGraph}\pysiglinewithargsret{\code{config.}\bfcode{testGraph}}{\emph{config}, \emph{marker='-`}, \emph{graph\_dest='antpics/'}, \emph{graph\_name='test\_graph'}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{config}} -- A graph specification in .config format.

\item {} 
\textbf{\texttt{marker}} -- A character or string of characters to delimit graph edges.

\item {} 
\textbf{\texttt{graph\_dest}} -- Where to save the graphviz pdf and associated dot file.

\item {} 
\textbf{\texttt{graph\_name}} -- A name for the graph (without extension)

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}



\subsection{node\_ops}
\label{node_ops::doc}\label{node_ops:tensor-decompositions-and-applications}\label{node_ops:node-ops}
The {\hyperref[node_ops::doc]{\emph{\emph{node\_ops}}}} module consists of a collection of mid to high level functions which take a \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/framework.html\#Tensor}{tensor} or structured list of tensors, perform a sequence of tensorflow operations, and return a tensor or structured list of tensors. All node\_ops functions conform to
the following specifications.
\begin{itemize}
\item {} 
All tensor input (if it has tensor input) is received by the function's first argument, which may be a single tensor, a list of tensors, or a structured list of tensors, e.g., a list of lists of tensors.

\item {} 
The return is a tensor, list of tensors or structured list of tensors.

\item {} 
The final argument is an optional \emph{name} argument for \href{https://www.tensorflow.org/versions/r0.7/how\_tos/variable\_scope/index.html}{variable\_scope}.

\end{itemize}


\subsubsection{Use Cases}
\label{node_ops:use-cases}\begin{description}
\item[{{\hyperref[node_ops::doc]{\emph{\emph{node\_ops}}}} functions may be used in a \href{https://www.tensorflow.org/}{tensorflow} script wherever you might use an equivalent sequence of tensorflow}] \leavevmode
ops during the graph building portion of a script.

\end{description}

{\hyperref[node_ops::doc]{\emph{\emph{node\_ops}}}} functions may be called in a .config file following the .config file syntax which is explained in {\hyperref[config_tutorial::doc]{\emph{\emph{Config Tutorial}}}}.


\subsubsection{Making Custom ops For use With \emph{config} module}
\label{node_ops:making-custom-ops-for-use-with-config-module}
The {\hyperref[config:config.AntGraph]{\emph{\code{AntGraph}}}} constructor in the \emph{config} module will add tensor operations to the tensorflow graph which are specified
in a config file and fit the node\_ops spec but not defined in the \emph{node\_ops} module. This leaves the user free to define new
node\_ops for use with the config module, and to use many pre-existing tensorflow and third party defined ops with the config
module as well.

The {\hyperref[config:config.AntGraph]{\emph{\code{AntGraph}}}} constructor has two arguments \emph{function\_map} and \emph{imports} which may be used to incorporate custom node\_ops.
\begin{itemize}
\item {} 
\textbf{function\_map} is a hashmap of function\_handle:function, key value pairs

\item {} 
\textbf{imports} is a hashmap of module\_name:path\_to\_module pairs for importing an entire module of custom node\_ops.

\end{itemize}


\subsubsection{Accessing Tensors Created in a node\_ops Function}
\label{node_ops:accessing-tensors-created-in-a-node-ops-function}
Tensors which are created by a node\_ops function but not returned to the caller are kept track of in an intuitive fashion
by calls to \textbf{tf.add\_to\_collection}. Tensors can be accessed later by calling \textbf{tf.get\_collection} by the following convention:

For a node\_ops function which was handed the argument \textbf{name='some\_name'}:
\begin{itemize}
\item {} 
The \textbf{nth weight tensor} created may be accessed as

\end{itemize}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{tf}\PYG{o}{.}\PYG{n}{get\PYGZus{}collection}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{some\PYGZus{}name\PYGZus{}weights}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{n}{n}\PYG{p}{]}
\end{Verbatim}
\begin{itemize}
\item {} 
The \textbf{nth bias tensor} created may be accessed as

\end{itemize}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{tf}\PYG{o}{.}\PYG{n}{get\PYGZus{}collection}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{some\PYGZus{}name\PYGZus{}bias}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{n}{n}\PYG{p}{]}
\end{Verbatim}
\begin{itemize}
\item {} 
The \textbf{nth preactivation tensor} created may be accessed as

\end{itemize}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{tf}\PYG{o}{.}\PYG{n}{get\PYGZus{}collection}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{some\PYGZus{}name\PYGZus{}preactivation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{n}{n}\PYG{p}{]}
\end{Verbatim}
\begin{itemize}
\item {} 
The \textbf{nth activation tensor} created may be accessed as

\end{itemize}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{tf}\PYG{o}{.}\PYG{n}{get\PYGZus{}collection}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{some\PYGZus{}name\PYGZus{}activations}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{n}{n}\PYG{p}{]}
\end{Verbatim}
\begin{itemize}
\item {} 
The \textbf{nth post dropout} tensor created may be accessed as

\end{itemize}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{tf}\PYG{o}{.}\PYG{n}{get\PYGZus{}collection}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{some\PYGZus{}name\PYGZus{}dropouts}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{n}{n}\PYG{p}{]}
\end{Verbatim}
\begin{itemize}
\item {} 
The \textbf{nth post batch normalization tensor} created may be accessed as

\end{itemize}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{tf}\PYG{o}{.}\PYG{n}{get\PYGZus{}collection}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{some\PYGZus{}name\PYGZus{}bn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{n}{n}\PYG{p}{]}
\end{Verbatim}
\begin{itemize}
\item {} 
The \textbf{nth tensor created not listed above} may be accessed as

\end{itemize}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{tf}\PYG{o}{.}\PYG{n}{get\PYGZus{}collection}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{some\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{n}{n}\PYG{p}{]}\PYG{p}{,}
\end{Verbatim}
\begin{itemize}
\item {} 
The \textbf{nth hidden layer size skip transform} (for \code{residual\_dnn}):

\end{itemize}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{tf}\PYG{o}{.}\PYG{n}{get\PYGZus{}collection}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{some\PYGZus{}name\PYGZus{}skiptransform}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{n}{n}\PYG{p}{]}
\end{Verbatim}
\begin{itemize}
\item {} 
The \textbf{nth skip connection} (for \code{residual\_dnn}):

\end{itemize}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{tf}\PYG{o}{.}\PYG{n}{get\PYGZus{}collection}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{some\PYGZus{}name\PYGZus{}skipconnection}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{n}{n}\PYG{p}{]}
\end{Verbatim}
\begin{itemize}
\item {} 
The \textbf{nth transform layer} (for \code{highway\_dnn}):

\end{itemize}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{tf}\PYG{o}{.}\PYG{n}{get\PYGZus{}collection}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{some\PYGZus{}name\PYGZus{}transform}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{n}{n}\PYG{p}{]}
\end{Verbatim}


\subsubsection{Weights}
\label{node_ops:weights}
Here is a simple wrapper for common initializations of tensorflow {\color{red}\bfseries{}{}`Variables{}`\_}. There is a option for
l2 regularization which is automatically added to the objective function when using the {\hyperref[generic_model::doc]{\emph{\emph{generic\_model}}}} module.

{\hyperref[node_ops:node_ops.weights]{\emph{\code{weights}}}}


\subsubsection{Placeholders}
\label{node_ops:placeholders}
Here is a simple wrapper for a tensorflow placeholder constructor that when used in conjunction with
the {\hyperref[config::doc]{\emph{\emph{config}}}} module, infers the correct dimensions of the \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/io\_ops.html\#placeholders}{placeholder} from a string hashed set
of numpy matrices.

{\hyperref[node_ops:node_ops.placeholder]{\emph{\code{placeholder}}}}


\subsubsection{Neural Networks}
\label{node_ops:neural-networks}
\begin{notice}{warning}{Warning:}
The output of a neural network node\_ops function is the output after activation of the last hidden layer.
For regression an additional call to {\hyperref[node_ops:node_ops.linear]{\emph{\code{linear}}}} must be made and for classification and additional call to
{\hyperref[node_ops:node_ops.mult_log_reg]{\emph{\code{mult\_log\_reg}}}} must be made.
\end{notice}


\paragraph{Initialization}
\label{node_ops:initialization}
Neural network weights are initialized with the following scheme where the range is dependent on the second
dimension of the input layer:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{n}{activation} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
   \PYG{n}{irange}\PYG{o}{=} \PYG{n}{initrange}\PYG{o}{*}\PYG{n}{numpy}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{l+m+mf}{2.0}\PYG{o}{/}\PYG{n+nb}{float}\PYG{p}{(}\PYG{n}{tensor\PYGZus{}in}\PYG{o}{.}\PYG{n}{get\PYGZus{}shape}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{as\PYGZus{}list}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{k}{else}\PYG{p}{:}
   \PYG{n}{irange} \PYG{o}{=} \PYG{n}{initrange}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mf}{1.0}\PYG{o}{/}\PYG{n}{numpy}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n+nb}{float}\PYG{p}{(}\PYG{n}{tensor\PYGZus{}in}\PYG{o}{.}\PYG{n}{get\PYGZus{}shape}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{as\PYGZus{}list}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}

\emph{initrange} above is defaulted to 1. The user has the choice of several distributions,
\begin{itemize}
\item {} 
`norm', `tnorm': \emph{irange} scales distribution with mean zero and standard deviation 1.

\item {} 
`uniform': \emph{irange} scales uniform distribution with range {[}-1, 1{]}.

\item {} 
`constant': \emph{irange} equals the initial scalar entries of the matrix.

\end{itemize}


\paragraph{Dropout}
\label{node_ops:dropout}
Dropout with the specified \emph{keep\_prob} is performed post activation.


\paragraph{Batch Normalization}
\label{node_ops:batch-normalization}
If requested batch normalization is performed after dropout.


\paragraph{Networks}
\label{node_ops:networks}
\code{dnn}

\code{residual\_dnn}

\code{highway\_dnn}

\code{convolutional\_net}


\subsubsection{Loss Functions and Evaluation Metrics}
\label{node_ops:loss-functions-and-evaluation-metrics}
{\hyperref[node_ops:node_ops.se]{\emph{\code{se}}}}

{\hyperref[node_ops:node_ops.mse]{\emph{\code{mse}}}}

{\hyperref[node_ops:node_ops.rmse]{\emph{\code{rmse}}}}

{\hyperref[node_ops:node_ops.mae]{\emph{\code{mae}}}}

{\hyperref[node_ops:node_ops.cross_entropy]{\emph{\code{cross\_entropy}}}}

{\hyperref[node_ops:node_ops.other_cross_entropy]{\emph{\code{other\_cross\_entropy}}}}

{\hyperref[node_ops:node_ops.perplexity]{\emph{\code{perplexity}}}}

{\hyperref[node_ops:node_ops.detection]{\emph{\code{detection}}}}

{\hyperref[node_ops:node_ops.recall]{\emph{\code{recall}}}}

{\hyperref[node_ops:node_ops.precision]{\emph{\code{precision}}}}

{\hyperref[node_ops:node_ops.accuracy]{\emph{\code{accuracy}}}}

{\hyperref[node_ops:node_ops.fscore]{\emph{\code{fscore}}}}


\subsubsection{Custom Activations}
\label{node_ops:custom-activations}
{\hyperref[node_ops:node_ops.ident]{\emph{\code{ident}}}}

\code{tanhlecun}

{\hyperref[node_ops:node_ops.mult_log_reg]{\emph{\code{mult\_log\_reg}}}}


\subsubsection{Matrix Operations}
\label{node_ops:matrix-operations}
{\hyperref[node_ops:node_ops.concat]{\emph{\code{concat}}}}

{\hyperref[node_ops:node_ops.x_dot_y]{\emph{\code{x\_dot\_y}}}}

{\hyperref[node_ops:node_ops.cosine]{\emph{\code{cosine}}}}

{\hyperref[node_ops:node_ops.linear]{\emph{\code{linear}}}}

{\hyperref[node_ops:node_ops.embedding]{\emph{\code{embedding}}}}

{\hyperref[node_ops:node_ops.lookup]{\emph{\code{lookup}}}}

{\hyperref[node_ops:node_ops.khatri_rao]{\emph{\code{khatri\_rao}}}}


\subsubsection{Tensor Operations}
\label{node_ops:tensor-operations}
Some tensor operations from Kolda and Bader's \emph{Tensor Decompositions and Applications} are provided here. For now these
operations only work on up to order 3 tensors.

{\hyperref[node_ops:node_ops.nmode_tensor_tomatrix]{\emph{\code{nmode\_tensor\_tomatrix}}}}

{\hyperref[node_ops:node_ops.nmode_tensor_multiply]{\emph{\code{nmode\_tensor\_multiply}}}}

{\hyperref[node_ops:node_ops.binary_tensor_combine]{\emph{\code{binary\_tensor\_combine}}}}

{\hyperref[node_ops:node_ops.ternary_tensor_combine]{\emph{\code{ternary\_tensor\_combine}}}}


\subsubsection{Batch Normalization}
\label{node_ops:id4}
{\hyperref[node_ops:node_ops.batch_normalize]{\emph{\code{batch\_normalize}}}}


\subsubsection{Dropout}
\label{node_ops:id5}
Dropout is automatically `turned' off during evaluation when used in conjuction with the {\hyperref[generic_model::doc]{\emph{\emph{generic\_model}}}} module.

{\hyperref[node_ops:node_ops.dropout]{\emph{\code{dropout}}}}


\subsubsection{API}
\label{node_ops:api}\label{node_ops:module-node_ops}\index{node\_ops (module)}\index{MissingShapeError}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.MissingShapeError}\pysigline{\strong{exception }\code{node\_ops.}\bfcode{MissingShapeError}}
Raised when {\hyperref[node_ops:node_ops.placeholder]{\emph{\code{placeholder}}}} can not infer shape.

\end{fulllineitems}

\index{accuracy() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.accuracy}\pysiglinewithargsret{\code{node\_ops.}\bfcode{accuracy}}{\emph{*args}, \emph{**kwargs}}{}
\end{fulllineitems}

\index{batch\_normalize() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.batch_normalize}\pysiglinewithargsret{\code{node\_ops.}\bfcode{batch\_normalize}}{\emph{*args}, \emph{**kwargs}}{}~\begin{description}
\item[{Batch Normalization: Adapted from tensorflow \href{https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn.py}{nn.py} and skflow \href{https://github.com/tensorflow/skflow/blob/master/skflow/ops/batch\_norm\_ops.py}{batch\_norm\_ops.py} .}] \leavevmode
\href{http://arxiv.org/pdf/1502.03167v3.pdf}{Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift}

\end{description}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tensor\_in}} -- input \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/framework.html\#Tensor}{Tensor}

\item {} 
\textbf{\texttt{epsilon}} -- A float number to avoid being divided by 0.

\item {} 
\textbf{\texttt{name}} -- For \href{https://www.tensorflow.org/versions/r0.7/how\_tos/variable\_scope/index.html}{variable\_scope}

\end{itemize}

\item[{Returns}] \leavevmode
Tensor with variance bounded by a unit and mean of zero according to the batch.

\end{description}\end{quote}

\end{fulllineitems}

\index{binary\_tensor\_combine() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.binary_tensor_combine}\pysiglinewithargsret{\code{node\_ops.}\bfcode{binary\_tensor\_combine}}{\emph{*args}, \emph{**kwargs}}{}
For performing tensor multiplications with batches of data points against an order 3
weight tensor.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tensors}} -- A list of two matrices each with first dim batch-size

\item {} 
\textbf{\texttt{output\_dim}} -- The dimension of the third mode of the weight tensor

\item {} 
\textbf{\texttt{initrange}} -- For initializing weight tensor

\item {} 
\textbf{\texttt{name}} -- For variable scope

\end{itemize}

\item[{Returns}] \leavevmode
A matrix with shape batch\_size X output\_dim

\end{description}\end{quote}

\end{fulllineitems}

\index{binary\_tensor\_combine2() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.binary_tensor_combine2}\pysiglinewithargsret{\code{node\_ops.}\bfcode{binary\_tensor\_combine2}}{\emph{*args}, \emph{**kwargs}}{}
\end{fulllineitems}

\index{concat() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.concat}\pysiglinewithargsret{\code{node\_ops.}\bfcode{concat}}{\emph{*args}, \emph{**kwargs}}{}
Matrix multiplies each \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/framework.html\#Tensor}{tensor} in \emph{tensors} by its own weight matrix and adds together the results.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tensors}} -- A list of tensors.

\item {} 
\textbf{\texttt{output\_dim}} -- Dimension of output

\item {} 
\textbf{\texttt{name}} -- An optional identifier for unique \href{https://www.tensorflow.org/versions/r0.7/how\_tos/variable\_scope/index.html}{variable\_scope}.

\end{itemize}

\item[{Returns}] \leavevmode
A tensor with shape {[}None, output\_dim{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cosine() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.cosine}\pysiglinewithargsret{\code{node\_ops.}\bfcode{cosine}}{\emph{*args}, \emph{**kwargs}}{}
Takes the cosine of vectors in corresponding rows of the two matrix \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/framework.html\#Tensor}{tensors} in operands.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{operands}} -- A list of two tensors to take cosine of.

\item {} 
\textbf{\texttt{name}} -- An optional name for unique variable scope.

\end{itemize}

\item[{Returns}] \leavevmode
A tensor with dimensions (operands{[}0{]}.shape{[}0{]}, 1)

\item[{Raises}] \leavevmode
ValueError when operands do not have matching shapes.

\end{description}\end{quote}

\end{fulllineitems}

\index{cross\_entropy() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.cross_entropy}\pysiglinewithargsret{\code{node\_ops.}\bfcode{cross\_entropy}}{\emph{*args}, \emph{**kwargs}}{}
\end{fulllineitems}

\index{detection() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.detection}\pysiglinewithargsret{\code{node\_ops.}\bfcode{detection}}{\emph{*args}, \emph{**kwargs}}{}
\end{fulllineitems}

\index{dropout() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.dropout}\pysiglinewithargsret{\code{node\_ops.}\bfcode{dropout}}{\emph{*args}, \emph{**kwargs}}{}~\begin{description}
\item[{Adds dropout node. Adapted from skflow \href{https://github.com/tensorflow/skflow/blob/master/skflow/ops/dropout\_ops.py}{dropout\_ops.py} .}] \leavevmode
\href{https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf}{Dropout A Simple Way to Prevent Neural Networks from Overfitting}

\end{description}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tensor\_in}} -- Input \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/framework.html\#Tensor}{tensor}.

\item {} 
\textbf{\texttt{prob}} -- The percent of weights to keep.

\item {} 
\textbf{\texttt{name}} -- A name for the tensor.

\end{itemize}

\item[{Returns}] \leavevmode
\href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/framework.html\#Tensor}{Tensor} of the same shape of \emph{tensor\_in}.

\end{description}\end{quote}

\end{fulllineitems}

\index{embedding() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.embedding}\pysiglinewithargsret{\code{node\_ops.}\bfcode{embedding}}{\emph{*args}, \emph{**kwargs}}{}
A wrapper for \href{https://www.tensorflow.org/}{tensorflow's} \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/nn.html\#embeddings}{embedding\_lookup}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tensors}} -- A list of two \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/framework.html\#Tensor}{tensors} , matrix, indices

\item {} 
\textbf{\texttt{name}} -- Unique name for variable scope

\end{itemize}

\item[{Returns}] \leavevmode
A matrix \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/framework.html\#Tensor}{tensor} where the i-th row = matrix{[}indices{[}i{]}{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{fan\_scale() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.fan_scale}\pysiglinewithargsret{\code{node\_ops.}\bfcode{fan\_scale}}{\emph{initrange}, \emph{activation}, \emph{tensor\_in}}{}
\end{fulllineitems}

\index{fscore() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.fscore}\pysiglinewithargsret{\code{node\_ops.}\bfcode{fscore}}{\emph{*args}, \emph{**kwargs}}{}
\end{fulllineitems}

\index{ident() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.ident}\pysiglinewithargsret{\code{node\_ops.}\bfcode{ident}}{\emph{tensor\_in}, \emph{name='ident'}}{}
Identity function for grouping tensors in graph, during config parsing.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{tensor\_in}} -- A \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/framework.html\#Tensor}{Tensor} or list of tensors

\item[{Returns}] \leavevmode
tensor\_in

\end{description}\end{quote}

\end{fulllineitems}

\index{khatri\_rao() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.khatri_rao}\pysiglinewithargsret{\code{node\_ops.}\bfcode{khatri\_rao}}{\emph{*args}, \emph{**kwargs}}{}
From \href{https://cse.wwu.edu/computer-science/palzerd}{David Palzer}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tensors}} -- 

\item {} 
\textbf{\texttt{name}} -- 

\end{itemize}

\item[{Returns}] \leavevmode


\end{description}\end{quote}

\end{fulllineitems}

\index{linear() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.linear}\pysiglinewithargsret{\code{node\_ops.}\bfcode{linear}}{\emph{*args}, \emph{**kwargs}}{}
Linear map: \(\sum_i(args[i] * W_i)\), where \(W_i\) is a variable.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{args}} -- a 2D \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/framework.html\#Tensor}{Tensor}

\item {} 
\textbf{\texttt{output\_size}} -- int, second dimension of W{[}i{]}.

\item {} 
\textbf{\texttt{bias}} -- boolean, whether to add a bias term or not.

\item {} 
\textbf{\texttt{bias\_start}} -- starting value to initialize the bias; 0 by default.

\item {} 
\textbf{\texttt{distribution}} -- Distribution for lookup weight initialization

\item {} 
\textbf{\texttt{initrange}} -- Initrange for weight distribution.

\item {} 
\textbf{\texttt{l2}} -- Floating point number determining degree of of l2 regularization for these weights in gradient descent update.

\item {} 
\textbf{\texttt{name}} -- VariableScope for the created subgraph; defaults to ``Linear''.

\end{itemize}

\item[{Returns}] \leavevmode
A 2D Tensor with shape {[}batch x output\_size{]} equal to
\(\sum_i(args[i] * W_i)\), where \(W_i\) are newly created matrices.

\item[{Raises}] \leavevmode
ValueError: if some of the arguments has unspecified or wrong shape.

\end{description}\end{quote}

\end{fulllineitems}

\index{lookup() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.lookup}\pysiglinewithargsret{\code{node\_ops.}\bfcode{lookup}}{\emph{*args}, \emph{**kwargs}}{}
A wrapper for \href{https://www.tensorflow.org/}{tensorflow's} \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/nn.html\#embeddings}{embedding\_lookup} which infers the shape of the
weight matrix and placeholder value from the parameter \emph{data}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{dataname}} -- Used exclusively by config.py

\item {} 
\textbf{\texttt{data}} -- A {\hyperref[loader:loader.HotIndex]{\emph{\code{HotIndex}}}} object

\item {} 
\textbf{\texttt{indices}} -- A \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/io\_ops.html\#placeholders}{Placeholder}. If indices is none the dimensions will be inferred from \emph{data}

\item {} 
\textbf{\texttt{distribution}} -- Distribution for lookup weight initialization

\item {} 
\textbf{\texttt{initrange}} -- Initrange for weight distribution.

\item {} 
\textbf{\texttt{l2}} -- Floating point number determining degree of of l2 regularization for these weights in gradient descent update.

\item {} 
\textbf{\texttt{shape}} -- The dimensions of the output \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/framework.html\#Tensor}{tensor}, typically {[}None, output-size{]}

\item {} 
\textbf{\texttt{makeplace}} -- A boolean to tell whether or not a placeholder has been created for this data (Used by config.py)

\item {} 
\textbf{\texttt{name}} -- A name for unique variable scope.

\end{itemize}

\item[{Returns}] \leavevmode
tf.nn.embedding\_lookup(wghts, indices), wghts, indices

\end{description}\end{quote}

\end{fulllineitems}

\index{mae() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.mae}\pysiglinewithargsret{\code{node\_ops.}\bfcode{mae}}{\emph{*args}, \emph{**kwargs}}{}
Mean Absolute Error

\end{fulllineitems}

\index{mse() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.mse}\pysiglinewithargsret{\code{node\_ops.}\bfcode{mse}}{\emph{*args}, \emph{**kwargs}}{}
Mean Squared Error.

\end{fulllineitems}

\index{mult\_log\_reg() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.mult_log_reg}\pysiglinewithargsret{\code{node\_ops.}\bfcode{mult\_log\_reg}}{\emph{*args}, \emph{**kwargs}}{}
Performs mulitnomial logistic regression forward pass. Weights and bias initialized to zeros.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tensor\_in}} -- A \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/framework.html\#Tensor}{tensor} or \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/io\_ops.html\#placeholders}{placeholder}

\item {} 
\textbf{\texttt{numclasses}} -- For classificatio

\item {} 
\textbf{\texttt{data}} -- For shape inference.

\item {} 
\textbf{\texttt{dtype}} -- For {\hyperref[node_ops:node_ops.weights]{\emph{\code{weights}}}} initialization.

\item {} 
\textbf{\texttt{initrange}} -- For {\hyperref[node_ops:node_ops.weights]{\emph{\code{weights}}}} initialization.

\item {} 
\textbf{\texttt{seed}} -- For {\hyperref[node_ops:node_ops.weights]{\emph{\code{weights}}}} initialization.

\item {} 
\textbf{\texttt{l2}} -- For {\hyperref[node_ops:node_ops.weights]{\emph{\code{weights}}}} initialization.

\item {} 
\textbf{\texttt{name}} -- For \href{https://www.tensorflow.org/versions/r0.7/how\_tos/variable\_scope/index.html}{variable\_scope}

\end{itemize}

\item[{Returns}] \leavevmode
A tensor shape=(tensor\_in.shape{[}0{]}, numclasses)

\end{description}\end{quote}

\end{fulllineitems}

\index{nmode\_tensor\_multiply() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.nmode_tensor_multiply}\pysiglinewithargsret{\code{node\_ops.}\bfcode{nmode\_tensor\_multiply}}{\emph{*args}, \emph{**kwargs}}{}
Nth mode tensor multiplication (for order three tensor) from Kolda and Bader \href{http://dl.acm.org/citation.cfm?id=1655230}{Tensor Decompositions and Applications}
Works for vectors (matrix with a 1 dimension or matrices)
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tensors}} -- A list of tensors the first is an order three tensor the second and order 2

\item {} 
\textbf{\texttt{mode}} -- The mode to perform multiplication against.

\item {} 
\textbf{\texttt{leave\_flattened}} -- Whether or not to reshape tensor back to order 3

\item {} 
\textbf{\texttt{keep\_dims}} -- Whether or not to remove 1 dimensions

\item {} 
\textbf{\texttt{name}} -- For variable scope

\end{itemize}

\item[{Returns}] \leavevmode
Either an order 3 or order 2 tensor

\end{description}\end{quote}

\end{fulllineitems}

\index{nmode\_tensor\_tomatrix() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.nmode_tensor_tomatrix}\pysiglinewithargsret{\code{node\_ops.}\bfcode{nmode\_tensor\_tomatrix}}{\emph{*args}, \emph{**kwargs}}{}
Nmode tensor unfolding (for order three tensor) from Kolda and Bader \href{http://dl.acm.org/citation.cfm?id=1655230}{Tensor Decompositions and Applications}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tensor}} -- Order 3 tensor to unfold

\item {} 
\textbf{\texttt{mode}} -- Mode to unfold (0,1,2, columns, rows, or fibers)

\item {} 
\textbf{\texttt{name}} -- For variable scoping

\end{itemize}

\item[{Returns}] \leavevmode
A matrix (order 2 tensor) with shape dim(mode) X \(\Pi_{othermodes}\) dim(othermodes)

\end{description}\end{quote}

\end{fulllineitems}

\index{other\_cross\_entropy() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.other_cross_entropy}\pysiglinewithargsret{\code{node\_ops.}\bfcode{other\_cross\_entropy}}{\emph{*args}, \emph{**kwargs}}{}
Logistic Loss

\end{fulllineitems}

\index{perplexity() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.perplexity}\pysiglinewithargsret{\code{node\_ops.}\bfcode{perplexity}}{\emph{*args}, \emph{**kwargs}}{}
\end{fulllineitems}

\index{placeholder() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.placeholder}\pysiglinewithargsret{\code{node\_ops.}\bfcode{placeholder}}{\emph{*args}, \emph{**kwargs}}{}
Wrapper to create \href{https://www.tensorflow.org/}{tensorflow} \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/io\_ops.html\#placeholders}{Placeholder} which infers dimensions given data.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{dtype}} -- Tensorflow dtype to initiliaze a Placeholder.

\item {} 
\textbf{\texttt{shape}} -- Dimensions of Placeholder

\item {} 
\textbf{\texttt{data}} -- Data to infer dimensions of Placeholder from.

\item {} 
\textbf{\texttt{name}} -- Unique name for variable scope.

\end{itemize}

\item[{Returns}] \leavevmode
A \href{https://www.tensorflow.org/}{Tensorflow} Placeholder.

\end{description}\end{quote}

\end{fulllineitems}

\index{precision() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.precision}\pysiglinewithargsret{\code{node\_ops.}\bfcode{precision}}{\emph{*args}, \emph{**kwargs}}{}
Percentage of classes detected which are correct.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{targets}} -- A one hot encoding of class labels (num\_points X numclasses)

\item {} 
\textbf{\texttt{predictions}} -- A real valued matrix with indices ranging between zero and 1 (num\_points X numclasses)

\item {} 
\textbf{\texttt{threshold}} -- The detection threshold (between zero and 1)

\item {} 
\textbf{\texttt{detects}} -- In case detection is precomputed for efficiency when evaluating both precision and recall

\end{itemize}

\item[{Returns}] \leavevmode
A scalar value

\end{description}\end{quote}

\end{fulllineitems}

\index{recall() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.recall}\pysiglinewithargsret{\code{node\_ops.}\bfcode{recall}}{\emph{*args}, \emph{**kwargs}}{}
Percentage of actual classes predicted
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{targets}} -- A one hot encoding of class labels (num\_points X numclasses)

\item {} 
\textbf{\texttt{predictions}} -- A real valued matrix with indices ranging between zero and 1 (num\_points X numclasses)

\item {} 
\textbf{\texttt{threshold}} -- The detection threshold (between zero and 1)

\item {} 
\textbf{\texttt{detects}} -- In case detection is precomputed for efficiency when evaluating both precision and recall

\end{itemize}

\item[{Returns}] \leavevmode
A scalar value

\end{description}\end{quote}

\end{fulllineitems}

\index{rmse() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.rmse}\pysiglinewithargsret{\code{node\_ops.}\bfcode{rmse}}{\emph{*args}, \emph{**kwargs}}{}
Root Mean Squared Error

\end{fulllineitems}

\index{se() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.se}\pysiglinewithargsret{\code{node\_ops.}\bfcode{se}}{\emph{*args}, \emph{**kwargs}}{}
Squared Error.

\end{fulllineitems}

\index{ternary\_tensor\_combine() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.ternary_tensor_combine}\pysiglinewithargsret{\code{node\_ops.}\bfcode{ternary\_tensor\_combine}}{\emph{*args}, \emph{**kwargs}}{}
For performing tensor multiplications with batches of data points against an order 3
weight tensor.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{tensors}} -- 

\item {} 
\textbf{\texttt{output\_dim}} -- 

\item {} 
\textbf{\texttt{initrange}} -- 

\item {} 
\textbf{\texttt{name}} -- 

\end{itemize}

\item[{Returns}] \leavevmode


\end{description}\end{quote}

\end{fulllineitems}

\index{weights() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.weights}\pysiglinewithargsret{\code{node\_ops.}\bfcode{weights}}{\emph{*args}, \emph{**kwargs}}{}
Wrapper parameterizing common constructions of tf.Variables.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{distribution}} -- A string identifying distribution `tnorm' for truncated normal, `rnorm' for random normal, `constant' for constant, `uniform' for uniform.

\item {} 
\textbf{\texttt{shape}} -- Shape of weight tensor.

\item {} 
\textbf{\texttt{dtype}} -- dtype for weights

\item {} 
\textbf{\texttt{initrange}} -- Scales standard normal and trunctated normal, value of constant dist., and range of uniform dist. {[}-initrange, initrange{]}.

\item {} 
\textbf{\texttt{seed}} -- For reproducible results.

\item {} 
\textbf{\texttt{l2}} -- Floating point number determining degree of of l2 regularization for these weights in gradient descent update.

\item {} 
\textbf{\texttt{name}} -- For variable scope.

\end{itemize}

\item[{Returns}] \leavevmode
A tf.Variable.

\end{description}\end{quote}

\end{fulllineitems}

\index{x\_dot\_y() (in module node\_ops)}

\begin{fulllineitems}
\phantomsection\label{node_ops:node_ops.x_dot_y}\pysiglinewithargsret{\code{node\_ops.}\bfcode{x\_dot\_y}}{\emph{*args}, \emph{**kwargs}}{}
Takes the inner product for rows of operands{[}1{]}, and operands{[}2{]},
and adds optional bias, operands{[}3{]}, operands{[}4{]}.
If either operands{[}1{]} or operands{[}2{]} or both is a list of tensors
then a list of the pairwise dot products (with bias when len(operands) \textgreater{} 2)
of the lists is returned.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{operands}} -- A list of 2, 3, or 4 \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/framework.html\#Tensor}{tensors} (the first two tensors may be replaced by lists of tensors
in which case the return value will a list of the dot products
for all members of the cross product of the two lists.).

\item {} 
\textbf{\texttt{name}} -- An optional identifier for unique \href{https://www.tensorflow.org/versions/r0.7/how\_tos/variable\_scope/index.html}{variable\_scope}.

\end{itemize}

\item[{Returns}] \leavevmode
A tensor or list of tensors with dimension (operands{[}1{]}.shape{[}0{]}, 1).

\item[{Raises}] \leavevmode
Value error when operands is not a list of at least two tensors.

\end{description}\end{quote}

\end{fulllineitems}



\subsection{generic\_model}
\label{generic_model:generic-model}\label{generic_model::doc}\label{generic_model:tensor-decompositions-and-applications}
A general purpose model builder equipped with generic train, and predict functions which takes parameters for
optimization strategy, mini-batch, etc...
\phantomsection\label{generic_model:module-generic_model}\index{generic\_model (module)}\index{Model (class in generic\_model)}

\begin{fulllineitems}
\phantomsection\label{generic_model:generic_model.Model}\pysiglinewithargsret{\strong{class }\code{generic\_model.}\bfcode{Model}}{\emph{objective, placeholderdict, maxbadcount=20, momentum=None, mb=1000, verbose=True, epochs=50, learnrate=0.003, save=False, opt='grad', decay={[}1, 1.0{]}, evaluate=None, predictions=None, logdir='log/', random\_seed=None, model\_name='generic', clip\_gradients=0.0, make\_histograms=False, best\_model\_path='/tmp/model.ckpt', save\_tensors=\{\}, tensorboard=False, train\_evaluate=None}}{}
Generic model builder for training and predictions.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{objective}} -- Loss function

\item {} 
\textbf{\texttt{placeholderdict}} -- A dictionary of placeholders

\item {} 
\textbf{\texttt{maxbadcount}} -- For early stopping

\item {} 
\textbf{\texttt{momentum}} -- The momentum for tf.MomentumOptimizer

\item {} 
\textbf{\texttt{mb}} -- The mini-batch size

\item {} 
\textbf{\texttt{verbose}} -- Whether to print dev error, and save\_tensor evals

\item {} 
\textbf{\texttt{epochs}} -- maximum number of epochs to train for.

\item {} 
\textbf{\texttt{learnrate}} -- learnrate for gradient descent

\item {} 
\textbf{\texttt{save}} -- Save best model to \emph{best\_model\_path}.

\item {} 
\textbf{\texttt{opt}} -- Optimization strategy. May be `adam', `ada', `grad', `momentum'

\item {} 
\textbf{\texttt{decay}} -- Parameter for decaying learn rate.

\item {} 
\textbf{\texttt{evaluate}} -- Evaluation metric

\item {} 
\textbf{\texttt{predictions}} -- Predictions selected from feed forward pass.

\item {} 
\textbf{\texttt{logdir}} -- Where to put the tensorboard data.

\item {} 
\textbf{\texttt{random\_seed}} -- Random seed for TensorFlow initializers.

\item {} 
\textbf{\texttt{model\_name}} -- Name for model

\item {} 
\textbf{\texttt{clip\_gradients}} -- The limit on gradient size. If 0.0 no clipping is performed.

\item {} 
\textbf{\texttt{make\_histograms}} -- Whether or not to make histograms for model weights and activations

\item {} 
\textbf{\texttt{best\_model\_path}} -- File to save best model to during training.

\item {} 
\textbf{\texttt{save\_tensors}} -- A hashmap of str:Tensor mappings. Tensors are evaluated during training. Evaluations of these tensors on best model are accessible via property {\hyperref[generic_model:generic_model.Model.evaluated_tensors]{\emph{\code{evaluated\_tensors}}}}.

\item {} 
\textbf{\texttt{tensorboard}} -- Whether to make tensorboard histograms of weights and activations, and graphs of dev\_error.

\end{itemize}

\item[{Returns}] \leavevmode
{\hyperref[generic_model:generic_model.Model]{\emph{\code{Model}}}}

\end{description}\end{quote}
\paragraph{Attributes}
\paragraph{Methods}
\index{average\_secs\_per\_epoch (generic\_model.Model attribute)}

\begin{fulllineitems}
\phantomsection\label{generic_model:generic_model.Model.average_secs_per_epoch}\pysigline{\bfcode{average\_secs\_per\_epoch}}
The average number of seconds to complete an epoch.

\end{fulllineitems}

\index{best\_completed\_epochs (generic\_model.Model attribute)}

\begin{fulllineitems}
\phantomsection\label{generic_model:generic_model.Model.best_completed_epochs}\pysigline{\bfcode{best\_completed\_epochs}}
Number of epochs completed during at point of best dev eval during training (fractional)

\end{fulllineitems}

\index{best\_dev\_error (generic\_model.Model attribute)}

\begin{fulllineitems}
\phantomsection\label{generic_model:generic_model.Model.best_dev_error}\pysigline{\bfcode{best\_dev\_error}}
The best dev error reached during training.

\end{fulllineitems}

\index{completed\_epochs (generic\_model.Model attribute)}

\begin{fulllineitems}
\phantomsection\label{generic_model:generic_model.Model.completed_epochs}\pysigline{\bfcode{completed\_epochs}}
Number of epochs completed during training (fractional)

\end{fulllineitems}

\index{eval() (generic\_model.Model method)}

\begin{fulllineitems}
\phantomsection\label{generic_model:generic_model.Model.eval}\pysiglinewithargsret{\bfcode{eval}}{\emph{tensor\_in}, \emph{data}, \emph{supplement=None}}{}
Evaluation of model.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{data}} -- {\hyperref[loader:loader.DataSet]{\emph{\code{DataSet}}}} to evaluate on.

\item[{Returns}] \leavevmode
Result of evaluating on data for \code{self.evaluate}

\end{description}\end{quote}

\end{fulllineitems}

\index{evaluated\_tensors (generic\_model.Model attribute)}

\begin{fulllineitems}
\phantomsection\label{generic_model:generic_model.Model.evaluated_tensors}\pysigline{\bfcode{evaluated\_tensors}}
A dictionary of evaluations on best model for tensors and keys specified by \emph{save\_tensors} argument to constructor.

\end{fulllineitems}

\index{placeholderdict (generic\_model.Model attribute)}

\begin{fulllineitems}
\phantomsection\label{generic_model:generic_model.Model.placeholderdict}\pysigline{\bfcode{placeholderdict}}
Dictionary of model placeholders

\end{fulllineitems}

\index{plot\_train\_dev\_eval() (generic\_model.Model method)}

\begin{fulllineitems}
\phantomsection\label{generic_model:generic_model.Model.plot_train_dev_eval}\pysiglinewithargsret{\bfcode{plot\_train\_dev\_eval}}{\emph{figure\_file='testfig.pdf'}}{}
\end{fulllineitems}

\index{predict() (generic\_model.Model method)}

\begin{fulllineitems}
\phantomsection\label{generic_model:generic_model.Model.predict}\pysiglinewithargsret{\bfcode{predict}}{\emph{data}, \emph{supplement=None}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{data}} -- {\hyperref[loader:loader.DataSet]{\emph{\code{DataSet}}}} to make predictions from.

\item[{Returns}] \leavevmode
A set of predictions from feed forward defined by \code{self.predictions}

\end{description}\end{quote}

\end{fulllineitems}

\index{train() (generic\_model.Model method)}

\begin{fulllineitems}
\phantomsection\label{generic_model:generic_model.Model.train}\pysiglinewithargsret{\bfcode{train}}{\emph{train}, \emph{dev=None}, \emph{supplement=None}, \emph{eval\_schedule='epoch'}, \emph{train\_dev\_eval\_factor=3}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{data}} -- {\hyperref[loader:loader.DataSet]{\emph{\code{DataSet}}}} to train on.

\item[{Returns}] \leavevmode
A trained {\hyperref[generic_model:generic_model.Model]{\emph{\code{Model}}}}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{get\_feed\_list() (in module generic\_model)}

\begin{fulllineitems}
\phantomsection\label{generic_model:generic_model.get_feed_list}\pysiglinewithargsret{\code{generic\_model.}\bfcode{get\_feed\_list}}{\emph{batch}, \emph{placeholderdict}, \emph{supplement=None}, \emph{dropouts=None}, \emph{dropout\_flag='train'}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{batch}} -- A dataset object.

\item {} 
\textbf{\texttt{placeholderdict}} -- A dictionary where the keys match keys in batch, and the values are placeholder tensors

\item {} 
\textbf{\texttt{supplement}} -- A dictionary of numpy input matrices with keys corresponding to placeholders in placeholderdict, where the row size of the matrices do not correspond to the number of datapoints. For use with input data intended for \href{https://www.tensorflow.org/versions/r0.7/api\_docs/python/nn.html\#embeddings}{embedding\_lookup}.

\item {} 
\textbf{\texttt{dropouts}} -- Dropout tensors in graph.

\item {} 
\textbf{\texttt{dropout\_flag}} -- Whether to use Dropout probabilities for feed forward.

\end{itemize}

\item[{Returns}] \leavevmode
A feed dictionary with keys of placeholder tensors and values of numpy matrices, paired by key

\end{description}\end{quote}

\end{fulllineitems}

\index{parse\_summary\_val() (in module generic\_model)}

\begin{fulllineitems}
\phantomsection\label{generic_model:generic_model.parse_summary_val}\pysiglinewithargsret{\code{generic\_model.}\bfcode{parse\_summary\_val}}{\emph{summary\_str}}{}
Helper function to parse numeric value from tf.scalar\_summary
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{summary\_str}} -- Return value from running session on tf.scalar\_summary

\item[{Returns}] \leavevmode
A dictionary containing the numeric values.

\end{description}\end{quote}

\end{fulllineitems}



\subsection{Models}
\label{models:models}\label{models::doc}\label{models:distributed-representations-of-words-and-phrases-and-their-compositionality}
The models below are available in ANTk. If the model takes a config file then a sample config is provided.


\subsubsection{Skipgram}
\label{models:skipgram}\label{models:module-skipgram}\index{skipgram (module)}\index{SkipGramVecs (class in skipgram)}

\begin{fulllineitems}
\phantomsection\label{models:skipgram.SkipGramVecs}\pysiglinewithargsret{\strong{class }\code{skipgram.}\bfcode{SkipGramVecs}}{\emph{textfile}, \emph{vocabulary\_size=12735}, \emph{batch\_size=128}, \emph{embedding\_size=128}, \emph{skip\_window=1}, \emph{num\_skips=2}, \emph{valid\_size=16}, \emph{valid\_window=100}, \emph{num\_sampled=64}, \emph{num\_steps=100000}, \emph{verbose=False}}{}
Trains a skip gram model from \href{http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}{Distributed Representations of Words and Phrases and their Compositionality}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{textfile}} -- Plain text file or zip file with plain text files.

\item {} 
\textbf{\texttt{vocabulary\_size}} -- How many words to use from text

\item {} 
\textbf{\texttt{batch\_size}} -- mini-batch size

\item {} 
\textbf{\texttt{embedding\_size}} -- Dimension of the embedding vector.

\item {} 
\textbf{\texttt{skip\_window}} -- How many words to consider left and right.

\item {} 
\textbf{\texttt{num\_skips}} -- How many times to reuse an input to generate a label.

\item {} 
\textbf{\texttt{valid\_size}} -- Random set of words to evaluate similarity on.

\item {} 
\textbf{\texttt{valid\_window}} -- Only pick dev samples in the head of the distribution.

\item {} 
\textbf{\texttt{num\_sampled}} -- Number of negative examples to sample.

\item {} 
\textbf{\texttt{num\_steps}} -- How many mini-batch steps to take

\item {} 
\textbf{\texttt{verbose}} -- Whether to calculate and print similarities for a sample of words

\end{itemize}

\end{description}\end{quote}
\paragraph{Methods}
\index{plot\_embeddings() (skipgram.SkipGramVecs method)}

\begin{fulllineitems}
\phantomsection\label{models:skipgram.SkipGramVecs.plot_embeddings}\pysiglinewithargsret{\bfcode{plot\_embeddings}}{\emph{filename='tsne.png'}, \emph{num\_terms=500}}{}
Plot tsne reduction of learned word embeddings in 2-space.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{filename}} -- File to save plot to.

\item {} 
\textbf{\texttt{num\_terms}} -- How many words to plot.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{build\_dataset() (in module skipgram)}

\begin{fulllineitems}
\phantomsection\label{models:skipgram.build_dataset}\pysiglinewithargsret{\code{skipgram.}\bfcode{build\_dataset}}{\emph{words}, \emph{vocabulary\_size}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{words}} -- A list of word tokens from a text file

\item {} 
\textbf{\texttt{vocabulary\_size}} -- How many word tokens to keep.

\end{itemize}

\item[{Returns}] \leavevmode
data (text transformed into list of word ids `UNK'=0), count (list of pairs (word:word\_count) indexed by word id), dictionary (word:id hashmap), reverse\_dictionary (id:word hashmap)

\end{description}\end{quote}

\end{fulllineitems}

\index{generate\_batch() (in module skipgram)}

\begin{fulllineitems}
\phantomsection\label{models:skipgram.generate_batch}\pysiglinewithargsret{\code{skipgram.}\bfcode{generate\_batch}}{\emph{data}, \emph{batch\_size}, \emph{num\_skips}, \emph{skip\_window}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{data}} -- list of word ids corresponding to text

\item {} 
\textbf{\texttt{batch\_size}} -- Size of batch to retrieve

\item {} 
\textbf{\texttt{num\_skips}} -- How many times to reuse an input to generate a label.

\item {} 
\textbf{\texttt{skip\_window}} -- How many words to consider left and right.

\end{itemize}

\item[{Returns}] \leavevmode


\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_tsne() (in module skipgram)}

\begin{fulllineitems}
\phantomsection\label{models:skipgram.plot_tsne}\pysiglinewithargsret{\code{skipgram.}\bfcode{plot\_tsne}}{\emph{embeddings}, \emph{labels}, \emph{filename='tsne.png'}, \emph{num\_terms=500}}{}
Makes tsne plot to visualize word embeddings. Need sklearn, matplotlib for this to work.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{filename}} -- Location to save labeled tsne plots

\item {} 
\textbf{\texttt{num\_terms}} -- Num of words to plot

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{read\_data() (in module skipgram)}

\begin{fulllineitems}
\phantomsection\label{models:skipgram.read_data}\pysiglinewithargsret{\code{skipgram.}\bfcode{read\_data}}{\emph{filename}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{filename}} -- A zip file to open and read from

\item[{Returns}] \leavevmode
A list of the space delimited tokens from the textfile.

\end{description}\end{quote}

\end{fulllineitems}



\subsubsection{Matrix Factorization}
\label{models:module-mfmodel}\label{models:matrix-factorization}\index{mfmodel (module)}\index{mf() (in module mfmodel)}

\begin{fulllineitems}
\phantomsection\label{models:mfmodel.mf}\pysiglinewithargsret{\code{mfmodel.}\bfcode{mf}}{\emph{data}, \emph{configfile}, \emph{lamb=0.001}, \emph{kfactors=20}, \emph{learnrate=0.01}, \emph{verbose=True}, \emph{epochs=1000}, \emph{maxbadcount=20}, \emph{mb=500}, \emph{initrange=1}, \emph{eval\_rate=500}, \emph{random\_seed=None}, \emph{develop=False}, \emph{train\_dev\_eval\_factor=3}}{}
\end{fulllineitems}



\paragraph{Sample Config}
\label{models:sample-config}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{dotproduct} \PYG{n}{x\PYGZus{}dot\PYGZus{}y}\PYG{p}{(}\PYG{p}{)}
    \PYG{o}{\PYGZhy{}}\PYG{n}{huser} \PYG{n}{lookup}\PYG{p}{(}\PYG{n}{dataname}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{initrange}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{,} \PYG{n}{shape}\PYG{o}{=}\PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{]}\PYG{p}{)}
    \PYG{o}{\PYGZhy{}}\PYG{n}{hitem} \PYG{n}{lookup}\PYG{p}{(}\PYG{n}{dataname}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{item}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{initrange}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{,} \PYG{n}{shape}\PYG{o}{=}\PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{]}\PYG{p}{)}
    \PYG{o}{\PYGZhy{}}\PYG{n}{ibias} \PYG{n}{lookup}\PYG{p}{(}\PYG{n}{dataname}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{item}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{initrange}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{,} \PYG{n}{shape}\PYG{o}{=}\PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
    \PYG{o}{\PYGZhy{}}\PYG{n}{ubias} \PYG{n}{lookup}\PYG{p}{(}\PYG{n}{dataname}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{initrange}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{,} \PYG{n}{shape}\PYG{o}{=}\PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{Verbatim}

Low Rank Matrix Factorization is a popular machine learning technique used to produce recommendations
given a set of ratings a user has given an item. The known ratings are collected in a user-item utility matrix
and the missing entries are predicted by optimizing a low rank factorization of the utility matrix given the known
entries. The basic idea behind matrix factorization models is that the information encoded for items
in the columns of the utility matrix, and for users in the rows of the utility matrix is not
exactly independent. We optimize the objective function \(\sum_{(u,i)} (R_{ui} - P_i^T U_u)^2\) over the observed
ratings for user \emph{u} and item \emph{i} using gradient descent.

{\hspace*{\fill}\includegraphics{{factormodel}.png}\hspace*{\fill}}

We can express the same optimization in the form of a computational graph that will play nicely with tensorflow:

{\hspace*{\fill}\includegraphics{{graphmf}.png}\hspace*{\fill}}

Here \(xitem_i\), and \(xuser_j\) are some representation of the indices for the user and item vectors in the utility matrix.
These could be one hot vectors, which can then be matrix multiplied by the \emph{P} and \emph{U} matrices to select the corresponding
user and item vectors. In practice it is much faster to let \(xitem_i\), and \(xuser_j\) be vectors of indices
which can be used by tensorflow's \textbf{gather} or \textbf{embedding\_lookup} functions to select the corresponding vector from
the \emph{P} and \emph{U} matrices.


\subsubsection{DSSM (Deep Structured Semantic Model) Variant}
\label{models:module-dssm_model}\label{models:dssm-deep-structured-semantic-model-variant}\index{dssm\_model (module)}\index{dssm() (in module dssm\_model)}

\begin{fulllineitems}
\phantomsection\label{models:dssm_model.dssm}\pysiglinewithargsret{\code{dssm\_model.}\bfcode{dssm}}{\emph{data, configfile, layers={[}10, 10, 10{]}, bn=True, keep\_prob=0.95, act='tanhlecun', initrange=1, kfactors=10, lamb=0.1, mb=500, learnrate=0.0001, verbose=True, maxbadcount=10, epochs=100, model\_name='dssm', random\_seed=500, eval\_rate=500}}{}
\end{fulllineitems}


{\hspace*{\fill}\includegraphics{{dssm}.png}\hspace*{\fill}}


\paragraph{Sample Config}
\label{models:id4}
\begin{Verbatim}[commandchars=\\\{\}]
dotproduct x\PYGZus{}dot\PYGZus{}y()
\PYGZhy{}user\PYGZus{}vecs ident()
\PYGZhy{}\PYGZhy{}huser lookup(dataname=\PYGZsq{}user\PYGZsq{}, initrange=\PYGZdl{}initrange, shape=[None, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}hage dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=.8)
\PYGZhy{}\PYGZhy{}\PYGZhy{}agelookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}age placeholder(tf.float32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}user placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}hsex dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}sexlookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}sex\PYGZus{}weights weights(\PYGZsq{}tnorm\PYGZsq{}, tf.float32, [2, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}sexes embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}sex placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}user placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}hocc dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}occlookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}occ\PYGZus{}weights weights(\PYGZsq{}tnorm\PYGZsq{}, tf.float32, [21, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}occs embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}occ placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}user placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}hzip dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}ziplookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}zip\PYGZus{}weights weights(\PYGZsq{}tnorm\PYGZsq{}, tf.float32, [1000, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}zips embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}zip placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}user placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}husertime dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}time placeholder(tf.float32)
\PYGZhy{}item\PYGZus{}vecs ident()
\PYGZhy{}\PYGZhy{}hitem lookup(dataname=\PYGZsq{}item\PYGZsq{}, initrange=\PYGZdl{}initrange, shape=[None, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}hgenre dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}genrelookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}genres placeholder(tf.float32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}item placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}hmonth dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}monthlookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}month\PYGZus{}weights weights(\PYGZsq{}tnorm\PYGZsq{}, tf.float32, [12, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}months embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}month placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}item placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}hyear dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}yearlookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}year placeholder(tf.float32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}item placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}htfidf dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}tfidflookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}tfidf\PYGZus{}doc\PYGZus{}term placeholder(tf.float32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}item placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}hitemtime dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}time placeholder(tf.float32)
\PYGZhy{}ibias lookup(dataname=\PYGZsq{}item\PYGZsq{}, shape=[None, 1], initrange=\PYGZdl{}initr
\end{Verbatim}


\subsubsection{Weighted DSSM variant}
\label{models:weighted-dssm-variant}\label{models:module-dsaddmodel}\index{dsaddmodel (module)}\index{dsadd() (in module dsaddmodel)}

\begin{fulllineitems}
\phantomsection\label{models:dsaddmodel.dsadd}\pysiglinewithargsret{\code{dsaddmodel.}\bfcode{dsadd}}{\emph{data}, \emph{configfile}, \emph{initrange=0.1}, \emph{kfactors=20}, \emph{lamb=0.01}, \emph{mb=500}, \emph{learnrate=0.003}, \emph{verbose=True}, \emph{maxbadcount=10}, \emph{epochs=100}, \emph{model\_name='dssm'}, \emph{random\_seed=500}, \emph{eval\_rate=500}}{}
\end{fulllineitems}


This model is the same architecture as the variant of DSSM above but with a different loss:

{\hspace*{\fill}\includegraphics{{weightedloss}.png}\hspace*{\fill}}


\subsubsection{Binary Tree of Deep Neural Networks for Multiple Inputs}
\label{models:binary-tree-of-deep-neural-networks-for-multiple-inputs}\label{models:module-tree_model}\index{tree\_model (module)}\index{tree() (in module tree\_model)}

\begin{fulllineitems}
\phantomsection\label{models:tree_model.tree}\pysiglinewithargsret{\code{tree\_model.}\bfcode{tree}}{\emph{data}, \emph{configfile}, \emph{lamb=0.001}, \emph{kfactors=20}, \emph{learnrate=0.0001}, \emph{verbose=True}, \emph{maxbadcount=20}, \emph{mb=500}, \emph{initrange=1e-05}, \emph{epochs=10}, \emph{random\_seed=None}, \emph{eval\_rate=500}, \emph{keep\_prob=0.95}, \emph{act='tanh'}}{}
\end{fulllineitems}


{\hspace*{\fill}\includegraphics{{tree1}.png}\hspace*{\fill}}


\paragraph{Sample Config}
\label{models:id5}
\begin{Verbatim}[commandchars=\\\{\}]
dotproduct x\PYGZus{}dot\PYGZus{}y()
\PYGZhy{}all\PYGZus{}user dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors], activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}tanh\PYGZus{}user tf.nn.tanh()
\PYGZhy{}\PYGZhy{}\PYGZhy{}merge\PYGZus{}user concat(\PYGZdl{}kfactors)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}huser lookup(dataname=\PYGZsq{}user\PYGZsq{}, initrange=\PYGZdl{}initrange, shape=[None, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hage dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}agelookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}age placeholder(tf.float32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}user placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hsex dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}sexlookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}sex\PYGZus{}weights weights(\PYGZsq{}tnorm\PYGZsq{}, tf.float32, [2, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}sexes embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}sex placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}user placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hocc dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}occlookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}occ\PYGZus{}weights weights(\PYGZsq{}tnorm\PYGZsq{}, tf.float32, [21, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}occs embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}occ placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}user placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hzip dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}ziplookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}zip\PYGZus{}weights weights(\PYGZsq{}tnorm\PYGZsq{}, tf.float32, [1000, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}zips embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}zip placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}user placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}husertime dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}time placeholder(tf.float32)
\PYGZhy{}all\PYGZus{}item dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors], activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}tanh\PYGZus{}item tf.nn.tanh()
\PYGZhy{}\PYGZhy{}\PYGZhy{}merge\PYGZus{}item concat(\PYGZdl{}kfactors)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hitem lookup(dataname=\PYGZsq{}item\PYGZsq{}, initrange=\PYGZdl{}initrange, shape=[None, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hgenre dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}genrelookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}genres placeholder(tf.float32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}item placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hmonth dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}monthlookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}month\PYGZus{}weights weights(\PYGZsq{}tnorm\PYGZsq{}, tf.float32, [12, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}months embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}month placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}item placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hyear dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}yearlookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}year placeholder(tf.float32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}item placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}htfidf dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}tfidflookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}tfidf\PYGZus{}doc\PYGZus{}term placeholder(tf.float32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}item placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hitemtime dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=None)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}time placeholder(tf.float32)
\PYGZhy{}ibias lookup(dataname=\PYGZsq{}item\PYGZsq{}, shape=[None, 1], initrange=\PYGZdl{}initrange)
\PYGZhy{}ubias lookup(dataname=\PYGZsq{}user\PYGZsq{}, shape=[None, 1], initrange=\PYGZdl{}initrange)
\end{Verbatim}


\subsubsection{A Deep Neural Network with Concatenated Input Streams}
\label{models:a-deep-neural-network-with-concatenated-input-streams}\label{models:module-dnn_concat_model}\index{dnn\_concat\_model (module)}\index{dnn\_concat() (in module dnn\_concat\_model)}

\begin{fulllineitems}
\phantomsection\label{models:dnn_concat_model.dnn_concat}\pysiglinewithargsret{\code{dnn\_concat\_model.}\bfcode{dnn\_concat}}{\emph{data, configfile, layers={[}16, 8{]}, activation='tanhlecun', initrange=0.001, bn=True, keep\_prob=0.95, concat\_size=24, uembed=32, iembed=32, learnrate=1e-05, verbose=True, epochs=10, maxbadcount=20, mb=2000, eval\_rate=500}}{}
\end{fulllineitems}


{\hspace*{\fill}\includegraphics{{dnn_concat}.png}\hspace*{\fill}}


\paragraph{Sample Config}
\label{models:id6}
\begin{Verbatim}[commandchars=\\\{\}]
out linear(1, True)
\PYGZhy{}h1 dnn([16, 8], activation=\PYGZsq{}tanhlecun\PYGZsq{}, bn=True, keep\PYGZus{}prob=.95)
\PYGZhy{}\PYGZhy{}x concat(24)
\PYGZhy{}\PYGZhy{}\PYGZhy{}huser lookup(dataname=\PYGZsq{}user\PYGZsq{}, initrange=.001, shape=[None, \PYGZdl{}embed])
\PYGZhy{}\PYGZhy{}\PYGZhy{}hitem lookup(dataname=\PYGZsq{}item\PYGZsq{}, initrange=.001, shape=[None, \PYGZdl{}embed])
\end{Verbatim}


\subsubsection{Multiplicative Interaction between Text, User, and Item}
\label{models:multiplicative-interaction-between-text-user-and-item}
{\hspace*{\fill}\includegraphics{{multoutputs}.png}\hspace*{\fill}}


\section{Tutorials}
\label{tutorials:tutorials}\label{tutorials::doc}

\subsection{Node Ops Tutorial}
\label{node_ops_tutorial::doc}\label{node_ops_tutorial:node-ops-tutorial}
Contains functions taking a tensor or structured list of tensors and returning a tensor or structured list of tensors.
The functions are commonly used compositions of tensorflow functions which operate on tensors.


\subsubsection{Weights and Placeholders}
\label{node_ops_tutorial:weights-and-placeholders}
{\hyperref[node_ops:node_ops.weights]{\emph{\code{weights}}}}

{\hyperref[node_ops:node_ops.placeholder]{\emph{\code{placeholder}}}}


\subsubsection{Loss Functions and Evaluation Metrics}
\label{node_ops_tutorial:loss-functions-and-evaluation-metrics}
{\hyperref[node_ops:node_ops.se]{\emph{\code{se}}}}

{\hyperref[node_ops:node_ops.mse]{\emph{\code{mse}}}}

{\hyperref[node_ops:node_ops.rmse]{\emph{\code{rmse}}}}

{\hyperref[node_ops:node_ops.mae]{\emph{\code{mae}}}}

{\hyperref[node_ops:node_ops.cross_entropy]{\emph{\code{cross\_entropy}}}}

{\hyperref[node_ops:node_ops.other_cross_entropy]{\emph{\code{other\_cross\_entropy}}}}

{\hyperref[node_ops:node_ops.perplexity]{\emph{\code{perplexity}}}}

{\hyperref[node_ops:node_ops.detection]{\emph{\code{detection}}}}

{\hyperref[node_ops:node_ops.recall]{\emph{\code{recall}}}}

{\hyperref[node_ops:node_ops.precision]{\emph{\code{precision}}}}

{\hyperref[node_ops:node_ops.accuracy]{\emph{\code{accuracy}}}}

{\hyperref[node_ops:node_ops.fscore]{\emph{\code{fscore}}}}


\subsubsection{Custom Activations}
\label{node_ops_tutorial:custom-activations}
{\hyperref[node_ops:node_ops.ident]{\emph{\code{ident}}}}

\code{tanhlecun}

{\hyperref[node_ops:node_ops.mult_log_reg]{\emph{\code{mult\_log\_reg}}}}


\subsubsection{Matrix Operations}
\label{node_ops_tutorial:matrix-operations}
{\hyperref[node_ops:node_ops.concat]{\emph{\code{concat}}}}

{\hyperref[node_ops:node_ops.x_dot_y]{\emph{\code{x\_dot\_y}}}}

{\hyperref[node_ops:node_ops.cosine]{\emph{\code{cosine}}}}

{\hyperref[node_ops:node_ops.linear]{\emph{\code{linear}}}}

{\hyperref[node_ops:node_ops.embedding]{\emph{\code{embedding}}}}

{\hyperref[node_ops:node_ops.lookup]{\emph{\code{lookup}}}}

{\hyperref[node_ops:node_ops.khatri_rao]{\emph{\code{khatri\_rao}}}}


\subsubsection{Tensor Operations}
\label{node_ops_tutorial:tensor-operations}
{\hyperref[node_ops:node_ops.nmode_tensor_tomatrix]{\emph{\code{nmode\_tensor\_tomatrix}}}}

{\hyperref[node_ops:node_ops.nmode_tensor_multiply]{\emph{\code{nmode\_tensor\_multiply}}}}

{\hyperref[node_ops:node_ops.binary_tensor_combine]{\emph{\code{binary\_tensor\_combine}}}}

{\hyperref[node_ops:node_ops.ternary_tensor_combine]{\emph{\code{ternary\_tensor\_combine}}}}


\subsubsection{Tricks for Training}
\label{node_ops_tutorial:tricks-for-training}
{\hyperref[node_ops:node_ops.batch_normalize]{\emph{\code{batch\_normalize}}}}

{\hyperref[node_ops:node_ops.dropout]{\emph{\code{dropout}}}}


\subsubsection{Neural Networks}
\label{node_ops_tutorial:neural-networks}
\code{dnn}

\code{residual\_dnn}

\code{highway\_dnn}

\code{convolutional\_net}


\subsubsection{Making an op}
\label{node_ops_tutorial:making-an-op}

\subsection{Generic Model Tutorial}
\label{generic_model_tutorial:generic-model-tutorial}\label{generic_model_tutorial::doc}
The generic\_model module abstracts away from many common training scenarios for a reusable model training interface.

Here is sample code in straight tensorflow for the simply Mnist tutorial.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k+kn}{import} \PYG{n+nn}{tensorflow} \PYG{k+kn}{as} \PYG{n+nn}{tf}
\PYG{k+kn}{from} \PYG{n+nn}{tensorflow.examples.tutorials.mnist} \PYG{k+kn}{import} \PYG{n}{input\PYGZus{}data}
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{n}{os}\PYG{o}{.}\PYG{n}{environ}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{CUDA\PYGZus{}VISIBLE\PYGZus{}DEVICES}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{mnist} \PYG{o}{=} \PYG{n}{input\PYGZus{}data}\PYG{o}{.}\PYG{n}{read\PYGZus{}data\PYGZus{}sets}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MNIST\PYGZus{}data/}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{one\PYGZus{}hot}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{placeholder}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{,} \PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{l+m+mi}{784}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{W} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{784}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{b} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{softmax}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{W}\PYG{p}{)} \PYG{o}{+} \PYG{n}{b}\PYG{p}{)}
\PYG{n}{y\PYGZus{}} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{placeholder}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{,} \PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{cross\PYGZus{}entropy} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}sum}\PYG{p}{(}\PYG{n}{y\PYGZus{}}\PYG{o}{*}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{correct\PYGZus{}prediction} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{equal}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{y\PYGZus{}}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{accuracy} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{cast}\PYG{p}{(}\PYG{n}{correct\PYGZus{}prediction}\PYG{p}{,} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{train\PYGZus{}step} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{train}\PYG{o}{.}\PYG{n}{GradientDescentOptimizer}\PYG{p}{(}\PYG{l+m+mf}{0.01}\PYG{p}{)}\PYG{o}{.}\PYG{n}{minimize}\PYG{p}{(}\PYG{n}{cross\PYGZus{}entropy}\PYG{p}{)}
\PYG{n}{accuracy\PYGZus{}summary} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{scalar\PYGZus{}summary}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Accuracy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{accuracy}\PYG{p}{)}
\PYG{n}{session} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{Session}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{summary\PYGZus{}writer} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{train}\PYG{o}{.}\PYG{n}{SummaryWriter}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log/logistic\PYGZus{}regression}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{session}\PYG{o}{.}\PYG{n}{graph}\PYG{o}{.}\PYG{n}{as\PYGZus{}graph\PYGZus{}def}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{session}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{initialize\PYGZus{}all\PYGZus{}variables}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{:}
  \PYG{n}{batch\PYGZus{}xs}\PYG{p}{,} \PYG{n}{batch\PYGZus{}ys} \PYG{o}{=} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{train}\PYG{o}{.}\PYG{n}{next\PYGZus{}batch}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{)}
  \PYG{n}{session}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{n}{train\PYGZus{}step}\PYG{p}{,} \PYG{n}{feed\PYGZus{}dict}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{p}{:} \PYG{n}{batch\PYGZus{}xs}\PYG{p}{,} \PYG{n}{y\PYGZus{}}\PYG{p}{:} \PYG{n}{batch\PYGZus{}ys}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
  \PYG{n}{acc}\PYG{p}{,} \PYG{n}{accuracy\PYGZus{}summary\PYGZus{}str} \PYG{o}{=} \PYG{n}{session}\PYG{o}{.}\PYG{n}{run}\PYG{p}{(}\PYG{p}{[}\PYG{n}{accuracy}\PYG{p}{,} \PYG{n}{accuracy\PYGZus{}summary}\PYG{p}{]}\PYG{p}{,} \PYG{n}{feed\PYGZus{}dict}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{x}\PYG{p}{:} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{test}\PYG{o}{.}\PYG{n}{images}\PYG{p}{,}
                                                                            \PYG{n}{y\PYGZus{}}\PYG{p}{:} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{test}\PYG{o}{.}\PYG{n}{labels}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
  \PYG{n}{summary\PYGZus{}writer}\PYG{o}{.}\PYG{n}{add\PYGZus{}summary}\PYG{p}{(}\PYG{n}{accuracy\PYGZus{}summary\PYGZus{}str}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}
  \PYG{k}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Accuracy: }\PYG{l+s+si}{\PYGZpc{}f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{acc}\PYG{p}{)}
\end{Verbatim}

In the case of this simple Mnist example lines 1-14 process data and define the computational graph, whereas lines
16-28 involve choices about how to train the model, and actions to take during training. An ANTK {\hyperref[generic_model:generic_model.Model]{\emph{\code{Model}}}} object
parameterizes these choices for a wide variety of use cases to allow for reusable code to train a model. To achieve the
same result as our simple Mnist example we can replace lines 17-29 above as follows:

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k+kn}{import} \PYG{n+nn}{tensorflow} \PYG{k+kn}{as} \PYG{n+nn}{tf}
\PYG{k+kn}{from} \PYG{n+nn}{antk.core} \PYG{k+kn}{import} \PYG{n}{generic\PYGZus{}model}
\PYG{k+kn}{from} \PYG{n+nn}{tensorflow.examples.tutorials.mnist} \PYG{k+kn}{import} \PYG{n}{input\PYGZus{}data}
\PYG{k+kn}{from} \PYG{n+nn}{antk.core} \PYG{k+kn}{import} \PYG{n}{loader}
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{sys}
\PYG{n}{os}\PYG{o}{.}\PYG{n}{environ}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{CUDA\PYGZus{}VISIBLE\PYGZus{}DEVICES}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{mnist} \PYG{o}{=} \PYG{n}{input\PYGZus{}data}\PYG{o}{.}\PYG{n}{read\PYGZus{}data\PYGZus{}sets}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MNIST\PYGZus{}data/}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{one\PYGZus{}hot}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{placeholder}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{,} \PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{l+m+mi}{784}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{W} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{784}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{b} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{softmax}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{W}\PYG{p}{)} \PYG{o}{+} \PYG{n}{b}\PYG{p}{)}
\PYG{n}{y\PYGZus{}} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{placeholder}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{float}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{cross\PYGZus{}entropy} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}sum}\PYG{p}{(}\PYG{n}{y\PYGZus{}}\PYG{o}{*}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{predictions} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{correct\PYGZus{}prediction} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{equal}\PYG{p}{(}\PYG{n}{predictions}\PYG{p}{,} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{y\PYGZus{}}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{accuracy} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{cast}\PYG{p}{(}\PYG{n}{correct\PYGZus{}prediction}\PYG{p}{,} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{trainset} \PYG{o}{=} \PYG{n}{loader}\PYG{o}{.}\PYG{n}{DataSet}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{images}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{train}\PYG{o}{.}\PYG{n}{images}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{labels}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{train}\PYG{o}{.}\PYG{n}{labels}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{k}{print}\PYG{p}{(}\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{mnist}\PYG{o}{.}\PYG{n}{train}\PYG{o}{.}\PYG{n}{labels}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{devset} \PYG{o}{=} \PYG{n}{loader}\PYG{o}{.}\PYG{n}{DataSet}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{images}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{test}\PYG{o}{.}\PYG{n}{images}\PYG{p}{\PYGZcb{}}\PYG{p}{,}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{labels}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{test}\PYG{o}{.}\PYG{n}{labels}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{pholders} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{images}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{x}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{labels}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{y\PYGZus{}}\PYG{p}{\PYGZcb{}}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{generic\PYGZus{}model}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{cross\PYGZus{}entropy}\PYG{p}{,} \PYG{n}{pholders}\PYG{p}{,}
                            \PYG{n}{mb}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}
                            \PYG{n}{maxbadcount}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,}
                            \PYG{n}{learnrate}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{,}
                            \PYG{n}{verbose}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{,}
                            \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}
                            \PYG{n}{evaluate}\PYG{o}{=}\PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{accuracy}\PYG{p}{,}
                            \PYG{n}{model\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{simple\PYGZus{}mnist}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                            \PYG{n}{tensorboard}\PYG{o}{=}\PYG{n+nb+bp}{False}\PYG{p}{)}

\PYG{n}{dev} \PYG{o}{=} \PYG{n}{loader}\PYG{o}{.}\PYG{n}{DataSet}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{images}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{test}\PYG{o}{.}\PYG{n}{images}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{labels}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{test}\PYG{o}{.}\PYG{n}{labels}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{dev}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{train} \PYG{o}{=} \PYG{n}{loader}\PYG{o}{.}\PYG{n}{DataSet}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{images}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{train}\PYG{o}{.}\PYG{n}{images}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{labels}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{mnist}\PYG{o}{.}\PYG{n}{train}\PYG{o}{.}\PYG{n}{labels}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{train}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{train}\PYG{p}{,} \PYG{n}{dev}\PYG{o}{=}\PYG{n}{dev}\PYG{p}{,} \PYG{n}{eval\PYGZus{}schedule}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}
\end{Verbatim}

Notice that we had to change the evaluation function to take advantage of early stopping so that when the model does
better the evaluation function is less. So we evaluate on 1 - accuracy = error. Using {\hyperref[generic_model::doc]{\emph{\emph{generic\_model}}}} now allows us to easily test out
different training scenarios by changing some of the default settings.

We can go through all the options and see what is available. Replace your call to the {\hyperref[generic_model:generic_model.Model]{\emph{\code{Model}}}} constructor with
the following call that makes all default parameters explicit.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{model} \PYG{o}{=} \PYG{n}{generic\PYGZus{}model}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{cross\PYGZus{}entropy}\PYG{p}{,} \PYG{n}{pholders}\PYG{p}{,}
                            \PYG{n}{maxbadcount}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,}
                            \PYG{n}{momentum}\PYG{o}{=}\PYG{n+nb+bp}{None}\PYG{p}{,}
                            \PYG{n}{mb}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,}
                            \PYG{n}{verbose}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{,}
                            \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}
                            \PYG{n}{learnrate}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,}
                            \PYG{n}{save}\PYG{o}{=}\PYG{n+nb+bp}{False}\PYG{p}{,}
                            \PYG{n}{opt}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{grad}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                            \PYG{n}{decay}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{]}\PYG{p}{,}
                            \PYG{n}{evaluate}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{accuracy}\PYG{p}{,}
                            \PYG{n}{predictions}\PYG{o}{=}\PYG{n}{predictions}\PYG{p}{,}
                            \PYG{n}{logdir}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log/simple\PYGZus{}mnist}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                            \PYG{n}{random\PYGZus{}seed}\PYG{o}{=}\PYG{n+nb+bp}{None}\PYG{p}{,}
                            \PYG{n}{model\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{simple\PYGZus{}mnist}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                            \PYG{n}{clip\PYGZus{}gradients}\PYG{o}{=}\PYG{l+m+mf}{0.0}\PYG{p}{,}
                            \PYG{n}{make\PYGZus{}histograms}\PYG{o}{=}\PYG{n+nb+bp}{False}\PYG{p}{,}
                            \PYG{n}{best\PYGZus{}model\PYGZus{}path}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/tmp/model.ckpt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                            \PYG{n}{save\PYGZus{}tensors}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
                            \PYG{n}{tensorboard}\PYG{o}{=}\PYG{n+nb+bp}{False}\PYG{p}{)}\PYG{p}{:}
\end{Verbatim}

Suppose we want to save our best set of weights, and bias for this logistic regression model, and make a
tensorboard histogram plot of how the weights change over time. Also, we want to be able to make predictions with our trained model as well.

We just need to set a few arguments in the call to the {\hyperref[generic_model:generic_model.Model]{\emph{\code{Model}}}} constructor:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{save\PYGZus{}tensors}\PYG{o}{=}\PYG{p}{[}\PYG{n}{W}\PYG{p}{,} \PYG{n}{b}\PYG{p}{]}
\PYG{n}{make\PYGZus{}histograms}\PYG{o}{=}\PYG{n+nb+bp}{True}
\end{Verbatim}

You can view the graph with histograms with the usual tensorboard call from the terminal.

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} tensorboard \PYGZhy{}\PYGZhy{}logdir log/simple\PYGZus{}mnist
\end{Verbatim}

Also, to be able to make predictions with our trained model we need to set the predictions argument in the call to
the constructor as below:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{predictions}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{Verbatim}

Now we can get predictions from the trained model using:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{dev\PYGZus{}classes} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{devset}\PYG{p}{)}
\end{Verbatim}


\subsection{All in One Tutorial via Matrix Factorization}
\label{mf_tutorial::doc}\label{mf_tutorial:all-in-one-tutorial-via-matrix-factorization}
Part 1 starts off with a somewhat gentle introduction to the toolkit by implementing basic matrix factorization ratings
prediction on the MovieLens 100k dataset. Read the directions carefully and be prepared
use your copy and pasting skills. Part 2 explores developing a more complex model using deep neural nets to incorporated
user and item meta data into the model.
Carefully reading parts 1 and 2 will pay off when you engage in the task of building a new model.


\subsubsection{Part 1: Matrix Factorization Model}
\label{mf_tutorial:part-1-matrix-factorization-model}
Low Rank Matrix Factorization is a popular machine learning technique used to produce recommendations
given a set of ratings a user has given an item. The known ratings are collected in a user-item utility matrix
and the missing entries are predicted by optimizing a low rank factorization of the utility matrix given the known
entries. The basic idea behind matrix factorization models is that the information encoded for items
in the columns of the utility matrix, and for users in the rows of the utility matrix is not
exactly independent. We optimize the objective function \(\sum_{(u,i)} (R_{ui} - P_i^T U_u)^2\) over the observed
ratings for user \emph{u} and item \emph{i} using gradient descent.

{\hspace*{\fill}\includegraphics{{factormodel}.png}\hspace*{\fill}}

We can express the same optimization in the form of a computational graph that will play nicely with tensorflow:

{\hspace*{\fill}\includegraphics{{graphmf}.png}\hspace*{\fill}}

Here \(xitem_i\), and \(xuser_j\) are some representation of the indices for the user and item vectors in the utility matrix.
These could be one hot vectors, which can then be matrix multiplied by the \emph{P} and \emph{U} matrices to select the corresponding
user and item vectors. In practice it is much faster to let \(xitem_i\), and \(xuser_j\) be vectors of indices
which can be used by tensorflow's \textbf{gather} or \textbf{embedding\_lookup} functions to select the corresponding vector from
the \emph{P} and \emph{U} matrices.

This simple model isn't difficult to code directly in tensorflow, but it's simplicity allows a
demonstration of the functionality of the toolkit without having to tackle a more complex model.

We have some processed MovieLens 100k data prepared for this tutorial located at \href{http://sw.cs.wwu.edu/~tuora/aarontuor/ml100k.tar.gz}{http://sw.cs.wwu.edu/\textasciitilde{}tuora/aarontuor/ml100k.tar.gz} .
The original MovieLens 100k dataset is located at \href{http://grouplens.org/datasets/movielens/}{http://grouplens.org/datasets/movielens/} .
\begin{description}
\item[{To start let's import the modules we need, retrieve our prepared data,}] \leavevmode
and use the {\hyperref[loader::doc]{\emph{\emph{loader}}}} module's {\hyperref[loader:loader.read_data_sets]{\emph{\code{read\_data\_sets}}}} function to load our data:

\end{description}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{tensorflow} \PYG{k+kn}{as} \PYG{n+nn}{tf}
\PYG{k+kn}{from} \PYG{n+nn}{antk.core} \PYG{k+kn}{import} \PYG{n}{config}
\PYG{k+kn}{from} \PYG{n+nn}{antk.core} \PYG{k+kn}{import} \PYG{n}{generic\PYGZus{}model}
\PYG{k+kn}{from} \PYG{n+nn}{antk.core} \PYG{k+kn}{import} \PYG{n}{loader}


\PYG{n}{loader}\PYG{o}{.}\PYG{n}{maybe\PYGZus{}download}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ml100k.tar.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                  \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{http://sw.cs.wwu.edu/\PYGZti{}tuora/aarontuor/ml100k.tar.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{loader}\PYG{o}{.}\PYG{n}{untar}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ml100k.tar.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{loader}\PYG{o}{.}\PYG{n}{read\PYGZus{}data\PYGZus{}sets}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ml100k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{folders}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dev}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                              \PYG{n}{hashlist}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{item}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{Verbatim}

There is a lot more data in the ml100k folder than we need for demonstrating a basic MF model so we use the \textbf{hashlist} and
\textbf{folders} arguments to select only the data files we want.
We can view the dimensions types, and dictionary keys of the data we've loaded using the {\hyperref[loader:loader.DataSets.show]{\emph{\code{DataSets.show}}}} method,
which is a useful feature for debugging.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{data}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{Verbatim}

The previous command will display this to the terminal:

{\hspace*{\fill}\includegraphics{{datatest}.png}\hspace*{\fill}}

For this data there are 10,000 ratings in dev and test, and 80,000 ratings in train.
Notice that the data type of \emph{item} and \emph{user} above is {\hyperref[loader:loader.HotIndex]{\emph{\code{HotIndex}}}}. This is a data structure for storing
one hot vectors, with a field for a vector of indices into a one hot matrix and the column size of the one hot matrix.
This will be important as we intend to use the {\hyperref[node_ops:node_ops.lookup]{\emph{\code{lookup}}}} function, which takes {\hyperref[loader:loader.HotIndex]{\emph{\code{HotIndex}}}}
objects for its \emph{data} argument, makes a placeholder associated with this data and uses the {\hyperref[loader:loader.HotIndex.dim]{\emph{\code{dim}}}} attribute of the {\hyperref[loader:loader.HotIndex]{\emph{\code{HotIndex}}}}
data to create a \textbf{tf.Variable} tensor with the correct dimension. The output is an \textbf{embedding\_lookup} using the placeholder
and variable tensors created.

This model does better with the target ratings centered about the mean so let's center the ratings.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{data}\PYG{o}{.}\PYG{n}{train}\PYG{o}{.}\PYG{n}{labels}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{loader}\PYG{o}{.}\PYG{n}{center}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{train}\PYG{o}{.}\PYG{n}{labels}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{data}\PYG{o}{.}\PYG{n}{dev}\PYG{o}{.}\PYG{n}{labels}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{loader}\PYG{o}{.}\PYG{n}{center}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{dev}\PYG{o}{.}\PYG{n}{labels}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{Verbatim}

\begin{notice}{note}{Todo}

Make a plain text file named mf.config using the text below. We will use this to make the tensorflow computational graph:
\end{notice}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{dotproduct} \PYG{n}{x\PYGZus{}dot\PYGZus{}y}\PYG{p}{(}\PYG{p}{)}
    \PYG{o}{\PYGZhy{}}\PYG{n}{huser} \PYG{n}{lookup}\PYG{p}{(}\PYG{n}{dataname}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{initrange}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{,} \PYG{n}{shape}\PYG{o}{=}\PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{]}\PYG{p}{)}
    \PYG{o}{\PYGZhy{}}\PYG{n}{hitem} \PYG{n}{lookup}\PYG{p}{(}\PYG{n}{dataname}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{item}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{initrange}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{,} \PYG{n}{shape}\PYG{o}{=}\PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{]}\PYG{p}{)}
    \PYG{o}{\PYGZhy{}}\PYG{n}{ibias} \PYG{n}{lookup}\PYG{p}{(}\PYG{n}{dataname}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{item}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{initrange}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{,} \PYG{n}{shape}\PYG{o}{=}\PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
    \PYG{o}{\PYGZhy{}}\PYG{n}{ubias} \PYG{n}{lookup}\PYG{p}{(}\PYG{n}{dataname}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{initrange}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{,} \PYG{n}{shape}\PYG{o}{=}\PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{Verbatim}

The python syntax highlighting illustrates the fact that
the node specifications in a .config file are just python function calls with two things omitted, the first argument
which is a tensor or list of tensors, and the last argument which is the name of the tensor output which defines it's unique
variable scope. The first argument is derived from the structure of the config spec, inferred by a marker symbol which we have
chosen as `-`. The input is
the list of tensors or the single tensor in the spec at the next level below a node call. Tabbing is optional. It may be easier to read
a config file with tabbing if you are using node functions without a long sequence of arguments.
The second omitted argument, the name, is whatever directly follows the graph markers.

Now we make an {\hyperref[config:config.AntGraph]{\emph{\code{AntGraph}}}} object.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{variable\PYGZus{}scope}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mfgraph}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
\PYG{n}{ant} \PYG{o}{=} \PYG{n}{config}\PYG{o}{.}\PYG{n}{AntGraph}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mf.config}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                        \PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{o}{.}\PYG{n}{dev}\PYG{o}{.}\PYG{n}{features}\PYG{p}{,}
                        \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                        \PYG{n}{develop}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}
\end{Verbatim}

When you run the code now you will get a complete print of the tensors made from the config file because we have set the
\textbf{develop} argument to \textbf{True}.

\includegraphics{{tensor_print}.png}

We can get a visual representation of the graph with another line:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{ant}\PYG{o}{.}\PYG{n}{display\PYGZus{}graph}\PYG{p}{(}\PYG{p}{)}
\end{Verbatim}

When you run this code a graphviz dot pdf image of the graph you have composed should pop up on the screen (assuming you
have graphviz installed). This pdf file will show up in the pics folder with the name \textbf{no\_name.pdf}. There are of course
parameters for specifying the name and location where you want the picture to go. The dot specification will be located
in the same place as the picture and be named \textbf{no\_name.dot} unless you have specified a name for the file.

{\hspace*{\fill}\includegraphics{{no_name}.png}\hspace*{\fill}}

Shown in the graph picture above the {\hyperref[node_ops:node_ops.x_dot_y]{\emph{\code{x\_dot\_y}}}} function takes a list of tensors as its first argument.
The first two tensors are matrices whose rows are dot producted resulting in a vector containing a scalar for each row.
The second two tensors are optional biases. For this model, giving a user and item bias helps a great deal. When {\hyperref[node_ops:node_ops.lookup]{\emph{\code{lookup}}}}
is called more than once in a config file using the same \emph{data} argument the previously made placeholder tensor is used,
so here \emph{ibias} depends on the same placeholder as \emph{hbias} and \emph{ubias} depends on the same placeholder as \emph{huser}, which
is what we want.

The {\hyperref[config:config.AntGraph]{\emph{\code{AntGraph}}}} object, \emph{ant} is a complete record of the tensors created in graph building.
There are three accessible fields, {\hyperref[config:config.AntGraph.tensordict]{\emph{\code{tensordict}}}}, {\hyperref[config:config.AntGraph.placeholderdict]{\emph{\code{placeholderdict}}}}, and {\hyperref[config:config.AntGraph.tensor_out]{\emph{\code{tensor\_out}}}},
which are a dictionary of non-placeholder tensors made during graph creation, a dictionary of placeholder tensors made during
graph creation and the tensor or list of tensors which is the output of the top level node function.
These should be useful if we want to access tensors post graph creation.

Okay let's finish making this model:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{y} \PYG{o}{=} \PYG{n}{ant}\PYG{o}{.}\PYG{n}{tensor\PYGZus{}out}
\PYG{n}{y\PYGZus{}} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{placeholder}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{float}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{n+nb+bp}{None}\PYG{p}{]}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Target}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ant}\PYG{o}{.}\PYG{n}{placeholderdict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{y\PYGZus{}} \PYG{c+c1}{\PYGZsh{} put the new placeholder in the placeholderdict for training}
\PYG{n}{objective} \PYG{o}{=} \PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}sum}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{y\PYGZus{}} \PYG{o}{\PYGZhy{}} \PYG{n}{y}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+}
             \PYG{l+m+mf}{0.1}\PYG{o}{*}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}sum}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{ant}\PYG{o}{.}\PYG{n}{tensordict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{huser}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+}
             \PYG{l+m+mf}{0.1}\PYG{o}{*}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}sum}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{ant}\PYG{o}{.}\PYG{n}{tensordict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hitem}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+}
             \PYG{l+m+mf}{0.1}\PYG{o}{*}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}sum}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{ant}\PYG{o}{.}\PYG{n}{tensordict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ubias}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+}
             \PYG{l+m+mf}{0.1}\PYG{o}{*}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}sum}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{ant}\PYG{o}{.}\PYG{n}{tensordict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ibias}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{dev\PYGZus{}rmse} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{div}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}sum}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{y} \PYG{o}{\PYGZhy{}} \PYG{n}{y\PYGZus{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{data}\PYG{o}{.}\PYG{n}{dev}\PYG{o}{.}\PYG{n}{num\PYGZus{}examples}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{model} \PYG{o}{=} \PYG{n}{generic\PYGZus{}model}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{objective}\PYG{p}{,} \PYG{n}{ant}\PYG{o}{.}\PYG{n}{placeholderdict}\PYG{p}{,}
          \PYG{n}{mb}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,}
          \PYG{n}{learnrate}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,}
          \PYG{n}{verbose}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{,}
          \PYG{n}{maxbadcount}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,}
          \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}
          \PYG{n}{evaluate}\PYG{o}{=}\PYG{n}{dev\PYGZus{}rmse}\PYG{p}{,}
          \PYG{n}{predictions}\PYG{o}{=}\PYG{n}{y}\PYG{p}{)}
\end{Verbatim}

Notice that the {\hyperref[config:config.AntGraph.tensordict]{\emph{\code{tensordict}}}} enables easy access to \emph{huser}, \emph{hitem}, \emph{ubias}, \emph{ibias}, which we want to regularize to
prevent overfitting. The {\hyperref[generic_model:generic_model.Model]{\emph{\code{Model}}}} object we are creating \emph{model} needs the fields \emph{objective}, \emph{placeholderdict}, \emph{predictions}, and \emph{targets}.
If you don't specify the other parameters default values are set. \emph{objective} is used as the loss function for gradient
descent. \emph{placeholderdict} is used to pair placeholder tensors with matrices from a dataset dictionary with the same
keys. \emph{targets}, and \emph{predictions} are employed by the loss function during evaluation, and by the prediction function
to give outputs from a trained model.

Training is now as easy as:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{model}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{train}\PYG{p}{,} \PYG{n}{dev}\PYG{o}{=}\PYG{n}{data}\PYG{o}{.}\PYG{n}{dev}\PYG{p}{)}
\end{Verbatim}

You should get about 0.92 RMSE.

There are a few antk functionalities we can take advantage of to make our code more compact. Any node\_op function that
creates trainable weights has a parameter for adding l2 regularization to the weights of the model. We just change
our config as below and we can eliminate the four extra lines in the definition of \textbf{objective}.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{dotproduct} \PYG{n}{x\PYGZus{}dot\PYGZus{}y}\PYG{p}{(}\PYG{p}{)}
    \PYG{o}{\PYGZhy{}}\PYG{n}{huser} \PYG{n}{lookup}\PYG{p}{(}\PYG{n}{dataname}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{initrange}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{,} \PYG{n}{l2}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{shape}\PYG{o}{=}\PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{]}\PYG{p}{)}
    \PYG{o}{\PYGZhy{}}\PYG{n}{hitem} \PYG{n}{lookup}\PYG{p}{(}\PYG{n}{dataname}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{item}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{initrange}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{,} \PYG{n}{l2}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{shape}\PYG{o}{=}\PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{]}\PYG{p}{)}
    \PYG{o}{\PYGZhy{}}\PYG{n}{ibias} \PYG{n}{lookup}\PYG{p}{(}\PYG{n}{dataname}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{item}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{initrange}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{,} \PYG{n}{l2}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{shape}\PYG{o}{=}\PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
    \PYG{o}{\PYGZhy{}}\PYG{n}{ubias} \PYG{n}{lookup}\PYG{p}{(}\PYG{n}{dataname}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{initrange}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{,} \PYG{n}{l2}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{shape}\PYG{o}{=}\PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{Verbatim}

Also, we have a function for RMSE, and we can evaluate the mean absolute error using
the \textbf{save\_tensors} argument to the {\hyperref[generic_model::doc]{\emph{\emph{generic\_model}}}} constructor. Our code now looks like this:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{y} \PYG{o}{=} \PYG{n}{ant}\PYG{o}{.}\PYG{n}{tensor\PYGZus{}out}
\PYG{n}{y\PYGZus{}} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{placeholder}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{float}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{n+nb+bp}{None}\PYG{p}{]}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Target}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ant}\PYG{o}{.}\PYG{n}{placeholderdict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{y\PYGZus{}} \PYG{c+c1}{\PYGZsh{} put the new placeholder in the graph for training}
\PYG{n}{objective} \PYG{o}{=} \PYG{n}{node\PYGZus{}ops}\PYG{o}{.}\PYG{n}{se}\PYG{p}{(}\PYG{n}{y\PYGZus{}} \PYG{o}{\PYGZhy{}} \PYG{n}{y}\PYG{p}{)}
\PYG{n}{dev\PYGZus{}rmse} \PYG{o}{=}  \PYG{n}{node\PYGZus{}ops}\PYG{o}{.}\PYG{n}{rmse}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{y\PYGZus{}}\PYG{p}{)}
\PYG{n}{dev\PYGZus{}mae} \PYG{o}{=} \PYG{n}{node\PYGZus{}ops}\PYG{o}{.}\PYG{n}{mae}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{y\PYGZus{}}\PYG{p}{)}

\PYG{n}{model} \PYG{o}{=} \PYG{n}{generic\PYGZus{}model}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{objective}\PYG{p}{,} \PYG{n}{ant}\PYG{o}{.}\PYG{n}{placeholderdict}\PYG{p}{,}
          \PYG{n}{mb}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,}
          \PYG{n}{learnrate}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,}
          \PYG{n}{verbose}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{,}
          \PYG{n}{maxbadcount}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,}
          \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}
          \PYG{n}{evaluate}\PYG{o}{=}\PYG{n}{dev\PYGZus{}rmse}\PYG{p}{,}
          \PYG{n}{predictions}\PYG{o}{=}\PYG{n}{y}\PYG{p}{,}
          \PYG{n}{save\PYGZus{}tensors}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dev\PYGZus{}mae}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{dev\PYGZus{}mae}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{train}\PYG{p}{,} \PYG{n}{dev}\PYG{o}{=}\PYG{n}{data}\PYG{o}{.}\PYG{n}{dev}\PYG{p}{)}
\end{Verbatim}

If you don't wan't to evaluate a model during training, for instance if you are doing cross-validation, you can just hand the {\hyperref[generic_model:generic_model.Model.train]{\emph{\code{train}}}}
method a training set and omit the dev set. Note that here there must be keys in either the {\hyperref[loader:loader.DataSet]{\emph{\code{DataSet}}}}
{\hyperref[loader:loader.DataSet.features]{\emph{\code{features}}}}, or {\hyperref[loader:loader.DataSet.labels]{\emph{\code{labels}}}} dictionaries, that match with the keys from the {\hyperref[config:config.AntGraph.placeholderdict]{\emph{\code{placeholderdict}}}} which is handed
to the {\hyperref[generic_model:generic_model.Model]{\emph{\code{Model}}}} constructor. In our case we have placed a placeholder with the key \emph{ratings} in the
\code{placeholdedict} corresponding to
the \emph{ratings} key in our \emph{data} {\hyperref[loader:loader.DataSet]{\emph{\code{DataSet}}}}. So our {\hyperref[config:config.AntGraph.placeholderdict]{\emph{\code{placeholderdict}}}} is:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{item}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{o}{\PYGZlt{}}\PYG{n}{tensorflow}\PYG{o}{.}\PYG{n}{python}\PYG{o}{.}\PYG{n}{framework}\PYG{o}{.}\PYG{n}{ops}\PYG{o}{.}\PYG{n}{Tensor} \PYG{n+nb}{object} \PYG{n}{at} \PYG{l+m+mh}{0x7f0bea7b43d0}\PYG{o}{\PYGZgt{}}\PYG{p}{,}
 \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{o}{\PYGZlt{}}\PYG{n}{tensorflow}\PYG{o}{.}\PYG{n}{python}\PYG{o}{.}\PYG{n}{framework}\PYG{o}{.}\PYG{n}{ops}\PYG{o}{.}\PYG{n}{Tensor} \PYG{n+nb}{object} \PYG{n}{at} \PYG{l+m+mh}{0x7f0bea846e90}\PYG{o}{\PYGZgt{}}\PYG{p}{,}
 \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{o}{\PYGZlt{}}\PYG{n}{tensorflow}\PYG{o}{.}\PYG{n}{python}\PYG{o}{.}\PYG{n}{framework}\PYG{o}{.}\PYG{n}{ops}\PYG{o}{.}\PYG{n}{Tensor} \PYG{n+nb}{object} \PYG{n}{at} \PYG{l+m+mh}{0x7f0bea77fc90}\PYG{o}{\PYGZgt{}}\PYG{p}{\PYGZcb{}}
\end{Verbatim}

Now we have a trained model that does pretty well but it would be nice to automate a hyper-parameter search to find the best
we can do (should be around .91).

We can change our mf.config file to accept variables for hyperparameters by substituting hard values with variable names
prefixed with a `\$':

\begin{Verbatim}[commandchars=\\\{\}]
dotproduct x\PYGZus{}dot\PYGZus{}y()
     \PYGZhy{}huser lookup(dataname=\PYGZsq{}user\PYGZsq{}, initrange=\PYGZdl{}initrange, l2=\PYGZdl{}l2, shape=[None, \PYGZdl{}kfactors])
     \PYGZhy{}hitem lookup(dataname=\PYGZsq{}item\PYGZsq{}, initrange=\PYGZdl{}initrange, l2=\PYGZdl{}l2, shape=[None, \PYGZdl{}kfactors])
     \PYGZhy{}ibias lookup(dataname=\PYGZsq{}item\PYGZsq{}, initrange=\PYGZdl{}initrange, l2=\PYGZdl{}l2, shape=[None, 1])
     \PYGZhy{}ubias lookup(dataname=\PYGZsq{}user\PYGZsq{}, initrange=\PYGZdl{}initrange, l2=\PYGZdl{}l2, shape=[None, 1])
\end{Verbatim}

Now we have to let the {\hyperref[config:config.AntGraph]{\emph{\code{AntGraph}}}} constructor know what to bind these variables to with a \emph{variable\_bindings}
argument. So change the constructor call like so.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{variable\PYGZus{}scope}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mfgraph}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{ant} \PYG{o}{=} \PYG{n}{config}\PYG{o}{.}\PYG{n}{AntGraph}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mf.config}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                            \PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{o}{.}\PYG{n}{dev}\PYG{o}{.}\PYG{n}{features}\PYG{p}{,}
                            \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                            \PYG{n}{variable\PYGZus{}bindings} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{kfactors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{initrange}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mf}{0.001}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{l2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mf}{0.1}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{Verbatim}

\begin{notice}{note}{Todo}

Modify the code you've written to take command line arguments for the hyperparameters: \emph{kfactors}, \emph{initrange}, \emph{mb}, \emph{learnrate}, \emph{maxbadcount}, \emph{l2},
and \emph{epochs}, and conduct a parameter search for the best model.
\end{notice}


\subsubsection{Part 2: Tree Model}
\label{mf_tutorial:part-2-tree-model}
To demonstrate the power and flexibility of using a config file we can make this more complex model
below by changing a few lines of code and using a different config file:

{\hspace*{\fill}\includegraphics{{tree1}.png}\hspace*{\fill}}

We need to change the {\hyperref[loader:loader.read_data_sets]{\emph{\code{read\_data\_sets}}}} call to omit the optional \emph{hashlist} parameter so we get more features from
the data folder (if a \emph{hashlist} parameter is not supplied, {\hyperref[loader:loader.read_data_sets]{\emph{\code{read\_data\_sets}}}} reads all files with name prefixes
\textbf{features\_} and \textbf{labels\_} ).

\begin{notice}{note}{Todo}

Make a new python file tree.py with the code below:
\end{notice}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{tensorflow} \PYG{k+kn}{as} \PYG{n+nn}{tf}
\PYG{k+kn}{from} \PYG{n+nn}{antk.core} \PYG{k+kn}{import} \PYG{n}{config}
\PYG{k+kn}{from} \PYG{n+nn}{antk.core} \PYG{k+kn}{import} \PYG{n}{generic\PYGZus{}model}
\PYG{k+kn}{from} \PYG{n+nn}{antk.core} \PYG{k+kn}{import} \PYG{n}{loader}
\PYG{k+kn}{from} \PYG{n+nn}{antk.core} \PYG{k+kn}{import} \PYG{n}{node\PYGZus{}ops}

\PYG{n}{data} \PYG{o}{=} \PYG{n}{loader}\PYG{o}{.}\PYG{n}{read\PYGZus{}data\PYGZus{}sets}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ml100k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{folders}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dev}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{item}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{data}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{Verbatim}

Now we have some user and item meta data which we can examine:

\includegraphics{{ml100kmore}.png}

The idea of this model is to have a deep neural network for each stream of user meta data and item meta data. The user and item dnn's are
concatenated respectively and then fed to a user dnn and an item dnn. The outputs of these dnn's are dot producted to provide ratings predictions.
We can succinctly express this model in a .config file.

\begin{notice}{note}{Todo}

Make a plain text file called tree.config with the specs for our tree model.
\end{notice}

\begin{Verbatim}[commandchars=\\\{\}]
dotproduct x\PYGZus{}dot\PYGZus{}y()
\PYGZhy{}all\PYGZus{}user dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors], activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=0.95)
\PYGZhy{}\PYGZhy{}tanh\PYGZus{}user tf.nn.tanh()
\PYGZhy{}\PYGZhy{}\PYGZhy{}merge\PYGZus{}user concat(\PYGZdl{}kfactors)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}huser lookup(dataname=\PYGZsq{}user\PYGZsq{}, initrange=\PYGZdl{}initrange, shape=[None, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hage dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=0.95)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}agelookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}age placeholder(tf.float32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}user placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hsex dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=0.95)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}sexlookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}sex\PYGZus{}weights weights(\PYGZsq{}tnorm\PYGZsq{}, [2, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}sexes embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}sex placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}user placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hocc dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=0.95)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}occlookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}occ\PYGZus{}weights weights(\PYGZsq{}tnorm\PYGZsq{}, [21, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}occs embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}occ placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}user placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hzip dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=0.95)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}ziplookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}zip\PYGZus{}weights weights(\PYGZsq{}tnorm\PYGZsq{}, [1000, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}zips embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}zip placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}user placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}husertime dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=0.95)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}time placeholder(tf.float32)
\PYGZhy{}all\PYGZus{}item dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors], activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=0.95)
\PYGZhy{}\PYGZhy{}tanh\PYGZus{}item tf.nn.tanh()
\PYGZhy{}\PYGZhy{}\PYGZhy{}merge\PYGZus{}item concat(\PYGZdl{}kfactors)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hitem lookup(dataname=\PYGZsq{}item\PYGZsq{}, initrange=\PYGZdl{}initrange, shape=[None, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hgenre dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=0.95)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}genrelookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}genres placeholder(tf.float32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}item placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hmonth dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=0.95)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}monthlookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}month\PYGZus{}weights weights(\PYGZsq{}tnorm\PYGZsq{}, [12, \PYGZdl{}kfactors])
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}months embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}month placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}item placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hyear dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=0.95)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}yearlookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}year placeholder(tf.float32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}item placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}htfidf dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=0.95)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}tfidflookup embedding()
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}tfidf\PYGZus{}doc\PYGZus{}term placeholder(tf.float32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}item placeholder(tf.int32)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}hitemtime dnn([\PYGZdl{}kfactors,\PYGZdl{}kfactors,\PYGZdl{}kfactors],activation=\PYGZsq{}tanh\PYGZsq{},bn=True,keep\PYGZus{}prob=0.95)
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}time placeholder(tf.float32)
\PYGZhy{}ibias lookup(dataname=\PYGZsq{}item\PYGZsq{}, shape=[None, 1], initrange=\PYGZdl{}initrange)
\PYGZhy{}ubias lookup(dataname=\PYGZsq{}user\PYGZsq{}, shape=[None, 1], initrange=\PYGZdl{}initrange)
\end{Verbatim}

This model employs all the user and item meta-data we have at our disposal. The config file looks pretty complicated, and it is,
but at least it fits on a screen and we can \emph{read} the high level structure of the model. Imagine developing this model with straight python tensorflow code. This would be hundreds of lines of code and it would be much more difficult to \emph{see} what was going on with the model.
We can see what the model will look like without actually building the graph with the {\hyperref[config:config.testGraph]{\emph{\code{config.testGraph}}}} function.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{config}\PYG{o}{.}\PYG{n}{testGraph}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tree.config}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{Verbatim}

\includegraphics{{tree_test}.png}

This looks like a pretty cool model! We should probably normalize the meta data features for training though.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{data}\PYG{o}{.}\PYG{n}{train}\PYG{o}{.}\PYG{n}{labels}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{loader}\PYG{o}{.}\PYG{n}{center}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{train}\PYG{o}{.}\PYG{n}{labels}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{n+nb+bp}{None}\PYG{p}{)}
\PYG{n}{data}\PYG{o}{.}\PYG{n}{dev}\PYG{o}{.}\PYG{n}{labels}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{loader}\PYG{o}{.}\PYG{n}{center}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{dev}\PYG{o}{.}\PYG{n}{labels}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{n+nb+bp}{None}\PYG{p}{)}
\PYG{n}{data}\PYG{o}{.}\PYG{n}{user}\PYG{o}{.}\PYG{n}{features}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{loader}\PYG{o}{.}\PYG{n}{center}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{user}\PYG{o}{.}\PYG{n}{features}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{n+nb+bp}{None}\PYG{p}{)}
\PYG{n}{data}\PYG{o}{.}\PYG{n}{item}\PYG{o}{.}\PYG{n}{features}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{loader}\PYG{o}{.}\PYG{n}{center}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{item}\PYG{o}{.}\PYG{n}{features}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{n+nb+bp}{None}\PYG{p}{)}
\PYG{n}{data}\PYG{o}{.}\PYG{n}{user}\PYG{o}{.}\PYG{n}{features}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{loader}\PYG{o}{.}\PYG{n}{maxnormalize}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{user}\PYG{o}{.}\PYG{n}{features}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{data}\PYG{o}{.}\PYG{n}{item}\PYG{o}{.}\PYG{n}{features}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{loader}\PYG{o}{.}\PYG{n}{maxnormalize}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{item}\PYG{o}{.}\PYG{n}{features}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{Verbatim}

All our other features besides time are categorical and so use lookups. I think I normalized time during data processing but
it couldn't hurt to check. If you think it is a good idea you can whiten these data inputs to have zero mean and unit variance
with some convenience functions from the {\hyperref[loader::doc]{\emph{\emph{loader}}}} module. Now we should build our graph. Notice that we have omitted the
l2 variable in the config file. We are using dropout to regularize our output as an alternative, since this is a standard regularization
technique for deep neural networks.

Remember we need a python dictionary of numpy matrices whose keys match the names of placeholder and lookup operations that will
infer dimensions for the {\hyperref[config:config.AntGraph]{\emph{\code{AntGraph}}}} constructor. So we need to add these lines:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{datadict} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{user}\PYG{o}{.}\PYG{n}{features}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{datadict}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{item}\PYG{o}{.}\PYG{n}{features}\PYG{p}{)}
\PYG{n}{configdatadict} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{dev}\PYG{o}{.}\PYG{n}{features}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{configdatadict}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{n}{datadict}\PYG{p}{)}
\end{Verbatim}

Now we can build the graph. We'll set \textbf{develop} to \textbf{False} because a lot of tensors are going to get made. If something
goes wrong with a model this big set \textbf{develop} to \textbf{True} and pipe standard output to a file for analysis:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{variable\PYGZus{}scope}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mfgraph}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{ant} \PYG{o}{=} \PYG{n}{config}\PYG{o}{.}\PYG{n}{AntGraph}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tree.config}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                            \PYG{n}{data}\PYG{o}{=}\PYG{n}{configdatadict}\PYG{p}{,}
                            \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                            \PYG{n}{variable\PYGZus{}bindings} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{kfactors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{initrange}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mf}{0.001}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
                            \PYG{n}{develop}\PYG{o}{=}\PYG{n+nb+bp}{False}\PYG{p}{)}

\PYG{n}{y} \PYG{o}{=} \PYG{n}{ant}\PYG{o}{.}\PYG{n}{tensor\PYGZus{}out}
\PYG{n}{y\PYGZus{}} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{placeholder}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{float}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{[}\PYG{n+nb+bp}{None}\PYG{p}{,} \PYG{n+nb+bp}{None}\PYG{p}{]}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Target}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ant}\PYG{o}{.}\PYG{n}{placeholderdict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{y\PYGZus{}}  \PYG{c+c1}{\PYGZsh{} put the new placeholder in the graph for training}
\PYG{n}{objective} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}sum}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{y\PYGZus{}} \PYG{o}{\PYGZhy{}} \PYG{n}{y}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{dev\PYGZus{}rmse} \PYG{o}{=}  \PYG{n}{node\PYGZus{}ops}\PYG{o}{.}\PYG{n}{rmse}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{y\PYGZus{}}\PYG{p}{)}
\end{Verbatim}

Training this model will naturally take longer so we can set the evaluation schedule to be shorter than an epoch to check
in on how things are doing. Also, we will need a smaller learnrate for gradient descent. So we can initialize a {\hyperref[generic_model:generic_model.Model]{\emph{\code{Model}}}} object
with the following hyper-parameters as a first approximation, and then train away...

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{model} \PYG{o}{=} \PYG{n}{generic\PYGZus{}model}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{objective}\PYG{p}{,} \PYG{n}{ant}\PYG{o}{.}\PYG{n}{placeholderdict}\PYG{p}{,}
                            \PYG{n}{mb}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,}
                            \PYG{n}{learnrate}\PYG{o}{=}\PYG{l+m+mf}{0.0001}\PYG{p}{,}
                            \PYG{n}{verbose}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{,}
                            \PYG{n}{maxbadcount}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,}
                            \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}
                            \PYG{n}{evaluate}\PYG{o}{=}\PYG{n}{dev\PYGZus{}rmse}\PYG{p}{,}
                            \PYG{n}{predictions}\PYG{o}{=}\PYG{n}{y}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{train}\PYG{p}{,} \PYG{n}{dev}\PYG{o}{=}\PYG{n}{data}\PYG{o}{.}\PYG{n}{dev}\PYG{p}{,} \PYG{n}{supplement}\PYG{o}{=}\PYG{n}{datadict}\PYG{p}{,} \PYG{n}{eval\PYGZus{}schedule}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{)}
\end{Verbatim}

\begin{notice}{note}{Note:}
We added the supplement argument to {\hyperref[generic_model:generic_model.Model.train]{\emph{\code{train}}}} so that the placeholders related to meta-data could be added to the tensorflow
feed dictionary with the backend function \code{get\_feed\_dict} employed by the {\hyperref[generic_model:generic_model.Model]{\emph{\code{Model}}}} constructor.
\end{notice}

This model takes a while to train and from some poking around it is hard to find a set of hyperparameters that will approach the
accuracy of a basic matrix factorization model. The hyperparameters I have provided should give about 0.93 RMSE which isn't good for this data set.
We have a lot of things to try such as batch normalization, dropout, hidden layer size,
number of hidden layers, activation functions, optimization strategies, subsets of the meta data to incorporate into the mode, and of course the
standard learning rate and intitialization strategies.

\begin{notice}{note}{Todo}

Modify the code you've written to take arguments for the set of new hyperparameters, and optional optimization parameters
from the {\hyperref[generic_model:generic_model.Model]{\emph{\code{Model}}}} API. Perform a parameter search to see if you can do better than basic MF.
\end{notice}


\section{Command Line Scripts}
\label{command_line:command-line-scripts}\label{command_line::doc}

\subsection{datatest.py}
\label{command_line:datatest-py}

Tool for displaying data using {\hyperref[loader:loader.read_data_sets]{\emph{\code{loader.read\_data\_sets}}}}.


\begin{Verbatim}[commandchars=\\\{\}]
usage: datatest [\PYGZhy{}h] [\PYGZhy{}hashlist HASHLIST [HASHLIST ...]]
                [\PYGZhy{}cold \textbar{} \PYGZhy{}subfolders SUBFOLDERS [SUBFOLDERS ...]]
                datadirectory
\end{Verbatim}
\begin{description}
\item[{Positional arguments:}] \leavevmode\begin{optionlist}{3cm}
\item [datadirectory]  
Path to folder where data to be loaded and displayed is stored.
\end{optionlist}

\item[{Options:}] \leavevmode\begin{optionlist}{3cm}
\item [-hashlist]  
List of hashes to read. Files will be read of the form ``features\_\textless{}hash\textgreater{}.ext''  or''labels\_\textless{}hash\textgreater{}.ext'' where \textless{}hash\textgreater{} is a string in hashlist. If a hashlist is not specified all files of the form ``features\_\textless{}hash\textgreater{}.ext''  or ``labels\_\textless{}hash\textgreater{}.ext'' regardless what string \textless{}hash\textgreater{} is will be loaded.
\item [-cold=False]  
Extra loading and testing for cold datasets
\item [-subfolders=(`test', `dev', `train')]  
List of subfolders to load and display.
\end{optionlist}

\end{description}


\subsection{normalize.py}
\label{command_line:normalize-py}

Given the path to a file, Capitalization and punctuation is removed, except for infix apostrophes, e.g. ``hasn't'', ``David's''. The normalized text is saved with ``\_norm'' appended to the file name before the extension. The normalized text is saved in the same directory as the original text. Beginning and end of sentence tokens are not provided by this normalization script.


\begin{Verbatim}[commandchars=\\\{\}]
usage: normalize [\PYGZhy{}h] filepath
\end{Verbatim}
\begin{description}
\item[{Positional arguments:}] \leavevmode\begin{optionlist}{3cm}
\item [filepath]  
The path to the file including filename
\end{optionlist}

\end{description}


\section{Movie Lens Processing}
\label{command_line:movie-lens-processing}

\subsection{generateTermDoc.py}
\label{command_line:generatetermdoc-py}
\begin{Verbatim}[commandchars=\\\{\}]
usage: generateTermDoc [\PYGZhy{}h] datapath dictionary descriptions doc\PYGZus{}term\PYGZus{}file
\end{Verbatim}
\begin{description}
\item[{Positional arguments:}] \leavevmode\begin{optionlist}{3cm}
\item [datapath]  
Path to folder where dictionary and descriptions are located, and created document term matrix will be saved.
\item [dictionary]  
Name of the file containing line separated words in vocabulary.
\item [descriptions]  
Name of the file containing line separated text descriptions.
\item [doc\_term\_file]  
Name of the file to save the created sparse document term matrix.
\end{optionlist}

\end{description}


\subsection{ml100k\_item\_process.py}
\label{command_line:ml100k-item-process-py}

Reads MovieLens 100k item meta data and converts to feature files. 
features\_item\_month.index: The produced files are: 
A file storing a {\hyperref[loader:loader.HotIndex]{\emph{\code{HotIndex}}}} object of movie month releases.

	features\_item\_year.mat: A file storing a numpy array of movie year releases.

	features\_item\_genre.mat: A file storing a scipy sparse csr\_matrix of one hot encodings for movie genre.


\begin{Verbatim}[commandchars=\\\{\}]
usage: ml100k\PYGZus{}item\PYGZus{}process [\PYGZhy{}h] datapath outpath
\end{Verbatim}
\begin{description}
\item[{Positional arguments:}] \leavevmode\begin{optionlist}{3cm}
\item [datapath]  
The path to ml-100k dataset. Usually ``some\_relative\_path/ml-100k
\item [outpath]  
The path to the folder to store the processed Movielens 100k item data feature files.
\end{optionlist}

\end{description}


\subsection{ml100k\_user\_process.py}
\label{command_line:ml100k-user-process-py}

Tool to process Movielens 100k user Metadata.


\begin{Verbatim}[commandchars=\\\{\}]
usage: ml100k\PYGZus{}user\PYGZus{}process [\PYGZhy{}h] datapath outpath
\end{Verbatim}
\begin{description}
\item[{Positional arguments:}] \leavevmode\begin{optionlist}{3cm}
\item [datapath]  
Path to ml-100k
\item [outpath]  
Path to save created files to.
\end{optionlist}

\end{description}


\chapter{Indices and tables}
\label{index:indices-and-tables}\begin{itemize}
\item {} 
\DUspan{xref,std,std-ref}{genindex}

\item {} 
\DUspan{xref,std,std-ref}{modindex}

\item {} 
\DUspan{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{theindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{c}
\item {\texttt{config}}, \pageref{config:module-config}
\indexspace
\bigletter{d}
\item {\texttt{dnn\_concat\_model}}, \pageref{models:module-dnn_concat_model}
\item {\texttt{dsaddmodel}}, \pageref{models:module-dsaddmodel}
\item {\texttt{dssm\_model}}, \pageref{models:module-dssm_model}
\indexspace
\bigletter{g}
\item {\texttt{generic\_model}}, \pageref{generic_model:module-generic_model}
\indexspace
\bigletter{l}
\item {\texttt{loader}}, \pageref{loader:module-loader}
\indexspace
\bigletter{m}
\item {\texttt{mfmodel}}, \pageref{models:module-mfmodel}
\indexspace
\bigletter{n}
\item {\texttt{node\_ops}}, \pageref{node_ops:module-node_ops}
\indexspace
\bigletter{s}
\item {\texttt{skipgram}}, \pageref{models:module-skipgram}
\indexspace
\bigletter{t}
\item {\texttt{tree\_model}}, \pageref{models:module-tree_model}
\end{theindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}
