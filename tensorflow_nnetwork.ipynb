{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook Install with Codefolding extension\n",
    "\n",
    "```bash\n",
    "$> pip install ipython\n",
    "$> pip install jupyter\n",
    "$> pip install https://github.com/ipython-contrib/jupyter_contrib_nbextensions/tarball/master\n",
    "$> jupyter contrib nbextension install --user \n",
    "$> jupyter nbextension enable codefolding/main\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "    \"\"\"\n",
    "    Data structure for mini-batch gradient descent training involving non-sequential data.\n",
    "\n",
    "    :param features: (dict) A dictionary of string label names to data matrices.\n",
    "        Matrices may be of types :any:`IndexVector`, scipy sparse csr_matrix, or numpy array.\n",
    "    :param labels: (dict) A dictionary of string label names to data matrices.\n",
    "        Matrices may be of types :any:`IndexVector`, scipy sparse csr_matrix, or numpy array.\n",
    "    :param mix: (boolean) Whether or not to shuffle per epoch.\n",
    "\n",
    "    :examples:\n",
    "\n",
    "        >>> import numpy as np\n",
    "        >>> from antk.core.loader import DataSet\n",
    "        >>> d = DataSet({'id': np.eye(5)}, labels={'ones':np.ones((5, 2))})\n",
    "        >>> d #doctest: +NORMALIZE_WHITESPACE\n",
    "        antk.core.DataSet object with fields:\n",
    "        '_labels': {'ones': array([[ 1.,  1.],\n",
    "                                   [ 1.,  1.],\n",
    "                                   [ 1.,  1.],\n",
    "                                   [ 1.,  1.],\n",
    "                                   [ 1.,  1.]])}\n",
    "        'mix_after_epoch': False\n",
    "        '_num_examples': 5\n",
    "        '_index_in_epoch': 0\n",
    "        '_last_batch_size': 5\n",
    "        '_features': {'id': array([[ 1.,  0.,  0.,  0.,  0.],\n",
    "                                   [ 0.,  1.,  0.,  0.,  0.],\n",
    "                                   [ 0.,  0.,  1.,  0.,  0.],\n",
    "                                   [ 0.,  0.,  0.,  1.,  0.],\n",
    "                                   [ 0.,  0.,  0.,  0.,  1.]])}\n",
    "\n",
    "        >>> d.show() #doctest: +NORMALIZE_WHITESPACE\n",
    "        features:\n",
    "             id: (5, 5) <type 'numpy.ndarray'>\n",
    "        labels:\n",
    "             ones: (5, 2) <type 'numpy.ndarray'>\n",
    "\n",
    "        >>> d.next_batch(3) #doctest: +NORMALIZE_WHITESPACE\n",
    "        antk.core.DataSet object with fields:\n",
    "            '_labels': {'ones': array([[ 1.,  1.],\n",
    "                                       [ 1.,  1.],\n",
    "                                       [ 1.,  1.]])}\n",
    "            'mix_after_epoch': False\n",
    "            '_num_examples': 3\n",
    "            '_index_in_epoch': 0\n",
    "            '_last_batch_size': 3\n",
    "            '_features': {'id': array([[ 1.,  0.,  0.,  0.,  0.],\n",
    "                                       [ 0.,  1.,  0.,  0.,  0.],\n",
    "                                       [ 0.,  0.,  1.,  0.,  0.]])}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, labels=None, mix=False):\n",
    "        self._features = features  # hashmap of feature matrices\n",
    "        self._num_examples = features[features.keys()[0]].shape[0]\n",
    "        if labels:\n",
    "            self._labels = labels # hashmap of label matrices\n",
    "        else:\n",
    "            self._labels = {}\n",
    "        self._index_in_epoch = 0\n",
    "        self.mix_after_epoch = mix\n",
    "        self._last_batch_size = self._num_examples\n",
    "    \n",
    "    def __repr__(self):\n",
    "        attrs = vars(self)\n",
    "        return 'antk.core.DataSet object with fields:\\n' + '\\n'.join(\"\\t%r: %r\" % item for item in attrs.items())\n",
    "\n",
    "    # ======================================================================================\n",
    "    # =============================PROPERTIES===============================================\n",
    "    # ======================================================================================\n",
    "    @property\n",
    "    def features(self):\n",
    "        \"\"\"\n",
    "        :attribute: (dict) A dictionary with string keys and feature matrix values.\n",
    "        \"\"\"\n",
    "        return self._features\n",
    "\n",
    "    @property\n",
    "    def index_in_epoch(self):\n",
    "        \"\"\"\n",
    "        :attribute: (int) The number of data points that have been trained on in a particular epoch.\n",
    "        \"\"\"\n",
    "        return self._index_in_epoch\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        \"\"\"\n",
    "        :attribute: (dict) A dictionary with string keys and label matrix values.\n",
    "        \"\"\"\n",
    "        return self._labels\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        \"\"\"\n",
    "        :attribute: (int) Number of rows (data points) of the matrices in this :any:`DataSet`.\n",
    "        \"\"\"\n",
    "        return self._num_examples\n",
    "\n",
    "    # ======================================================================================\n",
    "    # =============================PUBLIC METHODS===========================================\n",
    "    # ======================================================================================\n",
    "    def reset_index_to_zero(self):\n",
    "        \"\"\"\n",
    "        :method: Sets :any:`index_in_epoch` to 0.\n",
    "        \"\"\"\n",
    "        self._index_in_epoch = 0\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        :method:\n",
    "        Return a sub DataSet of next batch-size examples.\n",
    "            If no shuffling (mix=False):\n",
    "                If `batch_size` is greater than the number of examples left in\n",
    "                the epoch then a batch size DataSet wrapping past beginning\n",
    "                (rows [index_in_epcoch:num_examples, 0::any:`num_examples`-:any:`index_in_epoch`]\n",
    "                will be returned.\n",
    "            If shuffling enabled (mix=True):\n",
    "                If `batch_size` is greater than the number of examples left in the epoch,\n",
    "                points will be shuffled and `batch_size` DataSet is returned starting from index 0.\n",
    "\n",
    "        :param batch_size: (int) The number of rows in the matrices of the sub DataSet.\n",
    "        :return: :any:`DataSet`\n",
    "        \"\"\"\n",
    "        if batch_size != self._last_batch_size and self._index_in_epoch != 0:\n",
    "            self.reset_index_to_zero()\n",
    "        self._last_batch_size = batch_size\n",
    "        assert batch_size <= self._num_examples\n",
    "        start = self._index_in_epoch\n",
    "        if self._index_in_epoch + batch_size > self._num_examples:\n",
    "            if not self.mix_after_epoch:\n",
    "                self._index_in_epoch = (self._index_in_epoch + batch_size) % self._num_examples\n",
    "                end = self._index_in_epoch\n",
    "                newbatch = DataSet(self._next_batch_(self._features, start, end),\n",
    "                                   self._next_batch_(self._labels, start, end))\n",
    "            else:\n",
    "                self.shuffle()\n",
    "                start = 0\n",
    "                end = batch_size\n",
    "                newbatch = DataSet(self._next_batch_(self._features, start, end),\n",
    "                                   self._next_batch_(self._labels, start, end))\n",
    "                self._index_in_epoch = batch_size\n",
    "            return newbatch\n",
    "        else:\n",
    "            end = self._index_in_epoch + batch_size\n",
    "            self._index_in_epoch = (batch_size + self._index_in_epoch) % self._num_examples\n",
    "            if self._index_in_epoch == 0 and self.mix_after_epoch:\n",
    "                self.shuffle()\n",
    "            return DataSet(self._next_batch_(self._features, start, end),\n",
    "                           self._next_batch_(self._labels, start, end))\n",
    "\n",
    "    def show(self):\n",
    "        \"\"\"\n",
    "        :method: Prints the data specs (dimensions, keys, type) in the :any:`DataSet` object\n",
    "        \"\"\"\n",
    "\n",
    "        print('features:')\n",
    "        for name, feature, in self.features.iteritems():\n",
    "            print('\\t %s: %s %s' % (name, feature.shape, type(feature)))\n",
    "        print('labels:')\n",
    "        for name, label in self.labels.iteritems():\n",
    "            print('\\t %s: %s %s' % (name, label.shape, type(label)))\n",
    "\n",
    "    def showmore(self):\n",
    "        \"\"\"\n",
    "        :method: Prints the data specs (dimensions, keys, type) in the :any:`DataSet` object,\n",
    "        along with a sample of up to the first twenty rows for matrices in DataSet.\n",
    "        \"\"\"\n",
    "\n",
    "        print('features:')\n",
    "        for name, feature in self.features.iteritems():\n",
    "            row = min(5, feature.shape[0])\n",
    "            print('\\t %s: \\nFirst %s rows:\\n%s\\n' % (name, row, feature[0:row]))\n",
    "        print('labels:')\n",
    "        for name, label in self.labels.iteritems():\n",
    "            row = min(5, label.shape[0])\n",
    "            print('\\t %s: \\nFirst %s rows:\\n%s\\n' % (name, row, label[0:row]))\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"\n",
    "        :method: The same random permutation is applied to the\n",
    "         rows of all the matrices in :any:`features` and :any:`labels` .\n",
    "        \"\"\"\n",
    "        perm = np.arange(self._num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        self._shuffle_(perm, self._features)\n",
    "        self._shuffle_(perm, self._labels)\n",
    "\n",
    "    def _shuffle_(self, order, datamap):\n",
    "        '''\n",
    "        :param order: A list of the indices for the row permutation\n",
    "        :param datamap:\n",
    "        :return: void\n",
    "        Shuffles the rows an individual matrix in the :any:`DataSet` object.'\n",
    "        '''\n",
    "        for matrix in datamap:\n",
    "            datamap[matrix] = datamap[matrix][order]\n",
    "\n",
    "    def _next_batch_(self, datamap, start, end=None):\n",
    "        '''\n",
    "        :param datamap: A hash map of matrices\n",
    "        :param start: starting row\n",
    "        :param end: ending row\n",
    "        :return: A hash map of slices of matrices from row start to row end\n",
    "        '''\n",
    "        if end is None:\n",
    "            end = self._num_examples\n",
    "        batch_data_map = {}\n",
    "        if end <= start:\n",
    "            start2 = 0\n",
    "            end2 = end\n",
    "            end = self._num_examples\n",
    "            wrapdata = {}\n",
    "            for matrix in datamap:\n",
    "                wrapdata[matrix] = datamap[matrix][start2:end2]\n",
    "                batch_data_map[matrix] = datamap[matrix][start:end]\n",
    "                if sps.issparse(batch_data_map[matrix]):\n",
    "                        batch_data_map[matrix] = sps.vstack([batch_data_map[matrix], wrapdata[matrix]])\n",
    "                else:\n",
    "                    batch_data_map[matrix] = np.concatenate([batch_data_map[matrix], wrapdata[matrix]], axis=0)\n",
    "        else:\n",
    "            for matrix in datamap:\n",
    "                batch_data_map[matrix] = datamap[matrix][start:end]\n",
    "        return batch_data_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Introduction: Neural Network\n",
    "\n",
    "A neural network is just a parametric function. If you can find the right parameters, a two layer neural network can \n",
    "approximate any function!\n",
    "\n",
    "Let $x \\in \\mathbb{R}^{1 \\times n}, W \\in \\mathbb{R}^{n \\times m},$ and $ U \\in \\mathbb{R}^{m \\times p}$. A two layer neural network is the parametric function $\\mathcal{q}: \\mathbb{R}^{1 \\times n} \\rightarrow \\mathbb{R}^{1 \\times p}$ where \n",
    "$\\mathcal{q} = g( U f(x W + b) + c)$, $U, W, b, c$ are the parameters to be learned, and the functions $g,h$ are model choices.\n",
    "\n",
    "![nnet graph](nnet_graph.png)\n",
    "\n",
    "Training a neural network involves a forward pass, which evaluates the function $q$ for a given set of parameters, and a backward pass which adjusts the parameters using gradient descent by way of the backpropagation algorithm, depending on how well $q$ approximates the training example targets. Tensorflow takes care of the math for the backward pass so we only need to worry about coding the forward pass for training.\n",
    "\n",
    "There are several choices for $f$. Below are a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aarontuor/venv/local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhAAAAFkCAYAAABxWwLDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34db6f9210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "x = np.linspace(-5, 5, 100) # 100 linearly spaced numbers between -10, 10\n",
    "\n",
    "# elementwise sigmoid\n",
    "f1 = 1/(1+np.exp(-x))\n",
    "\n",
    "# hyperbolic tangent: tanh\n",
    "f2 = (np.exp(2*x) -1)/(np.exp(2*x) + 1)\n",
    "\n",
    "# rectified linear units\n",
    "f3 = np.maximum(np.zeros(x.shape), x)\n",
    "\n",
    "plt.plot(x,f1, x, f2, x, f3)\n",
    "plt.gca().set_ylim(bottom=-1.5, top=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the computational graph of the function we want to make. Notice that $X$ is not a vector but a batch of vectors stacked together in a matrix, so we can send many vectors at a time through the neural net for training. \n",
    "\n",
    "![nnet graph](batch_nnet.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a graph and set as default\n",
    "g = tf.Graph()\n",
    "g.as_default() # Also better \"with tf.Graph().as_default() as g:\"\n",
    "\n",
    "# First make placeholders for inputs and targets\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784]) # The none dimension leaves us free to choose a batch_size at runtime\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10]) # Ten classes of digits\n",
    "\n",
    "# Now make trainable variables\n",
    "W = tf.Variable(0.01*tf.truncated_normal([784, 50], mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name='W'))\n",
    "U = tf.Variable(0.01*tf.truncated_normal([50, 10], mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name='U'))\n",
    "\n",
    "b = tf.Variable(tf.zeros([50])) # These don't need to be batchsize X 50 because of TF broadcasting\n",
    "c = tf.Variable(tf.zeros([10])) # Actually we really don't want the extra dimension because we want the bias to repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now compute q\n",
    "h = tf.nn.relu(tf.matmul(x, W) + b) # The hidden layer\n",
    "q = tf.nn.softmax(tf.matmul(h,U) + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "cross_entropy = -tf.reduce_sum(y*tf.log(q))\n",
    "\n",
    "# Train step\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "# Evaluate\n",
    "correct_prediction = tf.equal(tf.argmax(q,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "import time\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})\n",
    "    sys.stdout.write('%s' % sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n",
    "    time.sleep(.15)\n",
    "    sys.stdout.write('\\r')\n",
    "sess.close() # release resources allocated to session also \"with tf.Session() as sess:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A reusable nnet\n",
    "This code is perfectly usable but we've written a perfectly good neural network that only works with the vectorized mnist data (or some other 784 dimensional data set)! Let's make our neural network reusable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pass 1 at nnet classifier\n",
    "def nnet_classifier1(x, hidden_size, output_size, activation):\n",
    "    \"\"\"\n",
    "    First pass at a classifier\n",
    "    \n",
    "    :param x: Input to the network\n",
    "    :param hidden_size: Size of second dimension of W the first weight matrix\n",
    "    :param output_size: Size of second dimension of U the second weight matrix\n",
    "    \"\"\"\n",
    "    fan_in = x.get_shape().as_list()[1]\n",
    "    scale = 1.0/np.sqrt(fan_in)\n",
    "    W = tf.Variable(scale*tf.truncated_normal([fan_in, hidden_size], \n",
    "                                             mean=0.0, stddev=1.0, \n",
    "                                             dtype=tf.float32, seed=None, name='W'))\n",
    "    \n",
    "    scale = 1.0/np.sqrt(hidden_size)\n",
    "    U = tf.Variable(scale*tf.truncated_normal([hidden_size, output_size], \n",
    "                                              mean=0.0, stddev=1.0, dtype=tf.float32, \n",
    "                                              seed=None, name='U'))\n",
    "\n",
    "    b = tf.Variable(tf.zeros([hidden_size]))\n",
    "    c = tf.Variable(tf.zeros([output_size]))\n",
    "    \n",
    "    h = activation(tf.matmul(x, W) + b) # The hidden layer\n",
    "    return tf.nn.softmax(tf.matmul(h,U) + c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A better reusable nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pass 2 at nnet classifier\n",
    "def nnet_classifier2(x, layers=[50,10], act=tf.nn.relu, name='nnet'):\n",
    "    \"\"\"\n",
    "    Second pass at a classifier, eliminate repeated code. Bonus: An arbitrarilly deep neural network.\n",
    "    \n",
    "    :param x: Input to the network\n",
    "    :param layers: Sizes of network layers\n",
    "    :param act: Activation function to produce hidden layers of neural network.\n",
    "    :param name: An identifier for retrieving tensors made by dnn\n",
    "    \"\"\"\n",
    "    \n",
    "    for ind, hidden_size in enumerate(layers):\n",
    "        with tf.variable_scope('layer_%s' % ind):\n",
    "            fan_in = x.get_shape().as_list()[1]\n",
    "            scale = 1.0/np.sqrt(fan_in)\n",
    "            W = tf.Variable(scale*tf.truncated_normal([fan_in, hidden_size], \n",
    "                                                     mean=0.0, stddev=1.0, \n",
    "                                                     dtype=tf.float32, seed=None, name='W'))\n",
    "            tf.add_to_collection(name + '_weights', W)\n",
    "            b = tf.Variable(tf.zeros([hidden_size])) \n",
    "            tf.add_to_collection(name + '_bias', b)\n",
    "            x = tf.matmul(x,W) + b\n",
    "            if ind != len(layers) - 1:\n",
    "                x = act(x, name='h' + str(ind)) # The hidden layer\n",
    "            tf.add_to_collection(name + '_activation', x)\n",
    "    return tf.nn.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate boilerplate training code by making a reusable model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleModel():\n",
    "    \"\"\"\n",
    "    A class for gradient descent training arbitrary models.\n",
    "    \n",
    "    :param loss: loss_tensor defined in graph\n",
    "    :param eval_tensor: For evaluating on dev set\n",
    "    :param ph_dict: A dictionary of tensorflow placeholders\n",
    "    :param learnrate: step_size for gradient descent\n",
    "    :param debug: Whether to print debugging info \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loss, eval_tensor, ph_dict, learnrate=0.01, debug=False):\n",
    "        self.loss = loss\n",
    "        self.ph_dict = ph_dict\n",
    "        self.eval_tensor = eval_tensor\n",
    "        self.debug=debug\n",
    "        self.train_step = tf.train.GradientDescentOptimizer(learnrate).minimize(loss)\n",
    "        self.init = tf.initialize_all_variables()\n",
    "        self.epoch = 0.0\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(self.init)\n",
    "        \n",
    "    def train(self, train_data, dev_data, mb=1000, num_epochs=1):\n",
    "        \"\"\"\n",
    "        :param train_data: A DataSet object of train data.\n",
    "        :param dev_data: A DataSet object of dev data.\n",
    "        :param mb: The mini-batch size.\n",
    "        :param num_epochs: How many epochs to train for.\n",
    "        \"\"\"\n",
    "        while self.epoch < num_epochs:\n",
    "            self.epoch += float(mb)/float(train.num_examples)\n",
    "            new_batch = train.next_batch(mb)\n",
    "            self.sess.run(self.train_step, feed_dict=self.get_feed_dict(new_batch, self.ph_dict))\n",
    "            sys.stdout.write('epoch %.2f\\tdev eval: %.4f' % (self.epoch, self.evaluate(dev_data)))\n",
    "            time.sleep(.2)\n",
    "            sys.stdout.write('\\r')\n",
    "        \n",
    "    def evaluate(self, data):\n",
    "        \"\"\"\n",
    "        Evaluation function\n",
    "        \n",
    "        :param data: The data to evaluate on.\n",
    "        :return: The return value of the evaluation function in numpy form\n",
    "        \"\"\"\n",
    "        return self.sess.run(self.eval_tensor, feed_dict=self.get_feed_dict(data, self.ph_dict))\n",
    "                                 \n",
    "    def get_feed_dict(self, batch, ph_dict):\n",
    "\n",
    "        \"\"\"\n",
    "        :param batch: A dataset object.\n",
    "        :param ph_dict: A dictionary where the keys match keys in batch, and the values are placeholder tensors\n",
    "        :return: A feed dictionary with keys of placeholder tensors and values of numpy matrices\n",
    "        \"\"\"\n",
    "        \n",
    "        datadict = batch.features.copy()\n",
    "        datadict.update(batch.labels)\n",
    "                                 \n",
    "        if self.debug:\n",
    "            for desc in ph_dict:\n",
    "                print('%s\\n\\tph: %s\\t%s\\tdt: %s\\t%s' % (desc,\n",
    "                                                        ph_dict[desc].get_shape().as_list(), ph_dict[desc].dtype, \n",
    "                                                        datadict[desc].shape, datadict[desc].dtype))\n",
    "        return {ph_dict[key]:datadict[key] for key in ph_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing SimpleModel class, and nnet_classifier operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     10
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch 2.17\tdev eval: 0.9459"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "# Data prep ================================================================\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train = DataSet({'images': mnist.train.images}, labels={'digits': mnist.train.labels}, mix=False)\n",
    "dev = DataSet({'images': mnist.test.images}, labels={'digits': mnist.test.labels}, mix=False)\n",
    "\n",
    "# Make graph ============================================================\n",
    "ph_dict = {'images': tf.placeholder(tf.float32, shape=[None, 784]),\n",
    "           'digits': tf.placeholder(tf.float32, shape=[None, 10])}\n",
    "\n",
    "prediction = nnet_classifier2(ph_dict['images'], \n",
    "                             layers = [50, 10], \n",
    "                             act = tf.nn.relu, \n",
    "                             name='nnet')\n",
    "\n",
    "# Loss function\n",
    "cross_entropy = -tf.reduce_sum(ph_dict['digits']*tf.log(prediction))\n",
    "\n",
    "# Evaluate\n",
    "correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(ph_dict['digits'],1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Make model\n",
    "model = SimpleModel(cross_entropy, accuracy, ph_dict, learnrate=0.01)\n",
    "\n",
    "# Train ================================================================\n",
    "model.train(train, dev, mb=100, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
